{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "pdf_path = \"../data/2502.00330v1.pdf\"\n",
    "doc = fitz.open(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on PDF Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_infos = doc.get_toc(simple=False) or []\n",
    "\n",
    "pdf_toc = []\n",
    "for item in toc_infos:\n",
    "    lvl = item[0] if len(item) > 0 else None\n",
    "    title = item[1] if len(item) > 1 else None\n",
    "    start_page = item[2] if len(item) > 2 else None\n",
    "    end_pos = item[3].get('to') if len(item) > 3 and item[3] else None\n",
    "    nameddest = item[3].get('nameddest') if len(item) > 3 and item[3] else None\n",
    "\n",
    "    if start_page is not None:\n",
    "        page = doc[start_page-1]\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "\n",
    "        lines = \"\"\n",
    "        for block in blocks:\n",
    "            x0, y0, x1, y1, text, _, _ = block\n",
    "            if len(lines) < 100:\n",
    "                if end_pos and x0 >= end_pos[0]:\n",
    "                    lines += text\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        pdf_toc.append({\n",
    "            \"level\": lvl,\n",
    "            \"title\": title,\n",
    "            \"page\": start_page,\n",
    "            \"position\": end_pos,\n",
    "            \"nameddest\": nameddest,\n",
    "            \"text\": lines[:200] + \"...\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pdf_toc: \n",
    "    lvl_1_toc = [item for item in pdf_toc if item[\"level\"] == 1]\n",
    "    sorted_lvl_1_toc = sorted(lvl_1_toc, key=lambda d: d['page'])\n",
    "    \n",
    "for item in sorted_lvl_1_toc:\n",
    "    print(item.get('nameddest'), item.get('title'), item.get('page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "DEFAULT_DPI = 144\n",
    "\n",
    "def load_pdf_page(page, dpi):\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(dpi/72, dpi/72))\n",
    "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    if pix.width > 3000 or pix.height > 3000:\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n",
    "        image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    return image\n",
    "\n",
    "def load_pdf(pdf_path, dpi=DEFAULT_DPI):\n",
    "    images = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for i in range(len(doc)):\n",
    "        page = doc[i]\n",
    "        image = load_pdf_page(page, dpi)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_pdf(pdf_path, dpi=DEFAULT_DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于level的位置，切分pdf\n",
    "toc_w_idx = []\n",
    "for idx, item in enumerate(lvl_1_toc):\n",
    "    level = item.get('nameddest')\n",
    "    title = item.get('title')\n",
    "    start_page = item.get('page')\n",
    "    start_pos = item.get('position')\n",
    "    if idx < len(lvl_1_toc) - 1:\n",
    "        next_item = lvl_1_toc[idx+1]\n",
    "        end_page = next_item.get('page')\n",
    "        end_pos = next_item.get('position')\n",
    "    else:\n",
    "        end_page = None\n",
    "        end_pos = None\n",
    "    \n",
    "    page_images = images[start_page-1:end_page]\n",
    "\n",
    "    toc_w_idx.append({'level':level, \n",
    "                      'title':title, \n",
    "                      'start_page':start_page, \n",
    "                      'start_pos':start_pos, \n",
    "                      'end_page':end_page, \n",
    "                      'end_pos':end_pos,\n",
    "                      'page_images':page_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level': 'section.1',\n",
       " 'title': 'Introduction',\n",
       " 'start_page': 1,\n",
       " 'start_pos': Point(62.362, 425.857),\n",
       " 'end_page': 2,\n",
       " 'end_pos': Point(62.362, 262.5),\n",
       " 'page_images': [<PIL.Image.Image image mode=RGB size=1191x1684>,\n",
       "  <PIL.Image.Image image mode=RGB size=1191x1684>]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc_w_idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics per Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def llm_gen_w_images(api_key, sys_prompt, qa_prompt, pil_images, temperature):\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=sys_prompt,\n",
    "        temperature=temperature)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        contents=[qa_prompt]+pil_images,\n",
    "        config=config)\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_example_json = {\n",
    "  \"topics\": [\n",
    "    {\n",
    "      \"topic\": \"Performance Advantages of Transformer Networks over RNNs in Machine Translation Tasks\",\n",
    "      \"description\": \"This topic broadly concerns the comparison of Transformer networks and Recurrent Neural Networks (RNNs) in the context of machine translation, focusing on the superior performance characteristics of Transformers.\",\n",
    "      \"summary\": \"The provided text focuses on the significant performance advantages of Transformer networks over traditional Recurrent Neural Network (RNN) based models in machine translation tasks. It argues, based on presented empirical evidence, that Transformers achieve higher BLEU scores, indicating better translation quality, across multiple language pairs and datasets.  The authors specifically attribute this superior performance to the self-attention mechanism within Transformers, which allows for more effective capture of long-range dependencies in the input text compared to the sequential processing inherent in RNNs. The text cites experimental results demonstrating faster training times for Transformers due to their parallelizable architecture, contrasting this with the inherent sequential bottleneck of RNNs.  While acknowledging the potential computational cost of Transformers for extremely long sequences, the authors downplay this limitation in the context of typical machine translation scenarios. They further support their claims by comparing Transformers to convolutional models, arguing for the greater suitability of attention mechanisms for natural language processing. The paper concludes that the shift from recurrent to attention-based models, exemplified by Transformers, represents a major advancement in the field of machine translation. The authors mention, but do not extensively analyze, the limitations imposed by dataset size on the Transformer performance.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Role of Multi-Headed Scaled Dot-Product Self-Attention in Enhancing Contextual Understanding within Transformer Networks\",\n",
    "        \"description\": \"This topic encompasses the specific type of self-attention (scaled dot-product) and its multi-headed variant used in Transformer networks, and how these mechanisms contribute to the model's ability to understand context within input sequences.\",\n",
    "        \"summary\": \"The provided paragraphs delve into the critical role of multi-headed scaled dot-product self-attention in enhancing contextual understanding within Transformer networks. It explains that self-attention allows each word in a sentence to attend to all other words, including itself, to derive a context-aware representation. The scaled dot-product mechanism is presented as a computationally efficient way to calculate attention weights, preventing issues that can arise with large dot products. The text emphasizes the significance of the 'multi-headed' aspect, where multiple self-attention operations are performed in parallel, each learning different aspects of the relationships between words.  This allows the model to capture diverse contextual nuances, such as syntactic and semantic dependencies, simultaneously. The authors argue that this multi-headed approach is crucial for capturing the richness of human language. They contrast this with simpler attention mechanisms, highlighting the ability of multi-headed attention to learn multiple 'representation subspaces'.  The text provides a brief mathematical overview of the scaled dot-product calculation, reinforcing its efficiency and effectiveness. The authors posit that without multi-headed attention, the Transformer's ability to model complex language structures would be significantly diminished. They conclude by highlighting the importance for future works, such as model interpretability and analysis.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "role_prompt = \"\"\"You are a sophisticated academic scholar with expertise in {domain}. \n",
    "You are renowned for your ability to grasp the key topics and ideas of research papers which are significant and insightful.\n",
    "\"\"\"\n",
    "\n",
    "topics_prompt = \"\"\"## TASK\n",
    "You are provided with pdf pages (in image format) from an academic paper in attachment.\n",
    "Analyze the provided paragraphs and identify key academic topics discussed.  \n",
    "For each topic, generate a JSON object containing the following:\n",
    "\n",
    "*   `topic`: A precise and information-rich name for the topic. This should be as specific as possible, potentially combining multiple concepts to accurately reflect the nuanced discussion in the text.  (e.g., 'Application of Transformer Networks to Machine Translation', 'Impact of Multi-Headed Self-Attention on Long-Range Dependency Capture in Transformers').\n",
    "*   `description`: A concise, general definition of the topic (1-2 sentences). Imagine you are explaining it to a colleague *unfamiliar* with the specific paper, but familiar with AI/NLP in general.  Keep the definition broad enough to encompass the general concept, even if the topic name is very specific.\n",
    "*   `summary`: A detailed summary (7-10 sentences) of the topic's treatment *within the provided text*. This should include:\n",
    "    *   The specific arguments made about the topic.\n",
    "    *   Any evidence or examples the authors use related to the topic.\n",
    "    *   The authors' conclusions or claims regarding the topic.\n",
    "    *   Any limitations or critiques of the topic presented by the authors.\n",
    "    *   Any comparisons to other related concepts or methods.\n",
    "\n",
    "Output your entire response as a single, valid JSON object. The highest level should be a list called 'topics'.\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "Example (using a hypothetical excerpt about Transformer Networks):\n",
    "\n",
    "```json\n",
    "{example_json}\n",
    "```\n",
    "\n",
    "### INPUT\n",
    "Refer to the images for the paper and please focus only on the '{section}' part.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "api_key = os.getenv('GEMINI_API_KEY_1')\n",
    "temperature = 0.7\n",
    "domain = \"Artificial Intelligence and LLMs\"\n",
    "\n",
    "responses = []\n",
    "for item in toc_w_idx:\n",
    "    section = item.get('title')\n",
    "    pil_images = item.get('page_images')\n",
    "\n",
    "    sys_prompt = role_prompt.format(domain=domain)\n",
    "    qa_prompt = topics_prompt.format(example_json=json.dumps(topics_example_json, ensure_ascii=False), section=section)\n",
    "    res = llm_gen_w_images(api_key, sys_prompt, qa_prompt, pil_images, temperature)\n",
    "    responses.append(res)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_repair import repair_json  # https://github.com/mangiucugna/json_repair/\n",
    "topics_lst = [repair_json(x) for x in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n",
      "5\n",
      "6\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for item in topics_lst:\n",
    "    dct = json.loads(item)\n",
    "    print(len(dct.get('topics')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_by_paragraph(pdf_path, header_height=80, footer_height=80, left_margin=50, right_margin=50):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, splitting it into paragraphs using PyMuPDF.\n",
    "    Also provides page number and bounding box for each paragraph.\n",
    "    Excludes header, footer, and left/right margin regions.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "        header_height (int): Height of the header region to exclude.\n",
    "        footer_height (int): Height of the footer region to exclude.\n",
    "        left_margin (int): Width of the left margin to exclude.\n",
    "        right_margin (int): Width of the right margin to exclude.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a paragraph\n",
    "              and contains the 'text', 'page', and 'pos' (position) keys.\n",
    "        Returns None if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        paragraphs = []\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            rect = page.rect  # Get the page rectangle\n",
    "            usable_rect = fitz.Rect(\n",
    "                rect.x0 + left_margin,  # Add left margin to x0\n",
    "                rect.y0 + header_height,\n",
    "                rect.x1 - right_margin,  # Subtract right margin from x1\n",
    "                rect.y1 - footer_height\n",
    "            )\n",
    "            blocks = page.get_text(\"dict\", clip=usable_rect)[\"blocks\"]\n",
    "\n",
    "            for b in blocks:\n",
    "                if b['type'] == 0:  # Text block\n",
    "                    block_text = \"\"\n",
    "                    block_rects = []\n",
    "\n",
    "                    for l in b[\"lines\"]:\n",
    "                        for s in l[\"spans\"]:\n",
    "                            block_text += s[\"text\"]\n",
    "                            block_rects.append(fitz.Rect(s[\"bbox\"]))\n",
    "\n",
    "                    if block_rects:\n",
    "                        block_rect = block_rects[0]\n",
    "                        for rect in block_rects[1:]:\n",
    "                            block_rect |= rect\n",
    "\n",
    "                    block_paragraphs = block_text.strip().split('\\n\\n')\n",
    "\n",
    "                    for p in block_paragraphs:\n",
    "                      if p.strip():\n",
    "                        paragraphs.append({\n",
    "                            'text': p.strip(),\n",
    "                            'page': page_num + 1,\n",
    "                            'pos': block_rect\n",
    "                        })\n",
    "\n",
    "        return paragraphs\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{pdf_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "pdf_file = \"../data/2502.00330v1.pdf\"  # Replace with your PDF file path\n",
    "paragraphs = extract_text_by_paragraph(pdf_file)\n",
    "\n",
    "if paragraphs:\n",
    "    for paragraph in paragraphs:\n",
    "        print(f\"Page: {paragraph['page']}\")\n",
    "        print(f\"Position: {paragraph['pos']}\")\n",
    "        print(f\"Text:\\n{paragraph['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Paragraphs to Specific Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\"topics\": [{\"topic\": \"Factors Influencing Many-Shot In-Context Learning (ICL) Performance and the Role of Example Scaling\", \"description\": \"This topic examines the factors that drive performance gains in many-shot ICL, focusing on the question of whether improvements are due to the number of examples themselves or the selection of high-quality examples.\", \"summary\": \"The text explores the key factors that influence the performance of many-shot ICL, particularly focusing on understanding the gains observed when scaling the number of examples. It raises the question of whether the performance improvement comes simply from having more examples in the context, effectively expanding the knowledge base, or whether it stems from an increased probability of selecting a particularly effective subset of positive examples that disproportionately contribute to performance. The text states that resolving this question is critical, since if expanding the context is more effective, then research should concentrate on techniques for long-context understanding. The authors argue that scaling ICL examples and addressing long-context challenges would dominate the end-to-end performance improvements. Previous works are cited which already tackled the question. The setup used will be a state of the art long-context model, to focus on the number of examples on BBH tasks. Moreover, intermediate outputs or rationales are model-generated and modifiable.\"},{\"topic\": \"The Effectiveness of Intelligent Example Selection in Many-Shot In-Context Learning\", \"description\": \"This topic investigates the idea that carefully choosing the right subset of examples can lead to superior performance in ICL compared to naively scaling the number of examples. In this case, example selection aims to reduce redundancy and identifying high-performing subsets.\", \"summary\": \"The text explores the concept that many-shot performance can be driven by only a few high-performing examples and considers whether the performance can be attributed to carefully selected, high-performing examples with disproportionate influence. Given a set of examples and a performance metric, the goal is to find a subset of examples that performs much better than a randomly selected set of examples or the full set of examples. This is a simplification to avoid enumerating all possible example subsets. The authors also claim that it is possible to match or outperform using all examples with fewer, carefully selected examples and states that intelligent example selection is still relevant in many-shot ICL, even when considering retrieval augmentation. Naively including as many examples as possible can be suboptimal, both in terms of computing cost and performance, especially in tasks where performance does not monotonically improve with the number of examples.\"},{\"topic\": \"Reusing Optimized Examples for Iterative Generation and Performance Enhancement in In-Context Learning\", \"description\": \"This topic is about reusing optimized examples to regenerate new examples with the aim to improve ICL performance.\", \"summary\": \"The text explores the possibility of further improvement to scaling examples even if the number of token is necessarily reduced. The approach is to consider the intermediate roles that represents reasoning paths. In particular, the optimized example set is reused as seed demonstrations for LLMs to re-generate the examples on the train set.\"}]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=os.getenv('GEMINI_API_KEY_1'))\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=sys_prompt,\n",
    "    temperature=temperature)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[qa_prompt]+pil_images)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "temperature = 0.7\n",
    "domain = \"Artificial Intelligence and LLMs\"\n",
    "section = toc_w_idx[2].get('title')\n",
    "start_page = toc_w_idx[2].get('start_page')\n",
    "end_page = toc_w_idx[2].get('end_page')\n",
    "pages = list(range(start_page-1, end_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_pages_and_save_fitz(input_pdf, output_pdf, pages):\n",
    "    \"\"\"\n",
    "    使用 pymupdf 从 PDF 文件中提取指定页码的页面，并保存为新的 PDF 文件。\n",
    "\n",
    "    Args:\n",
    "        input_pdf (str): 输入 PDF 文件的路径。\n",
    "        output_pdf (str): 输出 PDF 文件的路径。\n",
    "        pages (list): 要提取的页码列表，例如 [1, 3, 5]。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(input_pdf)\n",
    "        new_doc = fitz.open()  # 创建一个新的空白 PDF 文档\n",
    "\n",
    "        for page_num in pages:\n",
    "            # 注意：页码从 0 开始计数\n",
    "            if 0 <= page_num < doc.page_count:\n",
    "                page = doc[page_num]\n",
    "                new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)\n",
    "            else:\n",
    "                print(f\"警告：页码 {page_num + 1} 超出范围，已跳过。\")\n",
    "\n",
    "        new_doc.save(output_pdf)\n",
    "        print(f\"成功提取页面 {pages} 并保存到 {output_pdf}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件 {input_pdf} 未找到。\")\n",
    "    except Exception as e:\n",
    "        print(f\"发生错误：{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取页面 [4, 5, 6, 7] 并保存到 test_section_2.pdf\n"
     ]
    }
   ],
   "source": [
    "new_pdf = extract_pages_and_save_fitz(input_pdf=pdf_path, output_pdf=\"test_section_2.pdf\", pages=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_example_json = {\n",
    "  \"topics\": [\n",
    "    {\n",
    "      \"topic\": \"Performance Advantages of Transformer Networks over RNNs in Machine Translation Tasks\",\n",
    "      \"description\": \"This topic broadly concerns the comparison of Transformer networks and Recurrent Neural Networks (RNNs) in the context of machine translation, focusing on the superior performance characteristics of Transformers.\",\n",
    "      \"summary\": \"The provided text focuses on the significant performance advantages of Transformer networks over traditional Recurrent Neural Network (RNN) based models in machine translation tasks. It argues, based on presented empirical evidence, that Transformers achieve higher BLEU scores, indicating better translation quality, across multiple language pairs and datasets.  The authors specifically attribute this superior performance to the self-attention mechanism within Transformers, which allows for more effective capture of long-range dependencies in the input text compared to the sequential processing inherent in RNNs. The text cites experimental results demonstrating faster training times for Transformers due to their parallelizable architecture, contrasting this with the inherent sequential bottleneck of RNNs.  While acknowledging the potential computational cost of Transformers for extremely long sequences, the authors downplay this limitation in the context of typical machine translation scenarios. They further support their claims by comparing Transformers to convolutional models, arguing for the greater suitability of attention mechanisms for natural language processing. The paper concludes that the shift from recurrent to attention-based models, exemplified by Transformers, represents a major advancement in the field of machine translation. The authors mention, but do not extensively analyze, the limitations imposed by dataset size on the Transformer performance.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Role of Multi-Headed Scaled Dot-Product Self-Attention in Enhancing Contextual Understanding within Transformer Networks\",\n",
    "        \"description\": \"This topic encompasses the specific type of self-attention (scaled dot-product) and its multi-headed variant used in Transformer networks, and how these mechanisms contribute to the model's ability to understand context within input sequences.\",\n",
    "        \"summary\": \"The provided paragraphs delve into the critical role of multi-headed scaled dot-product self-attention in enhancing contextual understanding within Transformer networks. It explains that self-attention allows each word in a sentence to attend to all other words, including itself, to derive a context-aware representation. The scaled dot-product mechanism is presented as a computationally efficient way to calculate attention weights, preventing issues that can arise with large dot products. The text emphasizes the significance of the 'multi-headed' aspect, where multiple self-attention operations are performed in parallel, each learning different aspects of the relationships between words.  This allows the model to capture diverse contextual nuances, such as syntactic and semantic dependencies, simultaneously. The authors argue that this multi-headed approach is crucial for capturing the richness of human language. They contrast this with simpler attention mechanisms, highlighting the ability of multi-headed attention to learn multiple 'representation subspaces'.  The text provides a brief mathematical overview of the scaled dot-product calculation, reinforcing its efficiency and effectiveness. The authors posit that without multi-headed attention, the Transformer's ability to model complex language structures would be significantly diminished. They conclude by highlighting the importance for future works, such as model interpretability and analysis.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "role_prompt = \"\"\"You are a sophisticated academic scholar with expertise in {domain}. \n",
    "You are renowned for your ability to grasp the key topics and ideas of research papers which are significant and insightful.\n",
    "\"\"\"\n",
    "\n",
    "topics_prompt = \"\"\"## TASK\n",
    "You are provided with PDF (as attached) which is a part of an academic paper.\n",
    "Analyze through the specific section and identify the key academic topics discussed.  \n",
    "For each topic, generate a JSON object containing the following:\n",
    "\n",
    "*   `topic`: A precise and information-rich name for the topic. This should be as specific as possible, potentially combining multiple concepts to accurately reflect the nuanced discussion in the text.  (e.g., 'Application of Transformer Networks to Machine Translation', 'Impact of Multi-Headed Self-Attention on Long-Range Dependency Capture in Transformers').\n",
    "*   `description`: A concise, general definition of the topic (1-2 sentences). Imagine you are explaining it to a colleague *unfamiliar* with the specific paper, but familiar with AI/NLP in general.  Keep the definition broad enough to encompass the general concept, even if the topic name is very specific.\n",
    "*   `summary`: A detailed summary (7-10 sentences) of the topic's treatment *within the provided text*. This should include:\n",
    "    *   The specific arguments made about the topic.\n",
    "    *   Any evidence or examples the authors use related to the topic.\n",
    "    *   The authors' conclusions or claims regarding the topic.\n",
    "    *   Any limitations or critiques of the topic presented by the authors.\n",
    "    *   Any comparisons to other related concepts or methods.\n",
    "\n",
    "Output your entire response as a single, valid JSON object. The highest level should be a list called 'topics'.\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "Example (using a hypothetical excerpt section \"Methodology\" about Transformer Networks):\n",
    "\n",
    "```json\n",
    "{example_json}\n",
    "```\n",
    "\n",
    "### INPUT\n",
    "Refer to the attached pdf for the paper. Focus only on '{section}' section. And now get started!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "temperature = 0.7\n",
    "domain = \"Artificial Intelligence and LLMs\"\n",
    "\n",
    "sys_prompt = role_prompt.format(domain=domain)\n",
    "qa_prompt = topics_prompt.format(example_json=json.dumps(topics_example_json, ensure_ascii=False), section=section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_section_2.pdf\", 'rb') as f:\n",
    "    pdf_bytes = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "已有代码如下：\n",
    "\n",
    "\n",
    "```python\n",
    "import io\n",
    "import httpx\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "long_context_pdf_path = \"https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf\" # Replace with the actual URL of your large PDF\n",
    "doc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)\n",
    "```\n",
    "\n",
    "如果此处long_context_pdf_path为本地文件地址，则应当如何修订以上代码？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document presents a novel algorithm called BRIDGE (Bayesian Refinement and Iterative Demonstration Generation for Examples) designed to enhance many-shot In-Context Learning (ICL) with intelligent example selection and iterative example generation. It addresses the issues of redundancy and diminishing returns often seen when simply scaling the number of examples in ICL. BRIDGE operates through alternating steps of \"optimize\" and \"generate,\" using a Bayesian optimizer to identify optimal subsets of examples and leveraging Large Language Models (LLMs) to re-generate reasoning paths within those examples. The algorithm aims to improve both cost-performance trade-offs and reasoning path generation. The document details the methodology, including the Bayesian optimization subroutine with random scalarization, and presents experimental setups using various models (Gemini, Mistral, Claude) and challenging datasets (BBH, MATH, GSM-Hard, BIRD) to evaluate BRIDGE's effectiveness against baselines like using all available examples directly or with chain-of-thought (CoT) prompting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=os.getenv('GEMINI_API_KEY_1'))\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=sys_prompt,\n",
    "    temperature=temperature)\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "# filepath = pathlib.Path('file.pdf')\n",
    "# filepath.write_bytes(httpx.get(doc_url).content\n",
    "\n",
    "prompt = \"Summarize this document\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash-exp\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=pdf_bytes,\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_list = [\"Abstract\",\n",
    "                'Introduction', 'Related Work', 'Background',\n",
    "                \"Introduction and Motivation\", \"Computation Function\", \" Routing Function\",\n",
    "                \"Preliminary\", \"Problem Formulation\",\n",
    "                'Methods', 'Methodology', \"Method\", 'Approach', 'Approaches',\n",
    "                \"Materials and Methods\", \"Experiment Settings\",\n",
    "                'Experiment', \"Experimental Results\", \"Evaluation\", \"Experiments\",\n",
    "                \"Results\", 'Findings', 'Data Analysis',\n",
    "                \"Discussion\", \"Results and Discussion\", \"Conclusion\",\n",
    "                'References',\n",
    "                \"Acknowledgments\", \"Appendix\", \"FAQ\", \"Frequently Asked Questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdf_meta_det import extract_meta, dump_toml\n",
    "\n",
    "pattern = '|'.join(re.escape(section) for section in section_list)\n",
    "\n",
    "mtch_rslts = []\n",
    "for i in range(min(len(doc), 10)):\n",
    "    tmp_rslt = extract_meta(doc, pattern=pattern, page=i+1)\n",
    "    mtch_rslts.extend(tmp_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size, flags = 0, 0\n",
    "for item in mtch_rslts:\n",
    "    if item.get('size') > size:\n",
    "        size = item.get('size')\n",
    "    if item.get('flags') > flags:\n",
    "        flags = item.get('flags')\n",
    "print(size, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvsd_mtch_rslts = [item for item in mtch_rslts if item.get('size') == size and item.get('flags') == flags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_meta_det import extract_meta, dump_toml\n",
    "\n",
    "auto_level = 1\n",
    "addnl = False\n",
    "tmp_meta_ptrn = [dump_toml(m, auto_level, addnl) for m in rvsd_mtch_rslts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 tmp_meta_ptrn 写入 recipe.toml 文件\n",
    "with open('recipe.toml', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(tmp_meta_ptrn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from pdf_toc_gen import get_file_encoding, gen_toc\n",
    "\n",
    "recipe_file_path = 'recipe.toml'\n",
    "recipe_file = open(recipe_file_path, \"r\", encoding=get_file_encoding(recipe_file_path))\n",
    "recipe = toml.load(recipe_file)\n",
    "toc = gen_toc(doc, recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Paragraph Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_by_paragraph(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, splitting it into paragraphs using PyMuPDF.\n",
    "    Also provides page number and bounding box for each paragraph.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a paragraph\n",
    "              and contains the 'text', 'page', and 'pos' (position) keys.\n",
    "        Returns None if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        paragraphs = []\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for b in blocks:\n",
    "                if b['type'] == 0:\n",
    "                    block_text = \"\"\n",
    "                    block_rects = []  # Collect rectangles for the entire block\n",
    "\n",
    "                    for l in b[\"lines\"]:\n",
    "                        for s in l[\"spans\"]:\n",
    "                            block_text += s[\"text\"]\n",
    "                            block_rects.append(fitz.Rect(s[\"bbox\"]))\n",
    "                    \n",
    "                    # Combine the rects to get the overall block rect\n",
    "                    if block_rects:\n",
    "                        block_rect = block_rects[0]\n",
    "                        for rect in block_rects[1:]:\n",
    "                            block_rect |= rect  # Union of rectangles\n",
    "\n",
    "                    block_paragraphs = block_text.strip().split('\\n\\n') # You can further improve this with regex if needed\n",
    "\n",
    "                    for p in block_paragraphs:\n",
    "                      if p.strip():\n",
    "                        paragraphs.append({\n",
    "                            'text': p.strip(),\n",
    "                            'page': page_num + 1,  # Page numbers start from 1\n",
    "                            'pos': block_rect\n",
    "                        })\n",
    "\n",
    "        return paragraphs\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{pdf_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Position: Rect(61.630001068115234, 84.55769348144531, 509.07781982421875, 151.30718994140625)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-ShotReasoners Through Iterative Optimization andGeneration\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(62.07500076293945, 165.1982421875, 447.9085998535156, 179.2966766357422)\n",
      "Text:\n",
      "Xingchen Wan1, Han Zhou1 3*, Ruoxi Sun2*, Hootan Nakhost1, Ke Jiang1 and Sercan Ö. Arık1\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(61.8489990234375, 179.02157592773438, 466.102294921875, 192.78636169433594)\n",
      "Text:\n",
      "1Google Cloud AI Research, 2Google DeepMind, 3University of Cambridge, *Work done at Google Cloud AI Research\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(62.36199951171875, 218.54690551757812, 534.6339721679688, 415.0863037109375)\n",
      "Text:\n",
      "Recent advances in long-context large language models (LLMs) have led to the emerging paradigmof many-shot in-context learning (ICL), where it is observed that scaling many more demonstratingexamples beyond the conventional few-shot setup in the context can lead to performance benefits.However, despite its promise, it is unclear what aspects dominate the benefits and whether simplyscaling to more examples is the most effective way of improving many-shot ICL. In this work, we firstprovide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance canstill be attributed to often a few disproportionately influential examples and 2) identifying such influentialexamples (“optimize”) and using them as demonstrations to regenerate new examples (“generate\")can lead to further improvements. Inspired by the findings, we propose bridge, an algorithm thatalternates between the optimize step with Bayesian optimization to discover the influential sets ofexamples and the generate step to reuse this set to expand the reasoning paths of the examples back tothe many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we showthat bridge led to significant improvements across a diverse set of tasks, including symbolic reasoning,numerical reasoning, and code generation.\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(62.36199951171875, 442.4657897949219, 154.67959594726562, 455.41717529296875)\n",
      "Text:\n",
      "1. Introduction\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(61.457000732421875, 464.56927490234375, 533.2018432617188, 631.468505859375)\n",
      "Text:\n",
      "Recent advances in large language models (LLMs) have led to the emergence of in-context learning(ICL) as a promising new learning paradigm (Brown et al., 2020). ICL allows LLMs to learn tasksby simply being presented with a few examples within their context window. A key bottleneckfor ICL has been the supported context length of LLMs, but with advancements in novel modelarchitectures, computational infrastructures, and efficient serving methods, state-of-the-art modelssuch as Gemini (Anthropic, 2024; Reid et al., 2024) feature context windows of millions of tokens areovercoming this limitation. Such long-context LLMs open unprecedented avenues for the scaling ofICL – whereas previous LLMs were limited to processing only up to dozens of examples, current LLMscan now accommodate significantly more examples. More importantly, beyond merely supporting alonger context, it has also been shown that scaling more examples led to substantial performanceimprovements across tasks, creating a new promising paradigm known as many-shot learning (Agarwalet al., 2024; Bertsch et al., 2024).\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(62.01300048828125, 633.9342651367188, 534.7378540039062, 760.583984375)\n",
      "Text:\n",
      "Despite these advances, as a nascent paradigm, many-shot ICL still faces several challenges. Longcontext windows, while powerful, are computationally expensive and introduce significant latencyand cost to serving, making it impractical or uneconomical to fully exploit the maximum contextlength. Some trade-off decisions have to be made under virtually any realistic setting. To leveragethe expanded context while controlling the cost and latency under an acceptable limit, existing workstypically investigate the experimental setting whereas many examples as costs permit are simplyrandomly sub-sampled from the pool of all available examples and dumped into the context window.As observed both in prior works (Agarwal et al., 2024) and our investigations (Fig. 1), using thesame number of examples but with different combinations of examples as demonstrations can lead to\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(62.07500076293945, 775.86279296875, 471.6581115722656, 810.0433349609375)\n",
      "Text:\n",
      "Corresponding authors: {xingchenw, soarik}@google.comExpanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 1\n",
      "Position: Rect(10.940000534057617, 267.1500244140625, 37.619998931884766, 604.8900146484375)\n",
      "Text:\n",
      "arXiv:2502.00330v1  [cs.LG]  1 Feb 2025\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 533.1221923828125, 181.91946411132812)\n",
      "Text:\n",
      "dramatically different performance for the same task. Across different tasks, it has also been noted thatthe model behaves very differently when the number of examples is scaled up, with some showinga near-monotonic increase in performance as more examples are added, while others experienceperformance plateaus (e.g., gray line in the leftmost subfigure of Fig. 1) or even degradation (e.g., redline in the rightmost subfigure of Fig. 4). Understandably, such variability could pose challenges forpractitioners and present obstacles to the application of many-shot learning as an effective paradigmin practice.\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(62.36199951171875, 184.38426208496094, 534.4292602539062, 581.6204833984375)\n",
      "Text:\n",
      "To address these, this paper aims to answer key research questions and proposes an effectivenovel approach. First, we analyze the factors driving the many-shot ICL in the reinforced ICL setupcommon in challenging reasoning tasks where we are provided with a labeled set of inputs andfinal labels, but the intermediate reasoning path has to be model-generated. We find that while ICLperformance often increases with the number of shots, that improvement can often be at least partiallyattributed to a much smaller subset of examples that highly disproportionately contribute to theoverall task performance – as we scale the number of examples, the probability of including theseexamples also increases. In many cases, if, however, we judiciously isolate these influential examplesfrom the rest, the “many-shot” performance can be matched or even exceeded with this sometimesextremely small subset of well-chosen examples alone while adding more examples beyond this setoften provides little benefit or even harms performance. We also argue that the findings explain someof the phenomena observed. For example, uneven influence can lead to high variance across differentcombinations of examples, whereas plateauing performance may occur when we run out of goodexamples with positive performance influences. One natural implication of these is the efficiencygains by reducing redundancy in many-shot ICL and identifying the optimized subsets. However,the natural next question to ask is whether scaling ICL examples in LLMs can still be beneficialafter using up all beneficial examples identified in the previous step. We answer affirmatively tothis: to still leverage LLMs’ long context, these optimized, high-performing examples may serve asdemonstrations to re-generate the more effective reasoning paths rationales on the train set back intothe many-shot regime, which we find to often outperform both the original many-shot examples andthe optimized examples themselves. Building on these insights, we propose Bayesian Refinement andIterative Demonstration Generation for Examples (bridge), a search algorithm based on Bayesianoptimization to improve many-shot ICL and bridges the few- and many-shot learning paradigms byautomating the “optimize” and “generate” steps above iteratively. In the “optimize” step, it framesthe problem as a combinatorial optimization task to discover the optimal set of demonstrations (i.e.,many-to-few), and in the “generate” step, it uses the optimal set as seed examples to generate moreexamples for further performance enhancement (i.e., few-to-many). We demonstrate the effectivenessof bridge on Gemini, Mistral, and Claude models across a diverse range of tasks, including symbolicreasoning, numerical reasoning, and text-to-SQL generation.\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(62.36199951171875, 603.4277954101562, 430.81640625, 616.3792114257812)\n",
      "Text:\n",
      "2. What Drives Many-Shot In-Context Learning Performance?\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(62.36199951171875, 625.5313110351562, 533.1256713867188, 751.7835083007812)\n",
      "Text:\n",
      "Several previous studies on many-shot ICL (Agarwal et al., 2024; Bertsch et al., 2024) have investigatedthe presence of performance gains when we scale the number of examples. A key question that remainsunanswered, though, is what exactly leads to this improvement. For example, it is unknown whetherthe benefit is from scaling examples itself due to expanded knowledge in the context via moreexamples or because including more examples increases the probability of selecting a small subset ofdisproportionately positive examples, or a combination of the above with some task specificity. We arguethat answering this question is critical – if the benefit comes from expanded knowledge from includingmore examples, it suggests that scaling and addressing long-context understanding challenges woulddominate the end-to-end performance improvements, and future studies should aim to either include\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 2\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "2\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(79.70075225830078, 169.2661590576172, 197.14938354492188, 193.04417419433594)\n",
      "Text:\n",
      "046# Demos0.5\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(79.76533508300781, 142.73196411132812, 94.64557647705078, 155.25616455078125)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(80.05596923828125, 116.19776153564453, 94.6468734741211, 128.7219696044922)\n",
      "Text:\n",
      "0.7\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(64.80715942382812, 119.41024017333984, 77.33135986328125, 156.66250610351562)\n",
      "Text:\n",
      "Val. Acc\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(108.56996154785156, 88.52005004882812, 186.47630310058594, 98.53941345214844)\n",
      "Text:\n",
      "logical_deduction (7)\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(213.7528533935547, 177.78590393066406, 312.35693359375, 193.04417419433594)\n",
      "Text:\n",
      "043# Demos\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(195.0872039794922, 142.45635986328125, 209.96743774414062, 154.98056030273438)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(195.1517791748047, 110.53992462158203, 209.9700164794922, 123.06412506103516)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(228.1866912841797, 88.52005004882812, 297.5120544433594, 98.53941345214844)\n",
      "Text:\n",
      "geometric_shapes\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(310.4090576171875, 169.2661590576172, 427.63427734375, 193.04417419433594)\n",
      "Text:\n",
      "042# Demos0.6\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(310.6996765136719, 136.7021484375, 325.29058837890625, 149.22634887695312)\n",
      "Text:\n",
      "0.7\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(310.4736328125, 104.13813781738281, 325.2918701171875, 116.66233825683594)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(342.69586181640625, 88.52005004882812, 413.61676025390625, 98.53941345214844)\n",
      "Text:\n",
      "disambiguation_qa\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(459.4417724609375, 113.45391845703125, 482.5805358886719, 123.47328186035156)\n",
      "Text:\n",
      "Top-K\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(459.4417724609375, 126.1210708618164, 496.4191589355469, 136.14044189453125)\n",
      "Text:\n",
      "Bottom-K\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(459.4417724609375, 138.78822326660156, 521.3518676757812, 148.80758666992188)\n",
      "Text:\n",
      "Overall trendline\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(459.4417724609375, 151.45538330078125, 494.82366943359375, 161.47474670410156)\n",
      "Text:\n",
      "Reinf. ICL\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(459.4417724609375, 164.12252807617188, 505.9507751464844, 174.1418914794922)\n",
      "Text:\n",
      "All examples\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(62.36199951171875, 203.83226013183594, 534.3156127929688, 330.0834655761719)\n",
      "Text:\n",
      "Figure 1 | It does not always take “many shots” to achieve many-shot performance – with judiciousselection, it is possible to match or exceed many-shot performance achieved by using all available examples)with much fewer examples: Accuracy on held-out splits against the number of examples on 3 BBHtasks of 1) overall trendline (fitted with locally weighted smoothing (lowess)), 2) using top-Kmost positive examples, or 3) using bottom-K least positive examples based on the ranking of theimportance score described in Sec 2. Dotted lines refer to two many-shot baselines: reinforced ICL:using input, model-generated reasoning and output of all correctly-predicted inputs; All example:using all available input-output pairs from the train set. Lines and error bars show mean ± standarddeviation across 3 runs with the ordering of the examples shuffled each trial.\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(62.36199951171875, 347.1742858886719, 532.91357421875, 405.68048095703125)\n",
      "Text:\n",
      "as many examples as practically possible or to imitate the behavior of the LLM as if many examplesare included. If, on the other hand, the performance is dominated by a small effective subset ofexamples, more intelligent selection aiming to reduce redundancies and identify the high-performingsubsets should outweigh näively scaling examples.\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(62.36199951171875, 408.14630126953125, 533.1259155273438, 534.3974609375)\n",
      "Text:\n",
      "Prior work on few-shot setup has studied related problems such as the sensitivity to examples inthe context (Zhao et al., 2021; Zhou et al., 2024b). However, it is presently unknown to what extentthe findings still scale to the many-shot ICL setup because 1) in the many-shot setup, the influenceof each individual example would get much smaller, and 2) it is unknown whether careful exampleselection in the few-shot setup is still necessary if all examples can be included in the context, sinceby definition, any high-performing examples are subsets of all examples – if the long-context LLMis perfectly capable of identifying the most relevant pieces of information. If so, aside from otherpractical concerns like cost and latency, the need for users to manually curate examples may no longerbe required.\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(61.41299819946289, 536.86328125, 533.8192138671875, 757.95947265625)\n",
      "Text:\n",
      "Setup. We aim to shed insights on these important questions. We use the Gemini 1.5 Pro (Reidet al., 2024), the state-of-the-art long-context model, to focus on several representative tasks fromthe BBH tasks. All three tasks, as shown in by the gray lines in Fig. 1, benefit from increasing numberof examples to varying degrees (in logical_deduction, the performance initially increases withthe number of examples before plateauing and decreasing; in the other two tasks, there is a noisy butnear monotonic improvement throughout) – we will test the key findings in a much more extensivecollection of tasks in Sec. 4. Given the increased emphasis of modern LLMs on problem-solving andreasoning, we primarily focus on these tasks and adopt the reinforced ICL (Agarwal et al., 2024)setup, where we assume the availability of a labeled set of inputs and final labels to be used asmany-shot demonstrations, whereas any intermediate outputs or rationales leading to the finalanswer are model-generated and modifiable (although we also conduct preliminary experimentsin alternative setups such as low-resource machine translation in App. C.4). Lastly, we primarilyfocus on the tasks with the number of available labeled data up to 150-200 samples – while modernLLMs can often accommodate even more examples in the context, we focus on this range because1) we believe it is the most practically relevant and fills an important gap that neither few-shot ICLnor supervised (parameter-efficient) fine-tuning (which usually requires hundreds to thousands of\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 3\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "3\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 533.1240844726562, 141.27145385742188)\n",
      "Text:\n",
      "examples) conventionally address, and 2) while possible and of academic value, scaling beyondthis range typically starts incurring significant latency and computational overhead, which scalesquadratically w.r.t the input length for exact attention and is thus often practically less desired formost real-world use cases.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(61.9370002746582, 143.73728942871094, 534.4287109375, 271.1664733886719)\n",
      "Text:\n",
      "Many-shot performance can still be driven by few high-performing examples. A key test thatwould distinguish and disentangle the two possible sources of benefits from scaling mentioned atthe beginning of this section is whether we can attribute, at least to a large extent, the performanceimprovement from scaling examples back to a carefully selected, high-performing subset of exampleswith disproportionate influence. Formally, given a set of examples E = {𝑒𝑗}𝑚𝑗=1 and a performancemetric to be maximized 𝑔(·) : P(E) →ℝ(in this case, the accuracy on the validation set In this setup,the goal is to find whether we can construct a subset e∗= {𝑒∗𝑖}𝑛𝑖=1 ⊂E, s.t.𝑛≪𝑚such that 𝑔(e∗) ismuch better than a randomly selected set of examples e of similar size and/or can even be comparableor better than using the full set of examples 𝑔(E) in the context.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(429.1680908203125, 405.9056701660156, 526.88671875, 421.15142822265625)\n",
      "Text:\n",
      "064\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(406.3281555175781, 375.38079833984375, 424.4419250488281, 390.6265563964844)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(406.40679931640625, 335.07330322265625, 424.4450988769531, 350.3190612792969)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(395.3410339355469, 337.1418762207031, 407.5376281738281, 375.81475830078125)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(434.92181396484375, 295.1700134277344, 519.3118286132812, 307.3666076660156)\n",
      "Text:\n",
      "geometric_shapes\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(428.81695556640625, 517.7993774414062, 525.5800170898438, 536.373291015625)\n",
      "Text:\n",
      "051# Demos\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(406.3281555175781, 482.4519348144531, 424.4419250488281, 497.69769287109375)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(406.40679931640625, 432.4994201660156, 424.4450988769531, 447.74517822265625)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(395.3410339355469, 449.0355224609375, 407.5376281738281, 487.7084045410156)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(433.9325256347656, 416.3880920410156, 520.2647094726562, 428.5846862792969)\n",
      "Text:\n",
      "disambiguation_qa\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(390.84100341796875, 548.1552734375, 534.6494140625, 687.9555053710938)\n",
      "Text:\n",
      "Figure 2 | Good demonstrationslead to better re-generated ex-amples: trendlines between ac-curacy and # examples; notethat the re-generated exam-ples by using top-5 examplessets as demonstrations outper-form the original examples(gray line) by at all parts of thecurve.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(61.9370002746582, 273.63226318359375, 378.540283203125, 673.3195190429688)\n",
      "Text:\n",
      "Whereas a conclusive test would involve enumerating and eval-uating 𝑔(·) on the power set of E with |P(E)| = 2| E|, it is clearlycomputationally intractable, and a natural simplification is whetherwe can rank the individual examples in E with some importancescoring function 𝑀(𝑒) to construct example subsets based on theexample ranking. While many possible formulations of this are pos-sible, here we define 𝑀(𝑒) based on imputed input gradient, whichis a concept used in interpretable machine learning for importanceattribution (Samek et al., 2021; Selvaraju et al., 2017; Simonyan,2013; Sundararajan et al., 2017). In our context, directly com-puting input gradient is impossible as we only assume black-boxLLMs without gradient backpropagation and 𝑔(·) is not necessarilydifferentiable. To bypass these issues, we use a sample-efficientGaussian process regressor (GPR) (Williams and Rasmussen, 1995,2006) to approximate 𝑔(·) with ˆ𝑔(·), whose input gradient ∇eˆ𝑔(𝑒)is analytically available: we first randomly sample 𝑛subsets of E togive e1:𝑛= [e1, ..., e𝑛], where each subset of examples is representedas a 𝑚-dimensional binary column vector e𝑖∈{0, 1}𝑚with e( 𝑗)𝑖= 1if the 𝑗-th example is present or 0 otherwise; we then evaluate theperformance metric of each e𝑖to obtain g1:𝑛= [𝑔(e1), ..., 𝑔(e𝑛)]. Wethen compute and average the input gradient w.r.t. each possible{𝑒𝑗}𝑚𝑗=1 ∈E to obtain an approximated marginalized importance ofeach example in E1. Finally, we sort the examples based on 𝑀(𝑒)and construct subsets at regular interval from size 1 to |E| in both as-cending and descending directions. Formally, we order {𝑒𝑖}𝑛𝑖=1 suchthat 𝑀(𝑒1) ≤𝑀(𝑒2) ≤... ≤𝑀(𝑒𝑛); the ascending and descendingsets of size 𝑡∈[1, |E|] are given by a𝑡= e1:𝑡and d𝑡= e𝑛−𝑡:𝑛respec-tively. We then evaluate 𝑔(·) on these sets and show the results inFig. 1.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(61.457000732421875, 675.7843017578125, 532.9118041992188, 734.7033081054688)\n",
      "Text:\n",
      "As shown, while the gray lines (overall trend lines) often show apositive correlation between performance and an increasing numberof examples, we also observe often large gap between the green(top-𝑘examples) and the red (bottom-𝑘examples) lines, suggesting that different sampling strategies\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(73.95999908447266, 738.146240234375, 419.3858642578125, 754.0238037109375)\n",
      "Text:\n",
      "1We refer the readers to App. A for detailed derivation of the input gradient-based score.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 4\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "4\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(61.457000732421875, 82.76628112792969, 533.3374633789062, 371.60748291015625)\n",
      "Text:\n",
      "can lead to performance differences that far outweigh the effect from naïve scaling – e.g., if we establishan “exchange rate” between different example sets based on their imputed ordering, we can observethat including around top-10 examples (green lines) examples is as effective as or more effective thanthe set containing bottom-30 examples in geometric_shapes. More importantly, in both cases weobserve that the green lines, which represent an intelligent selection strategy more sophisticated thanrandom sampling, plateau far before the gray line, suggesting that it is possible to achieve comparableperformance with a much smaller number of examples: in disambiguation_qa, we find that usingfewer than 20 top examples is almost already as good as using all 42 examples whereas subsequentadditions only led to a few percent of gain, possibly within the margin of error with reshuffling(denoted by error bars on the figure). In the other tasks, we find the performance to peak muchearlier and adding more examples to the context actually led to performance deterioration. The resultssuggest 1) the fact that it is possible to match or outperform using all examples with fewer, carefullyselected examples means that intelligent example selection is still relevant even with many-shotICL, echoing findings from the recent works (Li et al., 2024b) that retrieval remains valuable forlong-context models in the RAG setup; and 2) naïvely including as many examples as possible canbe suboptimal both in terms of computing cost and performance – while it is trivially true for thetasks whose performance does not improve monotonically with the number of examples, we showthat it can even be true when it apparently does: e.g., on geometric_shapes, the near monotonicimprovement overall trend (gray line) may lead someone to conclude that it is beneficial to include asmany examples as possible, even though the green line representing intelligent selection saturatesand starts to decline earlier.\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(61.9370002746582, 374.0732727050781, 534.4330444335938, 513.87451171875)\n",
      "Text:\n",
      "Can we still benefit from scaling examples? Experiments above demonstrated the presence ofredundancy in many-shot ICL, revealing that using a smaller subset of examples can often reducethis redundancy without sacrificing performance. It is, however, a pruning operation that necessarilyreduce the input tokens consumed. This leads to a natural question: can we still benefit from scalingthrough expanding? For this question, it is important to recognize that under the reinforced ICL setup,while the inputs and labels in many-shot setups are fixed, the model-generated intermediate outputs,which represent reasoning paths, are modifiable. Given that these intermediate roles are shown to playa critical role in steering model behaviors (Wan et al., 2024), it is possible that examples previouslyidentified as non-important or non-beneficial may be again beneficial if the model-generated rationalescan be improved.\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(62.36199951171875, 516.3402709960938, 532.9144897460938, 588.3955078125)\n",
      "Text:\n",
      "To achieve so, we reuse the optimized example set from the previous steps as “seed” demonstrationsfor LLMs to re-generate the examples on the train set, the same set from which the optimized examplesare generated. As shown by Fig. 2 where we use an example set of different sizes as the seeds, theregeneration step not only increases the number of shots available but also results in better performanceacross the accuracy versus number-of-demonstrations trade-off.\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(62.36199951171875, 610.2027587890625, 158.4484405517578, 623.1541748046875)\n",
      "Text:\n",
      "3. Methodology\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(62.03499984741211, 632.3062744140625, 534.431884765625, 758.5574951171875)\n",
      "Text:\n",
      "The findings presented above highlight a significant need for improvements that extend beyondsimply increasing the number of examples straightforwardly. Instead, identifying the most usefulexample subset e∗is crucial both for effective cost-performance trade-offs and for better reasoningpath generation for more effective examples. Based on these insights, we propose Bayesian Refinementand Iterative Demonstration Generation for Examples, or bridge in short (described in Algorithm 1and depicted in Fig. 3, an optimization algorithm aiming to enhance many-shot ICL with intelligentexample selection and iterative example generation. At a high level, the outer loop of bridge isstructured in two alternating steps of “optimize” and “generate”. In the “optimize” step, the algorithmfocuses on discovering the optimal subset of examples e∗via a carefully designed (for low complexity,\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 5\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(453.8629150390625, 100.14226531982422, 500.11669921875, 117.73667907714844)\n",
      "Text:\n",
      "Evaluateperformance\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(177.2393341064453, 169.85549926757812, 183.49774169921875, 174.64590454101562)\n",
      "Text:\n",
      "❌\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(458.750244140625, 200.05889892578125, 478.2801208496094, 207.84671020507812)\n",
      "Text:\n",
      ", 84% )\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(458.750244140625, 220.25991821289062, 478.2930603027344, 228.0477294921875)\n",
      "Text:\n",
      ", 45% )\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(458.750244140625, 210.4852294921875, 477.8108825683594, 218.27304077148438)\n",
      "Text:\n",
      ", 75% )\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(125.60660552978516, 115.78174591064453, 196.90110778808594, 123.60148620605469)\n",
      "Text:\n",
      "Q1A1\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(389.573974609375, 165.3068084716797, 416.5403137207031, 173.12655639648438)\n",
      "Text:\n",
      "A3Q3\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(305.4608154296875, 145.7574462890625, 374.1833190917969, 153.5771942138672)\n",
      "Text:\n",
      "A1Q1A3Q3\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(390.17474365234375, 125.55643463134766, 415.85400390625, 133.3761749267578)\n",
      "Text:\n",
      "A1Q1\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(221.2763214111328, 112.05480194091797, 265.48388671875, 118.57125854492188)\n",
      "Text:\n",
      "Set of available\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(224.91128540039062, 119.87454986572266, 261.8399963378906, 126.39100646972656)\n",
      "Text:\n",
      "examples\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(176.5876922607422, 130.10511779785156, 182.84609985351562, 134.89552307128906)\n",
      "Text:\n",
      "❌\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(149.99241638183594, 196.58580017089844, 164.72479248046875, 204.40554809570312)\n",
      "Text:\n",
      "LLM\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(125.04660034179688, 135.33111572265625, 135.60325622558594, 143.15086364746094)\n",
      "Text:\n",
      "Q2\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(125.00587463378906, 154.88047790527344, 135.63290405273438, 162.70022583007812)\n",
      "Text:\n",
      "Q3\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(124.78186798095703, 174.4298553466797, 135.86244201660156, 182.24960327148438)\n",
      "Text:\n",
      "Q4\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(124.89387512207031, 193.9792022705078, 135.73985290527344, 201.7989501953125)\n",
      "Text:\n",
      "Q5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(124.9244155883789, 213.52857971191406, 135.72348022460938, 221.34832763671875)\n",
      "Text:\n",
      "Q6\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(188.19512939453125, 135.33111572265625, 197.5475311279297, 143.15086364746094)\n",
      "Text:\n",
      "A2\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(188.16458129882812, 154.88047790527344, 197.58737182617188, 162.70022583007812)\n",
      "Text:\n",
      "A3\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(187.94058227539062, 174.4298553466797, 197.81690979003906, 182.24960327148438)\n",
      "Text:\n",
      "A4\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(188.05258178710938, 193.9792022705078, 197.69430541992188, 201.7989501953125)\n",
      "Text:\n",
      "A5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(188.0729522705078, 213.52857971191406, 197.6677703857422, 221.34832763671875)\n",
      "Text:\n",
      "A6\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(250.10147094726562, 135.33111572265625, 258.1557922363281, 143.15086364746094)\n",
      "Text:\n",
      "A1\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(249.4192657470703, 154.88047790527344, 258.8420715332031, 162.70022583007812)\n",
      "Text:\n",
      "A3\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(249.30726623535156, 174.4298553466797, 258.9490051269531, 182.24960327148438)\n",
      "Text:\n",
      "A5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(249.32763671875, 193.9792022705078, 258.9224548339844, 201.7989501953125)\n",
      "Text:\n",
      "A6\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(389.4925537109375, 145.7574462890625, 416.62066650390625, 153.5771942138672)\n",
      "Text:\n",
      "A6Q6\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(347.10504150390625, 165.3068084716797, 357.9510498046875, 173.12655639648438)\n",
      "Text:\n",
      "Q5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(385.8270263671875, 216.96975708007812, 411.7560119628906, 223.4862060546875)\n",
      "Text:\n",
      "Bayesian\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(384.07574462890625, 226.74444580078125, 413.51708984375, 233.26089477539062)\n",
      "Text:\n",
      "Optimizer\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(489.3775634765625, 145.97230529785156, 501.75885009765625, 153.76011657714844)\n",
      "Text:\n",
      "75%\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(489.13323974609375, 164.87002563476562, 501.98370361328125, 172.6578369140625)\n",
      "Text:\n",
      "84%\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(232.4764862060547, 135.33111572265625, 241.89144897460938, 143.15086364746094)\n",
      "Text:\n",
      "Q1\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(231.87574768066406, 154.88047790527344, 242.50279235839844, 162.70022583007812)\n",
      "Text:\n",
      "Q3\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(231.7637481689453, 174.4298553466797, 242.6097412109375, 182.24960327148438)\n",
      "Text:\n",
      "Q5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(231.79429626464844, 193.9792022705078, 242.59336853027344, 201.7989501953125)\n",
      "Text:\n",
      "Q6\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(176.5876922607422, 110.39103698730469, 182.84609985351562, 115.51087951660156)\n",
      "Text:\n",
      "✅\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(176.5876922607422, 149.48976135253906, 182.84609985351562, 154.60960388183594)\n",
      "Text:\n",
      "✅\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(176.5876922607422, 188.5885009765625, 182.84609985351562, 193.70834350585938)\n",
      "Text:\n",
      "✅\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(176.5876922607422, 208.13787841796875, 182.84609985351562, 213.25772094726562)\n",
      "Text:\n",
      "✅\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(364.6485595703125, 165.3068084716797, 374.290283203125, 173.12655639648438)\n",
      "Text:\n",
      "A5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(432.969482421875, 199.4220733642578, 457.14227294921875, 217.01651000976562)\n",
      "Text:\n",
      "1 1 0 10 1 1 0\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(489.13323974609375, 125.11964416503906, 501.9967041015625, 132.90745544433594)\n",
      "Text:\n",
      "45%\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(428.7745361328125, 201.33026123046875, 430.8793640136719, 207.84671020507812)\n",
      "Text:\n",
      "(\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(428.7745361328125, 211.756591796875, 430.8793640136719, 218.27304077148438)\n",
      "Text:\n",
      "(\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(428.7745361328125, 221.53128051757812, 430.8793640136719, 228.0477294921875)\n",
      "Text:\n",
      "(\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(331.22113037109375, 191.3726348876953, 455.92242431640625, 208.3154296875)\n",
      "Text:\n",
      "UpdateSuggest New\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(460.82733154296875, 161.39694213867188, 475.55975341796875, 169.21669006347656)\n",
      "Text:\n",
      "LLM\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(229.80880737304688, 236.33616638183594, 469.427734375, 253.93060302734375)\n",
      "Text:\n",
      "Select argmaxA3Q3Q5A5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(72.2633056640625, 97.3777084350586, 314.2342834472656, 108.45569610595703)\n",
      "Text:\n",
      "GenerateOptimize\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(102.30009460449219, 229.16807556152344, 182.4446258544922, 236.98782348632812)\n",
      "Text:\n",
      "Use as demonstrations\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(109.68202209472656, 238.94276428222656, 175.0550537109375, 246.76251220703125)\n",
      "Text:\n",
      "for the next round\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(309.207763671875, 119.6916275024414, 335.23968505859375, 127.51136779785156)\n",
      "Text:\n",
      "Sample\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(303.11895751953125, 107.31035614013672, 411.711669921875, 115.13009643554688)\n",
      "Text:\n",
      "Pool of evaluated example sets\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(433.9266357421875, 219.6230926513672, 457.14227294921875, 226.79119873046875)\n",
      "Text:\n",
      "1 0 0 0\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(104.21430206298828, 116.06841278076172, 108.19259643554688, 125.84309387207031)\n",
      "Text:\n",
      "1\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(213.69076538085938, 99.77727508544922, 218.97886657714844, 109.55195617675781)\n",
      "Text:\n",
      "2\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(289.23077392578125, 105.64208221435547, 443.4736633300781, 120.62992858886719)\n",
      "Text:\n",
      "34\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(418.14459228515625, 188.40106201171875, 423.7552490234375, 198.17575073242188)\n",
      "Text:\n",
      "5\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(90.51956939697266, 231.40966796875, 335.1033630371094, 244.44259643554688)\n",
      "Text:\n",
      "67\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(61.457000732421875, 271.80828857421875, 534.6476440429688, 479.3554382324219)\n",
      "Text:\n",
      "Figure 3 | Overview of bridge: With a labeled dataset D, exemplified with 6 samples, at the Generationphase (left half), we generate initial examples by performing LLM inference on the inputs of D (“Q1-6”) with zero-shot prompting to obtain the initial responses “A1-6”, which include any intermediateoutputs critical for ICL (Step 1). At Step 2, consistent with reinforced ICL in Agarwal et al. (2024),we filter the responses to retain the subset of D where the LLM predicted correctly to ensure theexamples include correct reasoning steps to build E𝑘, the pool of examples at round 𝑘which form thesearch space for the subsequent Optimize step. At the Optimize step (right half), we initialize theproposed Bayesian optimizer by randomly sampling subsets e(0) ⊆E𝑘as demonstrations to be Step 3evaluated on a held-out validation dataset (D can be reused for this purpose) to obtain a performancemetric Step 4. The Bayesian optimizer (BO) is then updated with binary vector representations of ethat led to this validation performance as input and the metric itself as output and suggests a newsubset of examples to be used as demonstrations for the next step Step 5; Steps 4-5 are repeated(inner loop) until the BO budget is exhausted, after which the best evaluated set e∗𝑘is returned (Step6). This set is then used as a demonstration to generate the example pool for the next round E𝑘+1(Step 7).\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(62.36199951171875, 496.4462890625, 534.6759643554688, 568.50146484375)\n",
      "Text:\n",
      "robustness to overfitting and budget control) Bayesian optimization algorithm that naturally leveragesthe GPR surrogate used in Sec. 2; in the “generate” step, bridge utilizes the optimized subset asseed demonstrations to align the model with the best-performing examples seen so far to re-generatenew reasoning paths as an integral part of more effective examples back to the many-shot regime toleverage the long context. The two steps are iteratively repeated to progressively refine the examples.\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(61.9370002746582, 570.9662475585938, 534.4285278320312, 737.8665161132812)\n",
      "Text:\n",
      "Optimize step. While effective, directly using the importance scoring approach from Sec. 2 to identifythe e∗would require us to set the optimal number of examples to select ||e∗|| as a hyperparameter,the optimal value of which is task specific. Furthermore, a key motivation for the importance-basedranking in Sec. 2 is to attribute performance to individual examples; this is, however, not required ifwe simply would like to find an optimal subset e∗. To nevertheless use the GPR surrogate in Sec. 2which has shown an impressive sample-efficient, modeling capability, we propose to use Bayesianoptimization (BO) (Frazier, 2018; Garnett, 2023), a sample-efficient black-box optimization algorithmthat has recently shown promise in combinatorial problems (Daulton et al., 2022; Wan et al., 2021);it naturally synergizes with the GP surrogate yet automatically strikes a balance between explorationand exploitation to discover e∗without requiring us to set ||e∗|| beforehand, although bridge isalso compatible with alternative methods as a drop-in replacement of the “Optimize” step, which weinvestigate in detail in App. C.1.\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(79.29900360107422, 740.331298828125, 534.4326171875, 758.1895141601562)\n",
      "Text:\n",
      "Instead of consuming the entire query budget by sampling randomly, as illustrated by Algorithm 2,\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 6\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "6\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(62.00199890136719, 98.77928161621094, 170.80873107910156, 116.63748168945312)\n",
      "Text:\n",
      "Algorithm 1 bridge.\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(65.2760009765625, 114.66275787353516, 299.07550048828125, 381.395751953125)\n",
      "Text:\n",
      "1: Input: train set Dt, validation set Dv (can be the sameas the train set), number of iteration rounds 𝐾∈ℕ(outer-loop), evaluation budget for BO per iteration 𝑛eval (inner-loop).2: Output: Optimized set of examples E∗.3: [Generate] Generate the pool of initial examples E0by predicting the LLM on the train set with zero-shotprompting or few-shot prompting (if handwritten few-shot demonstrations are available). Each instance in E0is a concatenation of {input, model-generated reasoning,final outputs} for the subset of the train set where themodel obtained the correct prediction.4: for 𝑘∈{1, ..., 𝐾} (Outer loop) do5:[Optimize] Run Bayesian optimization (calling sub-routine Algorithm 2 on the validation set to obtaine∗𝑘←BayesOpt(𝑛eval=𝑛eval, E=E𝑘).6:[Generate] Re-generate examples E𝑘by re-predictingthe LLM on the train set, but with the optimized exam-ples e∗𝑘from the previous step as demonstrations; the{inputs, model-generated reasoning, output}-tuples areconcatenated to form the new set of examples E𝑘forthe next [Optimize] step.7: end for8: return Optimized example set E∗after 𝐾rounds.\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(311.3280029296875, 95.74626159667969, 532.9134521484375, 127.15347290039062)\n",
      "Text:\n",
      "Algorithm 2 Budget-controlled BO subroutinewith random scalarization (BayesOpt).\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(311.75396728515625, 125.1077651977539, 534.4110107421875, 382.46673583984375)\n",
      "Text:\n",
      "1: Input: Evaluation budget for BO per iteration 𝑛eval(inner-loop), full set of available samples E, numberof random initializations 𝑛init = min(16, 𝑛eval/2).2: Output: Optimized set of examples e∗⊆E𝑡.3: Randomlygenerate𝑛initsubsetse1:𝑛init:={e1, ..., e𝑛init}witheache∼{0, 1}| E𝑡|s.t.|e| ∼Uniform(1, |E𝑡|).4: Evaluate g1:𝑛init = [𝑔(e1, ..., e𝑛init]⊤and fit a GP one1:𝑛init as inputs and g1:𝑛init as outputs. Set D0 ←{e1:𝑛init, g1:𝑛init}5: for 𝑡∈{𝑛init, ..., 𝑛eval} (Inner loop) do6:Sample a random scalarization value𝛽𝑡∼Uniform(0, 1) and compute the scalarized objec-tive of this iteration ℎ𝑡(e) = TCH(𝛽𝑡, [𝑔(e), |e|]).7:Compute h1:𝑡for all previously evaluated pointsD𝑡−1, fit a GPR GP𝑡on [e1:𝑡, h1:𝑡] and obtain thenext configuration to evaluate by maximizing theacquisition function 𝛼(·): e𝑡= arg maxe⊆E 𝛼(e |GP𝑡).8:Evaluate 𝑔(·) with et and augment D𝑡←D𝑡−1 ∪(et, 𝑔(e𝑡))9: end for10: return e∗= arg maxe∈D 𝑔(e).\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(61.86000061035156, 403.4783020019531, 534.7354125976562, 719.4185180664062)\n",
      "Text:\n",
      "BO only requires some initializing samples to warm-start (Step 3). Afterward, it guides explorationby iteratively (re)fitting a GPR with the previously observed inputs and outputs so far. Formally, atiteration 𝑡∈[1, 𝑇], we have evaluated 𝑔(·) 𝑡times at e1:𝑡= [e1, ..., e𝑡]⊤with observed values g1:𝑡.Whereas a straightforward application of BO would directly train a GP on [e1:𝑡, g1:𝑡] as inputs-outputsand perform BO with 𝑔(·) as the objective function directly, a subtle but important distinction here isthat our goal is to identify a subset e∗that, when used as demonstrations on the train set, generates tothe most effective examples on the validation set, rather to simply find the highest-performing e∗onthe validation set. While we expect the two objectives to be correlated (i.e., e that led to high validationperformance is also likely to generate better samples on the train set), we also empirically find it isdesirable to encourage e∗to have a smaller cardinality akin to a ℓ0 regularization to reduce overfittingon the validation set and to discourage memorization in subsequent generations from the previousexample set E𝑡−1 of which e∗is a subset. To achieve so, we augment the performance maximizationmax 𝑔(e) with a sparsity objective which counts the number of non-zero elements in e: min Í𝑗𝑒( 𝑗) –this transforms the problem into a bi-objective optimization problem , where instead of maximizingfor the validation performance only, we also encourage sparsity as regularization. Practically, we solvethe problem with random scalarization (Knowles, 2006; Paria et al., 2020). Specifically, as hinted inStep 7 of Algorithm 2, at each BO iteration, we first sample a random scalar 𝛽𝑡∼Unif(𝛽LB, 𝛽UB) thatdetermines the weight of the performance objective 𝑔(·) of the 𝑡-th BO iteration (the weight of thesparsity objective is given by 1 −𝛽𝑡) and {𝛽LB, 𝛽UB} denote the lower and upper bounds of the weightfor 𝑔(·) which are set to {0.25, 1} by default. With this 𝛽𝑡, we then aggregate the vector objective[𝑔(e), Í𝑗𝑒( 𝑗)] back to a scalar ℎ𝑡(e) via Tchebyshev scalarization (TCH), a theoretically well-foundedscalarization scheme common in multi-objective optimization (Bowman Jr, 1976; Chugh, 2020;Steuer and Choo, 1983) given by:\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(148.11000061035156, 728.5752563476562, 422.03778076171875, 746.4334716796875)\n",
      "Text:\n",
      "ℎ𝑡(e) = maxn𝛽𝑡\u0000𝑔(e) −max{𝑔(e1), ..., 𝑔(et)}\u0001, −(1 −𝛽𝑡)∑︁\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(414.27703857421875, 728.5752563476562, 533.818603515625, 754.4627075195312)\n",
      "Text:\n",
      "𝑗𝑒( 𝑗)o,(1)\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 7\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "7\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(61.9370002746582, 82.76628112792969, 534.3098754882812, 181.91946411132812)\n",
      "Text:\n",
      "where the minus sign before the last term is to cast the sparsity objective as maximization. We optfor random scalarization that differs from step to step instead of a fixed scalarization weight or anyhard constraint on Í𝑗𝑒( 𝑗) to retain the flexibility of exploring the entire Pareto front since the exactrelation between the number of samples and performance can differ across tasks. Since 𝛽𝑡is in generaldifferent for each 𝑡, we then compute h𝑡= [ℎ𝑡(e1), ..., ℎ𝑡(e𝑡)] on previously evaluated outputs and fita GP on H𝑡:= [e1:𝑡, h𝑡], which induces a Gaussian posterior predictive distribution with mean andvariance at any e ⊆E (we use ˆℎ𝑡to denote that it is the GP approximation of the actual function ℎ𝑡):\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(116.79499816894531, 184.2612762451172, 533.8185424804688, 205.0503387451172)\n",
      "Text:\n",
      "𝔼ˆℎ𝑡(e)|H𝑡[ˆℎ𝑡(e)] = k𝑡(K + 𝜂2I)−1h𝑡, 𝕍ˆℎ𝑡(e) |H𝑡[ˆℎ𝑡(e)] = 𝑘(e, e) −k𝑡(K + 𝜂2I)−1k⊤𝑡,(2)\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(61.9370002746582, 208.71726989746094, 533.1018676757812, 309.6604309082031)\n",
      "Text:\n",
      "where k𝑡= [𝑘(e, e1), ..., 𝑘(e, e𝑡)] and 𝑘(·, ·) is the covariance function of the GP (we use Matern 2.5by default) which measures the similarity between two inputs – in our case, it is a function of thenumber of overlapping examples between two subsets of examples e, e′ ⊆E. To select the nextconfiguration to evaluate e𝑘, the BO optimizes an acquisition function, another key component ofBO that automatically trade-off exploration and exploitation. At each inner-loop BO iteration, wechoose the maximizer of the expected improvement (EI) (Zhan and Xing, 2020) for the next iteratione𝑡: e𝑡= arg maxe⊆E 𝛼(e) = arg maxe⊆E 𝔼ˆℎ𝑡(e) |H𝑡\u0002max{0, ˆℎ𝑡(e) −max𝑡′∈{1,𝑡} ˆℎ𝑡(et′)}\u0003.\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(61.97999954223633, 310.3363037109375, 532.9168090820312, 355.6823425292969)\n",
      "Text:\n",
      "Generate step. At each outer-loop round 𝑘∈{1, ..., 𝐾}, given the optimized e∗𝑘as demonstrations, weregenerate and replace the example pool with the correct predictions and their generated rationalesE𝑘←𝑓LLM(D𝑡, e∗𝑘⊆E𝑘−1) for subsequent optimize step.\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(62.36198425292969, 376.009765625, 155.14581298828125, 388.9611511230469)\n",
      "Text:\n",
      "4. Experiments\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(61.41299819946289, 398.11328125, 534.6484985351562, 646.3074951171875)\n",
      "Text:\n",
      "Model and evaluation data. We conduct experiments on an extensive collection of tasks requir-ing a different set of skills task difficulty on two Gemini 1.5 models (gemini-1.5-pro-001 andgemini-1.5-flash-001) while also testing key findings on Mistral family of models: MistralNeMo (mistral-nemo-12b) and Mistral Large (mistral-large-2407), and Claude 3.5 Sonnet:1) BIG-Bench Hard (BBH) tasks encompassing a wide range of challenging numerical reasoning,commonsense problem-solving, logical deduction and tabular reasoning tasks – we particularly focuson the subset of 16 BBH tasks where the model performances have not saturated; 2) Hendryck’sMATH (Hendrycks et al., 2021), a challenging numerical reasoning dataset; 3) GSM-Hard (Gaoet al., 2022), a more challenging variant of the classical grade-school GSM-8K (Cobbe et al., 2021)with the numbers in the questions replaced with much larger and rarer ones. To further probe theutility of many-shot learning and bridge in coding tasks, we also experiment on 4) BIRD (Li et al.,2024a), a challenging large-scale text-to-SQL generation benchmark where the LLM has to generateSQLite programs from natural language instructions that are executed on real-world databases. Forall datasets, when official train-test split is not available, we randomly split the data into train andtest splits; unless stated otherwise, a single unified train split is used both for the generation ofdemonstrations and is reused for validation (i.e., the objective of the optimize step in Algorithm 1;the test splits are held-out and only used for evaluation of the algorithm. We refer the readers toApp. B for detailed descriptions, prompt templates, and evaluation protocols used.\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(62.36199951171875, 648.7732543945312, 533.8512573242188, 761.8688354492188)\n",
      "Text:\n",
      "Experimental setup. For all tasks, we run bridge with 𝐾= 3 rounds (i.e., the number of outer-loopiterations in Algorithm 1) and within each round, we allow for 𝑛eval = 32 evaluations on the validationset (i.e., the number of inner-loop iterations in Algorithm 2) and we report the results at the end ofeach “optimize” and “generate” steps to visualize the iteration process. For baselines, we consider 1)using all provided examples and we consider three variants: a) using query-target only without anygenerated rationales (Direct), b) first prompt the LLM to generate rationales and answers, and usethe concatenation of query-rationale-target as demonstrations, regardless of whether the rationale ledto the correct answer (CoT), and c) prompting the LLM with both the query and the final, ground-truth\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 8\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "8\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(68.52594757080078, 85.15727233886719, 519.6425170898438, 115.89788818359375)\n",
      "Text:\n",
      "TasksAllReinf.IterativebridgeDirectCoTInfillICLReinf.(Ours)# Iterations-000121o1g2o2g3o\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(68.52590942382812, 116.54530334472656, 526.4078369140625, 268.0978698730469)\n",
      "Text:\n",
      "causal_judgement61.04.762.72.168.02.866.34.868.71.969.32.768.31.562.71.659.71.572.00.070.02.0date_understanding87.22.086.02.394.81.888.82.593.01.094.91.392.21.597.00.794.81.995.01.295.51.8disambiguation_qa74.22.263.31.172.32.076.82.474.61.475.11.571.82.477.53.680.51.881.32.978.81.5dyck_languages16.82.939.03.724.52.955.53.664.45.374.43.649.22.776.23.880.02.777.51.176.83.8formal_fallacies82.83.786.81.384.32.886.21.188.10.989.41.486.02.185.02.590.82.390.82.888.22.3geometric_shapes69.04.161.84.273.52.380.22.881.02.582.31.778.52.182.53.689.23.892.31.189.20.8hyperbaton70.84.193.23.189.52.690.21.191.52.286.22.596.50.994.21.594.82.896.50.597.20.4logical_deduction (7)56.84.463.07.469.85.965.83.568.92.669.52.970.21.570.84.571.73.771.51.869.22.2movie_recommendation75.01.063.72.268.02.865.21.668.82.082.01.967.01.269.50.569.33.172.81.867.01.2multistep_arithmetic_two86.52.296.80.888.81.896.50.595.90.894.51.396.20.894.51.197.00.798.00.796.81.8object_counting92.52.384.84.395.31.395.50.995.82.295.11.696.20.496.01.994.51.194.20.495.00.7ruin_names85.23.185.52.189.81.689.81.988.61.590.50.990.81.188.81.789.21.588.82.490.30.8salient_translation_error_detection66.02.456.21.572.50.569.01.673.81.173.41.368.80.871.00.769.52.274.00.774.51.1snarks94.11.895.52.395.10.692.73.294.31.995.51.593.43.095.80.095.11.696.91.597.61.8sports_understanding93.81.394.21.395.00.793.01.494.10.995.41.292.81.997.01.296.20.895.80.495.80.8tracking_shuffled_objects (7)76.07.252.52.164.32.862.34.264.52.265.54.695.80.495.01.2100.00.097.00.799.50.5\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(68.52594757080078, 268.3761291503906, 524.5431518554688, 280.8905029296875)\n",
      "Text:\n",
      "Average74.2274.0678.7079.6181.6182.3782.1184.6185.7787.1386.33\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(61.9370002746582, 288.2962646484375, 534.6441650390625, 509.3924865722656)\n",
      "Text:\n",
      "Table 1 | Test accuracy of gemini-1.5-pro-001 on selected BBH tasks with different promptingapproaches. “All” refers to using the entire labeled set of 75 examples as demonstrations (“Direct”:using all input-final answer pairs without any model-generated content; “CoT”: using all input-rationale-final answer triplet, where the rationale is model-generated; “Infill”: using all input-rationale-final answer triplet, where the rationale is filled in by prompting the model to generate the intermediatesteps given the inputs and ground-truth answers); “Reinf. ICL” refers to reinforced many-shotICL where we include the subset of train set that the LLM answered correctly under zero-shot asdemonstrations; “Iterative Reinf.” refers to the iterative variant of reinforced many-shot ICL where wedirectly use all the generated correct examples from the previous round as demonstrations for the nextround without the optimize step, and the different columns of bridge show the evolution of testaccuracy at different milestones: e.g., 1o refers the results with optimized e∗1 from initial examplesE0 as demonstrations (in general, we have e∗𝑘⊆E𝑘−1), and 1g refers to the results using E1 generatedby re-evaluating the train set with e∗1 as demonstrations. All results shown are averaged across 4random seeds with the standard deviation (stdev) denoted in the subscript. The best and second-bestresults along each row are bolded and underlined, respectively (ties are broken by favoring the resultwith lower stdev).\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(62.05699920654297, 526.4832763671875, 534.4338989257812, 653.1253662109375)\n",
      "Text:\n",
      "answer to fill in the rationale – this technique has been variously referred to as, e.g., infilling (Hu et al.,2023), rationalization (Zelikman et al., 2022), or more generally, teacher forcing (Chen et al., 2025)due to its conceptual similarity to teacher forcing in recurrent neural network (RNN) training (Lambet al., 2016) (Infill); 2) reinforced ICL (Agarwal et al., 2024), where all available input-output pairsfrom the correct predictions on the train set with zero-shot prompting are used; and 3) an iterativevariant of reinforced ICL which can also be seen as bridge without the optimize step: while werepeat the generation process on the train set 𝐾= 3 times, we do not first aim to select the optimizedsubset but instead use the entire generated examples from the previous step as demonstrationsE𝑘←𝑓LLM(D𝑡, E𝑘−1).\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(61.457000732421875, 655.2002563476562, 534.4293823242188, 754.3544921875)\n",
      "Text:\n",
      "Results and discussions. We show the test accuracy on the BBH tasks in Table 1 (Gemini 1.5 Pro;the number of examples for each entry in Table 1 are shown in Table 14 in App. C.2), Table 3 (Gemini1.5 Flash), Tables 4 and 5 (Mistral Large and Mistral NeMo) and Table 6 (Claude 3.5 Sonnet). OnMATH and GSM-Hard datasets, we show the Gemini 1.5 Pro results in Table 2. We observe thatnaïve many-shot scaling is in general ineffective and is outperformed by reinforced ICL; bridge,however, outperforms the base reinforced many-shot ICL by more than 7% and 3% on Tables 1 and3, respectively, and the extent of outperformance over the “Iterative reinforced ICL”, which leads to\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 9\n",
      "Position: Rect(528.4819946289062, 784.0443115234375, 532.9133911132812, 797.0913696289062)\n",
      "Text:\n",
      "9\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(116.04914093017578, 85.1656494140625, 469.685302734375, 118.27936553955078)\n",
      "Text:\n",
      "TasksReinf.IterativebridgeICLReinf.(Ours)# Iterations0121o1g2o2g3o\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(116.04914093017578, 118.97674560546875, 478.86126708984375, 142.50765991210938)\n",
      "Text:\n",
      "Hendryck’s MATH63.750.563.600.963.601.162.601.363.001.263.851.164.650.364.400.9GSM-Hard69.880.869.840.469.330.371.890.471.310.471.810.473.320.472.500.6\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(62.03499984741211, 149.9342498779297, 532.915283203125, 181.34146118164062)\n",
      "Text:\n",
      "Table 2 | Test accuracy of gemini-1.5-pro-001 on MATH and GSM-Hard datasets. Refer to thecaptions of Table 1 for detailed explanations.\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(96.855712890625, 284.9264831542969, 190.6112518310547, 309.1883544921875)\n",
      "Text:\n",
      "071# Demos\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(78.53845977783203, 257.46685791015625, 92.42121124267578, 279.1779479980469)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(78.59870910644531, 230.24224853515625, 92.42361450195312, 251.9533233642578)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(61.71244812011719, 233.39479064941406, 83.42353057861328, 268.1498718261719)\n",
      "Text:\n",
      "Val. Acc\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(125.55831146240234, 193.8150634765625, 161.85400390625, 211.18392944335938)\n",
      "Text:\n",
      "geometric\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(131.34780883789062, 202.23068237304688, 156.07452392578125, 219.59954833984375)\n",
      "Text:\n",
      "shapes\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(156.42984008789062, 250.4320068359375, 180.74392700195312, 264.90606689453125)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(156.42984008789062, 260.2736511230469, 179.0407257080078, 274.7477111816406)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(156.42984008789062, 270.11529541015625, 179.97909545898438, 284.58935546875)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(303.0768737792969, 273.8016357421875, 416.1645812988281, 309.1883544921875)\n",
      "Text:\n",
      "059# Demos0.6\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(303.3480224609375, 245.77630615234375, 316.9608154296875, 267.4873962402344)\n",
      "Text:\n",
      "0.7\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(303.13714599609375, 217.75094604492188, 316.9620361328125, 239.46202087402344)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(336.61468505859375, 193.8391571044922, 399.86602783203125, 211.20802307128906)\n",
      "Text:\n",
      "salient_translation\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(340.6869201660156, 202.23068237304688, 395.80914306640625, 219.59954833984375)\n",
      "Text:\n",
      "error_detection\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(433.72900390625, 284.9264831542969, 528.2994384765625, 309.1883544921875)\n",
      "Text:\n",
      "075# Demos\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(409.0695495605469, 265.67645263671875, 429.2284851074219, 287.3875427246094)\n",
      "Text:\n",
      "0.50\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(410.18426513671875, 237.97698974609375, 429.224853515625, 259.6880798339844)\n",
      "Text:\n",
      "0.75\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(410.8370056152344, 210.27752685546875, 429.2316589355469, 231.9886016845703)\n",
      "Text:\n",
      "1.00\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(449.99359130859375, 193.2245635986328, 511.03143310546875, 210.5934295654297)\n",
      "Text:\n",
      "tracking_shuffled\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(462.98858642578125, 202.23068237304688, 498.05023193359375, 219.59954833984375)\n",
      "Text:\n",
      "objects(7)\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(207.15048217773438, 284.9264831542969, 303.7332763671875, 309.1883544921875)\n",
      "Text:\n",
      "083# Demos\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(185.31442260742188, 261.63909912109375, 204.6827850341797, 283.3501892089844)\n",
      "Text:\n",
      "0.55\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(184.59136962890625, 239.82398986816406, 204.69244384765625, 261.5350646972656)\n",
      "Text:\n",
      "0.60\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(185.3746795654297, 218.00888061523438, 204.68519592285156, 239.71995544433594)\n",
      "Text:\n",
      "0.65\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(245.28407287597656, 202.23068237304688, 266.66351318359375, 219.59954833984375)\n",
      "Text:\n",
      "MATH\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(62.03499984741211, 312.57427978515625, 534.734375, 398.178466796875)\n",
      "Text:\n",
      "Figure4|Benefitsfromscalingexamplesnaïvely(redlines)isverytask-specific,buteach iteration ofbridgeaddresses it to a considerable degree by continually improv-ing upon the previous round:We randomly sample subsets of example pool E𝑘∀𝑘∈{0 (i.e., original examples generated with handcraft few-shot or zero-shot), 1, 2} and evaluate themon a held-out set in four representative tasks exhibiting different model behavior to example scaling.The trendlines are moving regressions fitted with lowess. Refers to additional figures in App. C.3.\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(61.270999908447266, 415.269287109375, 534.7313232421875, 731.2105102539062)\n",
      "Text:\n",
      "moderate improvements on BBH with Gemini Pro but no significant performance gains on MATH,GSM-Hard and BBH with Gemini Flash. Both demonstrate that optimize is an integral componentof bridge and implicitly validates the findings in Sec. 3 that many-shot performance can be drivenby few disproportionately influential examples, which constitutes a core motivation for our method.Barring some expected task-specific fluctuations, in both Tables 1 and 3, we also observe consistentand monotonic performance improvement as bridge progresses over the successive optimize andgenerate steps, eventually peaking at 2g on Gemini Pro and 2o on Gemini Flash (although theperformance difference between 2g and 2o on Gemini Flash is negligible and likely within margin oferror) – based on the overall results, we recommend stopping bridge at 2g or 2o. Interestingly, weobserve that in both cases, an additional optimize step (i.e., the 3o column) somewhat degradesperformance – our hypothesis is that as bridge progresses, the generated examples become morealigned with the optimal behavior and the degree of redundancy as we observed in Sec. 2 reduces,and it becomes more difficult to squeeze the number of examples without harming task performance– indeed, from Fig.4 where we concretely analyze the behavior of the LLM in different tasks byevaluating the LLM under random subsets of E0, ..., E2 as demonstrations in held-out splits, weobserve that the benefit from naïvely scaling examples under the base reinforced many-shot ICL(denoted by red lines) can be highly unstable across tasks: from the different subfigures of Fig. 4, wefind the performance to consistently improve with more examples (leftmost), improve then plateau(middle two figures) and even simply deteriorate with more examples (rightmost) – whereas thelatter two cases are direct manifestations that not all examples contribute positively to many-shotICL and naïvely scaling examples is suboptimal, we note that it remains true even in the former casewhere there is an apparent strong, positive correlation between number of demos and performance,as we demonstrated in Sec. 2.\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(79.29900360107422, 733.67529296875, 532.9086303710938, 751.5335083007812)\n",
      "Text:\n",
      "Remarkably, bridge alleviates the instability with each round of bridge continually improving\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 10\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "10\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(68.9425048828125, 85.16486358642578, 518.7542114257812, 117.99983215332031)\n",
      "Text:\n",
      "TasksAllReinf.IterativebridgeDirectCoTICLReinf.(Ours)# Iterations-00121o1g2o2g3o\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(68.94245910644531, 118.69129180908203, 525.980224609375, 280.5654602050781)\n",
      "Text:\n",
      "causal_judgement55.05.057.71.166.03.667.72.066.71.669.32.766.02.063.31.565.01.665.31.5date_understanding84.84.283.31.384.52.386.80.887.30.885.01.390.50.591.50.490.80.792.50.8disambiguation_qa68.87.254.21.575.50.577.81.678.53.577.51.379.01.177.51.276.30.874.31.1dyck_languages46.09.519.57.066.81.961.32.660.01.963.32.062.01.764.51.862.82.461.83.8formal_fallacies75.81.974.01.277.30.474.81.972.51.778.31.377.31.575.51.778.31.876.30.8geometric_shapes45.81.574.24.186.01.993.80.893.31.593.82.594.04.295.51.197.00.098.00.0hyperbaton87.03.188.51.588.51.595.51.193.31.586.57.695.51.195.80.894.80.493.31.5logical_deduction (7)37.53.341.01.959.53.461.91.957.54.761.85.157.51.170.50.966.51.175.00.7movie_recommendation80.53.356.20.867.01.275.81.375.82.970.32.373.32.377.31.578.82.072.83.2multistep_arithmetic_two55.021.384.02.991.30.894.01.492.51.896.32.396.80.497.80.494.80.895.80.4object_counting66.02.791.32.093.30.493.51.592.51.192.81.993.82.395.50.593.01.293.80.4ruin_names83.21.386.21.386.51.889.50.986.80.889.30.489.30.887.01.290.30.890.01.2salient_translation_error_detection62.03.758.82.064.81.571.52.264.02.962.80.871.00.769.82.069.00.767.30.4snarks81.20.792.01.289.21.888.92.286.51.588.92.089.91.889.60.790.60.683.73.5sports_understanding92.51.591.50.595.80.895.50.596.31.193.31.195.30.491.80.495.01.295.00.0tracking_shuffled_objects (7)63.35.472.36.092.23.183.51.180.01.698.00.793.82.298.00.097.80.497.50.5\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(68.94255065917969, 280.8626403808594, 523.988525390625, 294.2293395996094)\n",
      "Text:\n",
      "Average67.7770.2980.2581.9180.7281.6182.7983.7983.7783.25\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(62.03499984741211, 301.71630859375, 533.1194458007812, 333.1234436035156)\n",
      "Text:\n",
      "Table 3 | Test accuracy of gemini-1.5-flash-001 on BBH tasks. Refers to captions of Table 1 fordetailed explanations.\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(104.7967758178711, 344.654296875, 484.3081970214844, 380.33953857421875)\n",
      "Text:\n",
      "TasksReinf.IterativebridgeICLReinf.(Ours)# Iterations0121o1g2o2g3o\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(104.7967758178711, 381.0910339355469, 490.4780578613281, 556.5875854492188)\n",
      "Text:\n",
      "causal_judgement69.366.772.068.065.369.364.073.3date_understanding92.092.096.093.094.095.092.096.0disambiguation_qa82.082.079.081.087.087.084.086.0dyck_language56.062.056.070.059.070.063.071.0formal_fallacies90.082.086.089.089.090.083.085.0geometric_shapes87.080.093.088.085.095.071.094.0hyperbaton99.096.0100.0100.098.0100.0100.099.0logical_deduction (7)81.085.076.082.088.090.086.092.0movie_recommendation74.071.074.077.066.078.080.079.0multistep_arithmetic_two88.092.093.091.089.088.086.093.0object_counting99.099.099.098.098.098.0100.098.0ruin_names88.090.092.086.089.087.089.089.0salient_translation_error_detection66.068.070.078.069.075.072.073.0snarks95.895.897.294.495.895.895.893.1sports_understanding94.097.098.093.095.096.097.096.0tracking_shuffled_objects (7)96.068.0100.0100.073.0100.057.0100.0\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(104.7967758178711, 557.3399047851562, 490.47796630859375, 571.8668823242188)\n",
      "Text:\n",
      "Average84.8283.2287.0886.6583.7088.0782.8088.52\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(62.03499984741211, 579.4642944335938, 532.914794921875, 610.8715209960938)\n",
      "Text:\n",
      "Table 4 | Test accuracy of Mistral Large (mistral-large-2407) on BBH tasks. Refer to captions ofTable 1 for detailed explanations.\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(62.36199951171875, 627.9622802734375, 534.4300537109375, 700.0175170898438)\n",
      "Text:\n",
      "upon the previous round – in cases where scaling examples is already beneficial (geometric_shapes,leftmost figure), subsequent rounds of bridge led to much better performance-cost trade-offs withthe blue and green lines dominating over the red, whereas in other cases, bridge often “delays” thesaturation point (e.g., salient_translation) or at least ensure more examples does not lead todeterioration (e.g., tracking_shuffled_objects).\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(61.457000732421875, 702.4832763671875, 532.9158935546875, 760.988525390625)\n",
      "Text:\n",
      "On the BIRD dataset, we show the results in Table 7. Given the presence of a large trainingset (more than 9000 samples), we also compare against parameter-efficient supervised fine-tuning(PEFT) (Han et al., 2024), where we fine-tune the same target LLM with LoRA (Hu et al., 2021) oneither the entire training set or using a number of train samples sub-sampled from the full training\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 11\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "11\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(116.21390533447266, 85.1690673828125, 473.2001953125, 119.07908630371094)\n",
      "Text:\n",
      "TasksReinf.IterativebridgeICLReinf.(Ours)# Iterations0121o1g2o2g3o\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(116.2138900756836, 119.79327392578125, 477.8697814941406, 286.5638732910156)\n",
      "Text:\n",
      "causal_judgement53.365.362.760.058.762.764.064.0date_understanding66.071.068.069.069.078.070.075.0disambiguation_qa58.060.064.063.060.061.066.072.0dyck_languages17.021.022.018.027.026.022.030.0formal_fallacies64.055.053.063.059.052.051.059.0geometric_shapes65.065.069.072.072.060.069.068.0hyperbaton77.072.065.080.081.083.075.086.0logical_deduction (7)47.054.053.045.049.062.044.051.0movie_recommendation59.045.054.068.061.063.064.070.0multistep_arithmetic_two36.050.020.047.020.066.012.077.0object_counting81.081.082.083.079.085.075.087.0ruin_names69.060.057.076.057.072.057.070.0salient_translation_error_detection47.047.045.059.049.053.049.048.0snarks69.476.479.272.275.072.273.677.8sports_understanding86.075.069.091.072.091.074.093.0tracking_shuffled_objects (7)70.069.070.091.088.094.081.093.0\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(116.21390533447266, 287.2780456542969, 479.06317138671875, 301.0826721191406)\n",
      "Text:\n",
      "Average60.3060.4258.3066.0861.0467.5659.1670.05\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(62.03499984741211, 308.61029052734375, 532.9132690429688, 340.0184631347656)\n",
      "Text:\n",
      "Table 5 | Test accuracy of Mistral NeMo (mistral-nemo-12b) on BBH tasks. Refer to captions ofTable 1 for detailed explanations.\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(116.09552764892578, 351.53997802734375, 473.41400146484375, 384.8880615234375)\n",
      "Text:\n",
      "TasksReinf.IterativebridgeICLReinf.(Ours)# Iterations0121o1g2o2g3o\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(116.09550476074219, 385.5903625488281, 479.18048095703125, 549.5973510742188)\n",
      "Text:\n",
      "causal_judgement64.068.065.362.769.373.370.765.3date_understanding94.095.096.097.094.095.096.095.0disambiguation_qa73.082.079.081.087.087.084.086.0dyck_language68.068.065.074.085.090.092.087.0formal_fallacies93.094.097.096.095.098.096.095.0geometric_shapes92.094.098.088.090.085.096.089.0hyperbaton100.0100.0100.0100.0100.0100.0100.0100.0logical_deduction (7)92.096.096.089.095.097.091.093.0movie_recommendation87.090.092.089.090.088.093.090.0multistep_arithmetic_two99.099.099.099.099.099.0100.0100.0object_counting100.0100.0100.0100.0100.0100.0100.0100.0ruin_names93.093.094.091.094.094.092.094.0salient_translation_error_detection71.071.073.071.072.073.073.073.0snarks97.297.297.295.895.898.698.697.2sports_understanding92.091.094.093.094.094.093.091.0tracking_shuffled_objects (7)100.0100.0100.0100.0100.0100.0100.0100.0\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(116.09552764892578, 550.2996215820312, 478.7303161621094, 563.87548828125)\n",
      "Text:\n",
      "Average88.4589.8990.3589.1691.2692.0092.2090.97\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(62.03499984741211, 571.3822631835938, 534.7318115234375, 602.7894897460938)\n",
      "Text:\n",
      "Table 6 | Test accuracy of Claude 3.5 Sonnet (claude-3-5-sonnet@20240620) on BBH tasks.Refer to captions of Table 1 for detailed explanations.\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(62.36199951171875, 619.8803100585938, 533.1007690429688, 705.4844970703125)\n",
      "Text:\n",
      "set. We observe that whereas the few-shot chase prompt effectively improves upon the baselinezero-shot direct prompting, additional rounds of bridge led to further gains. The comparisonagainst LoRA also demonstrates the potential of bridge as an alternative to PEFT at least in certainscenarios. When provided with a similar number of labeled samples (i.e., 𝑛train = 256), we observethat LoRA performs much worse, and it only outperforms bridge when using up the entire train setfor training.\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(62.01300048828125, 707.9502563476562, 534.4262084960938, 752.906494140625)\n",
      "Text:\n",
      "Claude and Mistral results. From Tables4, 5, and 6, we find that while the base capabilities of thetested models differ significantly (e.g., Claude 3.5 Sonnet has a higher accuracy across the board),the high-level findings primarily derived from Gemini results largely hold. On Claude 3.5 Sonnet, we\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 12\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "12\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 533.1875, 168.36947631835938)\n",
      "Text:\n",
      "observe an almost identical high-level trend to Gemini, where each round of bridge incrementallyimproves performance up to 2g. On the other hand, while Mistral models seemingly benefit less fromscaling demonstrations especially in the smaller Mistral NeMo (e.g., sometimes the generate stepleads to drops in performance) directly, the improved quality of the generated demonstrations stillenables successive optimize step to improve on the preceding round, demonstrating the effectivenessof bridge even when the model does not benefit from scaling examples directly.\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(62.00199890136719, 170.8352508544922, 532.9193725585938, 202.24246215820312)\n",
      "Text:\n",
      "Additional experiments. We performed additional experiments to further validate the design ofbridge and tested the applicability of bridge beyond reasoning-heavy tasks. We:\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(75.08198547363281, 213.67527770996094, 534.6490478515625, 407.6724853515625)\n",
      "Text:\n",
      "1. Perform extensive ablation studies in App. C.1, where we studied the importance of Bayesianoptimization (Algorithm 2) and compared and combined bridge with heuristic learning-freeand learning-based demonstration selection such as retrieval and diversity-based learning-freecriteria; we found that in all cases bridge outperformed the alternative approaches, butbridge may also be complementary to various approaches proposed in previous works. Wealso conduct further experiments confirming the importance of the optimize step by restrictingbridge to perform refinement on the subset of the train set where the model predictedcorrectly initially only in Table 10, and confirmed that the optimize step meaningfully improvesupon the variant without it;2. Perform transfer learning analysis in App. C.5 where we used the many-shot examples gener-ated on GSM-Hard on GSM-8K, and we found that the generated examples are generalizable tosome extent to distributional shifts;3. Perform cost analysis in App. D where we provide a detailed cost breakdown of bridge ineach step.\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(62.36199951171875, 429.4797668457031, 159.2126007080078, 442.43115234375)\n",
      "Text:\n",
      "5. Related work\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(69.57000732421875, 454.8910217285156, 247.58177185058594, 480.01678466796875)\n",
      "Text:\n",
      "MethodExec.BreakdownAcc.SMC\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(69.57000732421875, 480.7742004394531, 253.32106018066406, 516.7391967773438)\n",
      "Text:\n",
      "Direct57.764.049.444.1chase prompt60.167.251.940.7chase + bridge\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(71.99639892578125, 513.2918090820312, 253.32015991210938, 549.6121826171875)\n",
      "Text:\n",
      "Round 059.165.751.342.1Round 161.268.650.648.3Round 262.068.553.049.0\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(69.57000732421875, 550.1885986328125, 253.32078552246094, 609.2615356445312)\n",
      "Text:\n",
      "PEFT (LoRA)𝑛train = 25658.264.052.240.7𝑛train = 102460.266.653.042.1𝑛train = 409661.367.553.946.2𝑛train = 9428 (All)63.868.658.848.9\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(62.03499984741211, 615.7952880859375, 252.31582641601562, 701.3995361328125)\n",
      "Text:\n",
      "Table 7 | Execution accuracy on the BIRDdev set with gemini-1.5-pro-001. {S,M, C} refer to the accuracy aggregatedacross {Simple, Moderate, Challenging}-level problems based on assigned diffi-culty.\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(62.36199951171875, 443.07928466796875, 534.6484985351562, 759.0205078125)\n",
      "Text:\n",
      "Scaling ICL. Before the advent of the long-context LLMs,early efforts in scaling ICL often studied LLMs customizedfor long context (Li et al., 2023) or required architecturalchanges assuming white-box model access (Hao et al.,2022). However, the tasks considered are often limited,e.g., to conventional, discriminative tasks like sentimentclassification rather than generative tasks as consideredin this work. Furthermore, these often study LLMs thatare merely capable of handling many examples, but theirbehavior may differ significantly to modern, natively long-context LLMs that may actively take advantage of the con-text – indeed, both these works show mixed results, evensignificant performance deterioration when scaling up thenumber of examples, a phenomenon not seen in modernlong-context LLMs like Gemini and Claude. Recent workslike Agarwal et al. (2024) and Bertsch et al. (2024), onthe other hand, reported significant gains in scaling ICLto hundreds or more examples and provided importantmotivation for our work. However, as mentioned in Sec. 2,these works primarily demonstrate the existence of thebenefit from scaling but do not focus on investigate the sources of the gain or improving the cost-effectiveness of many-shot ICL. Additionally, there have also been works focusing on applications ofmany-shot ICL to multi-modalities (Jiang et al., 2024), LLM jail-breaking (Anil et al., 2024), detecting\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 13\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "13\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(62.05699920654297, 82.76628112792969, 534.4299926757812, 114.17349243164062)\n",
      "Text:\n",
      "the risk of capturing incorrect skills (Lin and Lee, 2024), and analyzing memorization (Golchin et al.,2024).\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(61.04199981689453, 116.63926696777344, 534.7373046875, 459.6774597167969)\n",
      "Text:\n",
      "Example selection and generation. bridge combines the “optimize” and “generate” steps, andthere have been existing works sharing similar high-level ideas to each of the components. First, the“optimize” step can be seen as a method to improve the data quality with pruning and selection; inthis regard, given that data quality is known to be one of the most influential factors for training LLMs(Xia et al., 2024), many previous works have utilized some flavor of pruning to remove redundantor harmful data samples at different stages of training, including pre-training (Marion et al., 2023)and instruction tuning (Xia et al., 2024). In ICL, as mentioned in Sec. 2, given the sensitivity ofLLMs to examples, there have been numerous works analyzing prompt sensitivity and proposingexample selection techniques (Lu et al., 2022; Wan et al., 2024; Zhao et al., 2021; Zhou et al., 2024b).Recent work also explored heuristic-based prompt optimization based on similarity (Liu et al., 2022;Rubin et al., 2022), diversity (Levy et al., 2023; Xu et al., 2024), uncertainty (Wan et al., 2023a,b),fairness (Zhou et al., 2024a) etc. Our “generate” step, on the other hand, aims to acquire high-qualityexamples with the LLM itself. In this area, STaR (Zelikman et al., 2022) first proposes to bootstraprationales from LLM with a small number of seed examples, followed by fine-tuning on the rationalesthat lead to correct predictions; Self-Instruct (Wang et al., 2023) bootstraps LLMs to instructiondata. The “Reinforced ICL” technique introduced in Agarwal et al. (2024), upon which this workimproves, and several recent works (Chen et al., 2023; Khattab et al., 2023; Opsahl-Ong et al., 2024)use a similar technique to acquire and refine model-generated examples for ICL. Notwithstanding thesimilarities described, there are a few crucial differences with respect to these prior works: Almostall ICL works mentioned consider the few-shot setup, where selection is made necessary due to theconstraint on the number of examples allowed in the context. However, we show that even in themany-shot setup where that constraint is relaxed and example selection is no longer a necessity, itcan still be highly beneficial for performance and efficiency. Unlike the few-shot setup, bridge istailored for the many-shot setup with design decisions inspired by findings in Sec. 2, such as theimplementation of sparsity regularization in the optimization objective to enable scaling.\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(62.36199951171875, 481.4847412109375, 145.88560485839844, 494.4361267089844)\n",
      "Text:\n",
      "6. Conclusion\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(61.9370002746582, 503.58831787109375, 534.7410278320312, 751.7835083007812)\n",
      "Text:\n",
      "This paper focuses on understanding and enhancing the core factors underlying scaling ICL. We firstprovide an analysis of the nascent paradigm of many-shot ICL in LLMs and show that notwithstandingthe long-context abilities of LLMs, the common practice of naïvely dumping as many examples aspractically possible into the context can be both inefficient in cost and suboptimal in performance.Instead, the benefit from scaling examples can often be realized by identifying a subset of influentialexamples, and that subset can be used as demonstrations themselves to re-generate even moreexamples. Inspired by the findings, we propose bridge by automatically executing the “optimize”and “generate” steps iteratively. We demonstrate that bridge perform competitively on a wide rangeof tasks, significantly outperforming alternatives. We believe that this work builds the foundation forfuture research in many-shot ICL. First, we mainly focused on the restrictive black-box LLM setup,which is the most general and model-agnostic. However, for a more relaxed, white-box setup withaccess to LLM weights, it may be possible to perform optimization more efficiently – for example, itmay be possible to take advantage of the internal representations of the model in reducing the cost ofiterative optimization. Second, we currently focus on the “reinforced ICL” setup typical for reasoning-heavy tasks – while we have conducted experiments (e.g., low resource translation tasks) beyond thissetup, further validations on other types of tasks would be valuable. Lastly, after optimization, theexamples generated by bridge are currently static at test time, and it would also be interesting tocombine with a mechanism for sample-dependent ICL optimization to further enhance performance\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 14\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "14\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 391.7076721191406, 100.62448120117188)\n",
      "Text:\n",
      "and reduce cost – we defer these important directions to future work.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 120.43058013916016, 149.20933532714844, 131.3396759033203)\n",
      "Text:\n",
      "Acknowledgment\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(61.86000061035156, 137.9633026123047, 533.193359375, 182.91946411132812)\n",
      "Text:\n",
      "We thank all colleagues from Google Cloud AI Research for their valuable feedback. We also thankthe anonymous ICLR reviewers and area chairs whose feedback has been instrumental in improvingthis work.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 203.8857421875, 126.97654724121094, 216.83714294433594)\n",
      "Text:\n",
      "References\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 225.98924255371094, 534.427978515625, 257.8397521972656)\n",
      "Text:\n",
      "R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes,E. Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 261.4932861328125, 534.4256591796875, 293.3437805175781)\n",
      "Text:\n",
      "C. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford,et al. Many-shot jailbreaking. Anthropic, April, 2024.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 296.997314453125, 379.3586120605469, 314.8554992675781)\n",
      "Text:\n",
      "Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 318.9512939453125, 533.2575073242188, 364.3517761230469)\n",
      "Text:\n",
      "M. Balandat, B. Karrer, D. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. Botorch: Aframework for efficient monte-carlo bayesian optimization. Advances in neural information processingsystems, 33:21524–21538, 2020.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 368.0042724609375, 532.9124145507812, 399.85577392578125)\n",
      "Text:\n",
      "A. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig. In-context learning withlong-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 403.50830078125, 534.7385864257812, 448.9087829589844)\n",
      "Text:\n",
      "V. J. Bowman Jr. On the relationship of the tchebycheff norm and the efficient frontier of multiple-criteria objectives. In Multiple Criteria Decision Making: Proceedings of a Conference Jouy-en-Josas,France May 21–23, 1975, pages 76–86. Springer, 1976.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 452.561279296875, 534.6535034179688, 565.2645263671875)\n",
      "Text:\n",
      "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Ad-vances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Asso-ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 569.3602905273438, 533.0994873046875, 601.2107543945312)\n",
      "Text:\n",
      "K. Chen, Y. Chen, X. Yu, and N. Koudas. Reliable text-to-sql with adaptive abstention. arXiv preprintarXiv:2501.10858, 2025.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 604.8643188476562, 534.7325439453125, 676.9194946289062)\n",
      "Text:\n",
      "W.-L. Chen, C.-K. Wu, Y.-N. Chen, and H.-H. Chen. Self-ICL: Zero-shot in-context learning withself-generated demonstrations. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023Conference on Empirical Methods in Natural Language Processing, pages 15651–15662, Singapore,Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.968.URL https://aclanthology.org/2023.emnlp-main.968.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 681.0153198242188, 532.9121704101562, 712.8667602539062)\n",
      "Text:\n",
      "T. Chugh. Scalarizing functions in bayesian multiobjective optimization. In 2020 IEEE Congress onEvolutionary Computation (CEC), pages 1–8. IEEE, 2020.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 716.519287109375, 534.43017578125, 761.4765014648438)\n",
      "Text:\n",
      "K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,2021.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 15\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "15\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 534.6484375, 128.16574096679688)\n",
      "Text:\n",
      "R. Das, M. Zaheer, D. Thai, A. Godbole, E. Perez, J.-Y. Lee, L. Tan, L. Polymenakos, and A. McCal-lum. Case-based reasoning for natural language queries over knowledge bases. arXiv preprintarXiv:2104.08762, 2021.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 132.24229431152344, 532.9172973632812, 177.64175415039062)\n",
      "Text:\n",
      "S. Daulton, X. Wan, D. Eriksson, M. Balandat, M. A. Osborne, and E. Bakshy. Bayesian optimizationover discrete and mixed spaces via probabilistic reparameterization. Advances in Neural InformationProcessing Systems, 35:12760–12774, 2022.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 181.71824645996094, 483.21197509765625, 200.01974487304688)\n",
      "Text:\n",
      "P. I. Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 204.09727478027344, 532.9100952148438, 235.94778442382812)\n",
      "Text:\n",
      "L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aidedlanguage models. arXiv preprint arXiv:2211.10435, 2022.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 240.02427673339844, 534.4300537109375, 284.9804992675781)\n",
      "Text:\n",
      "J. Gardner, G. Pleiss, K. Q. Weinberger, D. Bindel, and A. G. Wilson. Gpytorch: Blackbox matrix-matrixgaussian process inference with gpu acceleration. Advances in neural information processing systems,31, 2018.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 289.50030517578125, 392.1648864746094, 307.8017883300781)\n",
      "Text:\n",
      "R. Garnett. Bayesian optimization. Cambridge University Press, 2023.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 311.8782958984375, 534.7384033203125, 343.7297668457031)\n",
      "Text:\n",
      "S. Golchin, M. Surdeanu, S. Bethard, E. Blanco, and E. Riloff. Memorization in in-context learning.arXiv preprint arXiv:2408.11546, 2024.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 347.8062744140625, 536.0632934570312, 446.95947265625)\n",
      "Text:\n",
      "F. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. TheFLORES evaluation datasets for low-resource machine translation: Nepali–English and Sinhala–English. In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP), pages 6098–6111, Hong Kong, China, Nov.2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1632. URL https://aclanthology.org/D19-1632.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 451.4792785644531, 534.7343139648438, 496.8787841796875)\n",
      "Text:\n",
      "J. M. Han, I. Babuschkin, H. Edwards, A. Neelakantan, T. Xu, S. Polu, A. Ray, P. Shyam, A. Ramesh,A. Radford, et al. Unsupervised neural machine translation with generative language models only.arXiv preprint arXiv:2110.05448, 2021.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 500.956298828125, 533.2574462890625, 532.8067626953125)\n",
      "Text:\n",
      "Z. Han, C. Gao, J. Liu, S. Q. Zhang, et al.Parameter-efficient fine-tuning for large models: Acomprehensive survey. arXiv preprint arXiv:2403.14608, 2024.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 536.88330078125, 532.9121704101562, 568.7337646484375)\n",
      "Text:\n",
      "Y. Hao, Y. Sun, L. Dong, Z. Han, Y. Gu, and F. Wei. Structured prompting: Scaling in-context learningto 1,000 examples. arXiv preprint arXiv:2212.06713, 2022.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 572.810302734375, 532.912109375, 604.6607666015625)\n",
      "Text:\n",
      "D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuringmathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 608.7373046875, 533.201904296875, 640.5887451171875)\n",
      "Text:\n",
      "E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rankadaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 644.665283203125, 532.9120483398438, 676.5157470703125)\n",
      "Text:\n",
      "E. J. Hu, M. Jain, E. Elmoznino, Y. Kaddar, G. Lajoie, Y. Bengio, and N. Malkin. Amortizing intractableinference in large language models. arXiv preprint arXiv:2310.04363, 2023.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 680.59228515625, 532.9120483398438, 712.4427490234375)\n",
      "Text:\n",
      "Y. Jiang, J. Irvin, J. H. Wang, M. A. Chaudhry, J. H. Chen, and A. Y. Ng. Many-shot in-context learningin multimodal foundation models. arXiv preprint arXiv:2405.09798, 2024.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 716.519287109375, 534.43017578125, 761.9197387695312)\n",
      "Text:\n",
      "O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma,T. T. Joshi, H. Moazam, et al. Dspy: Compiling declarative language model calls into self-improvingpipelines. arXiv preprint arXiv:2310.03714, 2023.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 16\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "16\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 534.6483154296875, 127.72244262695312)\n",
      "Text:\n",
      "J. Knowles. Parego: A hybrid algorithm with on-line landscape approximation for expensive mul-tiobjective optimization problems. IEEE transactions on evolutionary computation, 10(1):50–66,2006.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 132.19126892089844, 533.125732421875, 177.59078979492188)\n",
      "Text:\n",
      "A. M. Lamb, A. G. ALIAS PARTH GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professorforcing: A new algorithm for training recurrent networks. Advances in neural information processingsystems, 29, 2016.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 181.61729431152344, 534.4296875, 226.57345581054688)\n",
      "Text:\n",
      "J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, et al. Gecko:Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327,2024.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 231.0422821044922, 534.73974609375, 303.0974426269531)\n",
      "Text:\n",
      "I. Levy, B. Bogin, and J. Berant. Diverse demonstrations improve in-context compositional generaliza-tion. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401–1422, Toronto,Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.78.URL https://aclanthology.org/2023.acl-long.78.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 307.5662841796875, 533.1905517578125, 352.96575927734375)\n",
      "Text:\n",
      "J. Li, B. Hui, G. Qu, J. Yang, B. Li, B. Li, B. Wang, B. Qin, R. Geng, N. Huo, et al. Can llm alreadyserve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advancesin Neural Information Processing Systems, 36, 2024a.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 356.9913024902344, 533.190673828125, 388.8427734375)\n",
      "Text:\n",
      "M. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong.In-context learning with manydemonstration examples. arXiv preprint arXiv:2302.04931, 2023.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 392.8682861328125, 533.1016845703125, 424.7187805175781)\n",
      "Text:\n",
      "Z. Li, C. Li, M. Zhang, Q. Mei, and M. Bendersky. Retrieval augmented generation or long-contextllms? a comprehensive study and hybrid approach. arXiv preprint arXiv:2407.16833, 2024b.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 428.7442932128906, 534.7350463867188, 487.2504577636719)\n",
      "Text:\n",
      "Z. Lin and K. Lee. Dual operating modes of in-context learning. In R. Salakhutdinov, Z. Kolter, K. Heller,A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Con-ference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 30135–30188. PMLR, 21–27 Jul 2024. URL https://proceedings.mlr.press/v235/lin24l.html.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 491.71929931640625, 533.1257934570312, 550.2244873046875)\n",
      "Text:\n",
      "J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context examples forGPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on KnowledgeExtraction and Integration for Deep Learning Architectures, pages 100–114, Dublin, Ireland andOnline, May 2022. Association for Computational Linguistics.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 554.6942749023438, 534.4326782226562, 640.2974853515625)\n",
      "Text:\n",
      "Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp.Fantastically ordered prompts andwhere to find them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov,and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 8086–8098, Dublin, Ireland, May2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URLhttps://aclanthology.org/2022.acl-long.556.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 644.7672729492188, 532.9120483398438, 676.6177368164062)\n",
      "Text:\n",
      "M. Marion, A. Üstün, L. Pozzobon, A. Wang, M. Fadaee, and S. Hooker. When less is more: Investigatingdata pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 680.6432495117188, 534.6484985351562, 726.042724609375)\n",
      "Text:\n",
      "K. Opsahl-Ong, M. J. Ryan, J. Purtell, D. Broman, C. Potts, M. Zaharia, and O. Khattab. Optimiz-ing instructions and demonstrations for multi-stage language model programs. arXiv preprintarXiv:2406.11695, 2024.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 730.0682983398438, 534.6553955078125, 761.9146728515625)\n",
      "Text:\n",
      "B. Paria, K. Kandasamy, and B. Póczos. A flexible framework for multi-objective bayesian optimizationusing random scalarizations. In Uncertainty in Artificial Intelligence, pages 766–776. PMLR, 2020.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 17\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "17\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 532.915283203125, 114.61679077148438)\n",
      "Text:\n",
      "A. Patel, B. Li, M. S. Rasooli, N. Constant, C. Raffel, and C. Callison-Burch. Bidirectional languagemodels are also few-shot learners. arXiv preprint arXiv:2209.14500, 2022.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 118.64225769042969, 534.7374267578125, 164.04177856445312)\n",
      "Text:\n",
      "M. Pourreza, H. Li, R. Sun, Y. Chung, S. Talaei, G. T. Kakkar, Y. Gan, A. Saberi, F. Ozcan, and S. O.Arik. Chase-sql: Multi-path reasoning and preference optimized candidate selection in text-to-sql.International Conference on Learning Representations (ICLR), 2025.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 168.06724548339844, 534.4353637695312, 213.46774291992188)\n",
      "Text:\n",
      "M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou,O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions oftokens of context. arXiv preprint arXiv:2403.05530, 2024.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 217.49327087402344, 532.91259765625, 262.8927917480469)\n",
      "Text:\n",
      "B. Ru, X. Wan, X. Dong, and M. Osborne. Interpretable neural architecture search via bayesianoptimisation with weisfeiler-lehman kernels. International Conference on Learning Representations(ICLR), 2021.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 266.91827392578125, 537.3190307617188, 352.5224914550781)\n",
      "Text:\n",
      "O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. In M. Carpuat,M.-C. de Marneffe, and I. V. Meza Ruiz, editors, Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,pages 2655–2671, Seattle, United States, July 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.naacl-main.191. URL https://aclanthology.org/2022.naacl-main.191.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 356.9913024902344, 534.3060913085938, 401.948486328125)\n",
      "Text:\n",
      "W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. Müller. Explaining deep neuralnetworks and beyond: A review of methods and applications. Proceedings of the IEEE, 109(3):247–278, 2021.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 406.41729736328125, 532.9124755859375, 451.8167724609375)\n",
      "Text:\n",
      "R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visualexplanations from deep networks via gradient-based localization.In Proceedings of the IEEEinternational conference on computer vision, pages 618–626, 2017.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 455.84228515625, 533.1900024414062, 487.6937561035156)\n",
      "Text:\n",
      "K. Simonyan. Deep inside convolutional networks: Visualising image classification models and saliencymaps. arXiv preprint arXiv:1312.6034, 2013.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 491.71929931640625, 532.9152221679688, 523.5697021484375)\n",
      "Text:\n",
      "R. E. Steuer and E.-U. Choo. An interactive weighted tchebycheff procedure for multiple objectiveprogramming. Mathematical programming, 26:326–344, 1983.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 527.5952758789062, 532.9078979492188, 559.4457397460938)\n",
      "Text:\n",
      "M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In Internationalconference on machine learning, pages 3319–3328. PMLR, 2017.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 563.4712524414062, 532.9125366210938, 608.8717041015625)\n",
      "Text:\n",
      "X. Wan, V. Nguyen, H. Ha, B. Ru, C. Lu, and M. A. Osborne. Think global and act local: Bayesianoptimisation over high-dimensional categorical and mixed search spaces. In International Conferenceon Machine Learning, pages 10663–10674. PMLR, 2021.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 612.8972778320312, 534.734619140625, 684.9525146484375)\n",
      "Text:\n",
      "X. Wan, R. Sun, H. Dai, S. Arik, and T. Pfister.Better zero-shot reasoning with self-adaptiveprompting.In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Associa-tion for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada, July 2023a.Association for Computational Linguistics.doi: 10.18653/v1/2023.findings-acl.216.URLhttps://aclanthology.org/2023.findings-acl.216.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 689.4213256835938, 536.0632934570312, 761.4765014648438)\n",
      "Text:\n",
      "X. Wan, R. Sun, H. Nakhost, H. Dai, J. Eisenschlos, S. Arik, and T. Pfister. Universal self-adaptiveprompting. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing, pages 7437–7462, Singapore, Dec. 2023b.Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.461. URL https://aclanthology.org/2023.emnlp-main.461.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 18\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "18\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 534.7354736328125, 128.14541625976562)\n",
      "Text:\n",
      "X. Wan, R. Sun, H. Nakhost, and S. O. Arik. Teach better or show smarter? on instructions andexemplars in automatic prompt optimization. In The Thirty-eighth Annual Conference on Neural In-formation Processing Systems, 2024. URL https://openreview.net/forum?id=IdtoJVWVnX.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 132.37925720214844, 536.0632934570312, 217.98348999023438)\n",
      "Text:\n",
      "Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:Aligning language models with self-generated instructions. In A. Rogers, J. Boyd-Graber, andN. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages 13484–13508, Toronto, Canada, July 2023.Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 222.6412811279297, 532.9182739257812, 254.49179077148438)\n",
      "Text:\n",
      "C. Williams and C. Rasmussen. Gaussian processes for regression. Advances in neural informationprocessing systems, 8, 1995.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 258.706298828125, 532.9185791015625, 290.1134948730469)\n",
      "Text:\n",
      "C. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning, volume 2. MIT pressCambridge, MA, 2006.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 294.77130126953125, 534.7316284179688, 353.2764892578125)\n",
      "Text:\n",
      "M. Xia, S. Malladi, S. Gururangan, S. Arora, and D. Chen. LESS: selecting influential data for targetedinstruction tuning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna,Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=PG5fV50maR.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 357.9342956542969, 533.1239013671875, 389.7847595214844)\n",
      "Text:\n",
      "X. Xu, Y. Liu, P. Pasupat, M. Kazemi, et al. In-context learning with retrieved demonstrations forlanguage models: A survey. arXiv preprint arXiv:2401.11624, 2024.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 393.9992980957031, 534.7389526367188, 438.9554748535156)\n",
      "Text:\n",
      "E. Zelikman, Y. Wu, J. Mu, and N. Goodman. STar: Bootstrapping reasoning with reasoning. In A. H.Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems,2022. URL https://openreview.net/forum?id=_3ELRdg2sgI.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 443.6122741699219, 532.9171142578125, 475.4637756347656)\n",
      "Text:\n",
      "D. Zhan and H. Xing. Expected improvement for expensive optimization: a review. Journal of GlobalOptimization, 78(3):507–544, 2020.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 479.6772766113281, 536.0632934570312, 524.634521484375)\n",
      "Text:\n",
      "Z. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large languagemodels. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=5NTt8GFjUHkr.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 529.2913208007812, 533.1017456054688, 574.6917724609375)\n",
      "Text:\n",
      "Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shotperformance of language models. In Proceedings of the 38th International Conference on MachineLearning, ICML 2021, 18-24 July 2021, Virtual Event, pages 12697–12706, 2021.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 578.9052734375, 537.3190307617188, 664.509521484375)\n",
      "Text:\n",
      "H. Zhou, X. Wan, Y. Liu, N. Collier, I. Vulić, and A. Korhonen. Fairer preferences elicit improvedhuman-aligned large language model judgments. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen,editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,pages 1241–1252, Miami, Florida, USA, Nov. 2024a. Association for Computational Linguistics. doi:10.18653/v1/2024.emnlp-main.72. URL https://aclanthology.org/2024.emnlp-main.72/.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 669.1672973632812, 534.7352905273438, 714.5311889648438)\n",
      "Text:\n",
      "H. Zhou, X. Wan, L. Proleev, D. Mincu, J. Chen, K. A. Heller, and S. Roy. Batch calibration: Rethinkingcalibration for in-context learning and prompt engineering. In The Twelfth International Conferenceon Learning Representations, 2024b. URL https://openreview.net/forum?id=L3FHMoKZcS.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 19\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "19\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 85.9677734375, 380.8240051269531, 98.91917419433594)\n",
      "Text:\n",
      "A. Derivation of the Approximated Importance Score\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(61.270999908447266, 108.07127380371094, 532.9189453125, 194.06529235839844)\n",
      "Text:\n",
      "In this section, we give detailed derivation of the importance score used in Sec. 2 to rank the examples– on a high level, we use a similar approach to Ru et al. (2021) in determining the importance fromthe GP surrogate: Recalling that we are given a pool of examples E with |E| = 𝑚, a collection of𝑇subsets of e𝑖, each represented as a binary vector e𝑖∈{0, 1}𝑚and their corresponding scoreson the validation set 𝑔(·) : {0, 1}𝑚→ℝ, we first fit a GP regression with e1:𝑇= [e1, ..., e𝑇]⊤andg1:𝑇= [𝑔(e1, ..., 𝑔(e𝑇)]⊤, as presented in Eq. 2, the mean of the posterior GP ˆ𝑔(·) is given by:\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(213.72300720214844, 198.6622314453125, 533.8184814453125, 218.26426696777344)\n",
      "Text:\n",
      "𝔼ˆ𝑔(e)| G𝑇[ˆ𝑔(e)] = k1:𝑇(K + 𝜂2I)−1g1:𝑇,(3)\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(61.9370002746582, 223.38829040527344, 534.42822265625, 295.4434509277344)\n",
      "Text:\n",
      "where we define G𝑇as the shorthand of [e1:𝑇, g1:𝑇] to denote that the fitted function ˆ𝑔(e) is fittedon the observed input-output pairs; k𝑡= [𝑘(e, e1), ..., 𝑘(e, e𝑡)] and 𝑘(·, ·) is the covariance functionof the GP (we use Matern 2.5 by default). As mentioned in Sec. 2, whereas we do not assume anydifferentiability property from 𝑔(·) on e, since the approximated function ˆ𝑔(·) follows a posterior GP,its gradient w.r.t e is analytically available and is itself a GP, given by:\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(213.57598876953125, 302.625, 269.000732421875, 323.3580017089844)\n",
      "Text:\n",
      "∇e𝑔= 𝜕𝑔(e)\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(251.66299438476562, 302.625, 306.4498291015625, 329.8686828613281)\n",
      "Text:\n",
      "𝜕e= 𝜕k1:𝑇\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(290.625, 305.75225830078125, 533.818603515625, 329.8686828613281)\n",
      "Text:\n",
      "𝜕e (K + 𝜂2I)−1g1:𝑇,(4)\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 330.9932861328125, 533.1900024414062, 407.00848388671875)\n",
      "Text:\n",
      "noting that the expensive matrix inversion term, (K + 𝜂2I)−1 does not have a dependence on e and canbe directly cached from Eq. 3 when we compute the posterior mean. The derivative term is essentiallya differentiation operation of the covariance function to the input and can be easily computed eitheranalytically for common kernel choices or via automatic differentiation for popular GP or BO packageslike gpytorch (Gardner et al., 2018) or botorch (Balandat et al., 2020).\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 409.4742736816406, 534.7352294921875, 467.9794616699219)\n",
      "Text:\n",
      "With the computed ∇e𝑔∈ℝ𝑚, we can in principle compute the estimated derivative at any e ⊆E.However, in practice, we find the derivative estimate to be more reliable at the training points ofthe GP (i.e., [e1, ..., e𝑇]. We then evaluate the derivative at each of the training points, and the finalimportance score is marginalized by averaging across the training points:\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(241.97900390625, 475.7802734375, 294.9084777832031, 501.0234680175781)\n",
      "Text:\n",
      "𝑀(𝑒( 𝑗)) = 1\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(288.4339904785156, 493.967041015625, 294.10455322265625, 504.00341796875)\n",
      "Text:\n",
      "𝑇\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(298.0039978027344, 475.8102111816406, 312.4367370605469, 495.2305908203125)\n",
      "Text:\n",
      "𝑇∑︁\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(299.29498291015625, 483.1653137207031, 533.8184814453125, 512.63037109375)\n",
      "Text:\n",
      "𝑡=1∇eˆ𝑔|( 𝑗)e=e𝑡,(5)\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(61.9370002746582, 516.5112915039062, 532.9136962890625, 575.0175170898438)\n",
      "Text:\n",
      "where we use the superscript ( 𝑗) to denote that the estimated importance of the 𝑗-th individualexample (note the regular font 𝑒∈E denoting an individual example instead of the bold-face edenoting a set of examples in E). We then compute the importance score of all examples in E, whichis then used to generate the assigned ranking in the analysis of Sec. 2 such as Fig. 4.\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 596.4627685546875, 222.15640258789062, 609.4141845703125)\n",
      "Text:\n",
      "B. Implementation Details\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 624.4656372070312, 133.2820587158203, 635.3746948242188)\n",
      "Text:\n",
      "B.1. Datasets.\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 641.999267578125, 532.91259765625, 673.406494140625)\n",
      "Text:\n",
      "In the section below, we give detailed implementation details for the availability, data splittingprotocol, input prompts, and licensing information of the datasets used.\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(61.9370002746582, 675.8722534179688, 534.4258422851562, 761.4765014648438)\n",
      "Text:\n",
      "BIG-Bench Hard (BBH). BBH is a collection of 26 challenging reasoning tasks and a task is selectedif either 1) if it is studied in the seminal work on many-shot ICL (Agarwal et al., 2024) or 2) ifthe zero-shot performance of gemini-1.5-pro-001 is below 90%, which indicates non-saturationof performance – these criteria led to a set of 16 tasks that we consider in Sec. 4. For all tasks,we randomize the data points and reserve 40% (usually 100 samples, but some sub-tasks of BBHbenchmark have fewer data-points) as held-out sets for testing, whose inputs and labels are not\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 20\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "20\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(61.457000732421875, 82.76628112792969, 533.3853759765625, 181.91946411132812)\n",
      "Text:\n",
      "revealed to the model except for final evaluation. For the rest of the dataset, in Sec. 2, we use 50%(30% of all available data points including the held-out test set) as the “train-set” from which theexamples are generated and the other 50% for validation (i.e., the split where results in Fig. 4 isgenerated). In Sec. 4, we do not use the aforementioned validation set and use performance on thesame set that generates the examples as the optimization objective. The BBH dataset is publiclyavailable at https://github.com/suzgunmirac/BIG-Bench-Hard under an MIT license. Forall BBH tasks, we use the prompt templates below:\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 180.50320434570312, 523.3922729492188, 209.04795837402344)\n",
      "Text:\n",
      "1 Youwill be given a question. Thinkstep by stepbeforegiving a finalanswertothisquestion. Showyourfinalanswer{{TASK_SPECIFIC_CONSTRAINTS}}between<answer > and<\\answer >\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 208.89718627929688, 57.38054656982422, 218.68251037597656)\n",
      "Text:\n",
      "2\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 218.36117553710938, 139.9677734375, 228.14649963378906)\n",
      "Text:\n",
      "3 {{EXAMPLES}}\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 227.82620239257812, 72.9162368774414, 237.6115264892578)\n",
      "Text:\n",
      "4 ==\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 237.29116821289062, 57.38054656982422, 247.0764923095703)\n",
      "Text:\n",
      "5\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 246.75521850585938, 139.9677734375, 256.5405578613281)\n",
      "Text:\n",
      "6 {{QUESTION}}\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 256.2201843261719, 123.20488739013672, 266.0055236816406)\n",
      "Text:\n",
      "7 {{ llm () }}\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(61.04199981689453, 271.55828857421875, 533.12060546875, 411.35845947265625)\n",
      "Text:\n",
      "where we use a Jinja2-style syntax and the upper-cased blocks bracketed between double braces arevariables that are replaced at inference time: TASK_SPECIFIC_CONSTRAINTS denote the constraintinstruction specific to the type of the task. For example, for a multiple-choice task, this is replaced with“answer option letter only”; for a binary choice question, this is replaced with “Yes or No only” and fora free-form generation task, this is replaced by an empty string. EXAMPLES denote the concatenationof any examples e added to the input – for the initial generation step (i.e., Step 3 in Algorithm 1), weuse zero-shot prompting and EXAMPLES is an empty string. For the subsequent generation step, thisis replaced with the concatenation of the examples selected by bridge; finally, llm() denotes theplace where an LLM response is solicited; the answer is then extracted and postprocessed to matchwith a ground-truth answer to measure accuracy.\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(61.457000732421875, 413.82427978515625, 537.323974609375, 580.7235107421875)\n",
      "Text:\n",
      "MATH and GSM-Hard. In MATH and GSM-Hard, we similarly adopt the unified train set setup asmentioned in the previous paragraph. To minimize the chance of data contamination where thetraining inputs and outputs were leaked to the model during pre-training or instruction finetuning,we randomly sample 128 samples as the official test set as the train set from which the examplesare generated and use the rest of the official test set for testing. The MATH dataset is available athttps://github.com/hendrycks/math and GSM-Hard is available at https://huggingface.co/datasets/reasoning-machines/gsm-hard. Both datasets are licensed under an MIT license.On GSM-Hard, we use the same prompt as the BBH dataset mentioned above. On MATH, we usethe inner monologue prompt consists of a human-annotated few-shot prompt given by Agarwal et al.(2024) with an added preamble to ensure that the LLM generation follows the style of the examplesgiven, noting that any model-generated examples will be added to the initial human-annotatedexamples:\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 579.3081665039062, 522.9368896484375, 617.3179321289062)\n",
      "Text:\n",
      "1 Youwill be givenseveralexamplemathquestionsandtheirsolutions. At the end ,Youwill be givenanotherquestionthatyouwillneed to solve. Makesureyoufollowtheexamplesandalwaysfinishyouranswerwith ’FinalAnswer: Thefinalanswer is X. I hope it iscorrect.’ where X is thecorrectanswer.\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 617.1661376953125, 57.38054656982422, 626.9514770507812)\n",
      "Text:\n",
      "2\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 626.6311645507812, 106.59217834472656, 636.41650390625)\n",
      "Text:\n",
      "3 Problem:\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 636.0951538085938, 430.67205810546875, 645.8804931640625)\n",
      "Text:\n",
      "4 Findthedomain of theexpression $\\frac {\\ sqrt{x -2}}{\\ sqrt{5-x}}$.\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 645.5601806640625, 57.38054656982422, 655.3455200195312)\n",
      "Text:\n",
      "5\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 655.024169921875, 112.18110656738281, 664.8095092773438)\n",
      "Text:\n",
      "6 Solution:\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 664.4891357421875, 57.38054656982422, 674.2744750976562)\n",
      "Text:\n",
      "7\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 673.953125, 313.3421325683594, 683.7384643554688)\n",
      "Text:\n",
      "8 I need to findthedomainof thisexpression .\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(54.05699920654297, 683.4181518554688, 57.38054656982422, 693.2034912109375)\n",
      "Text:\n",
      "9\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(50.73400115966797, 692.8821411132812, 330.1034851074219, 702.66748046875)\n",
      "Text:\n",
      "10 Thisexpressionhas twosquarerootexpressions .\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(50.73400115966797, 702.34716796875, 57.38109588623047, 712.1325073242188)\n",
      "Text:\n",
      "11\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(50.73400115966797, 711.8111572265625, 408.3260803222656, 721.5964965820312)\n",
      "Text:\n",
      "12 I knowthevaluesundereachsquarerootmust be non -negative.\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(50.73400115966797, 721.2761840820312, 57.38109588623047, 731.0615234375)\n",
      "Text:\n",
      "13\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(50.73400115966797, 730.7401733398438, 246.28750610351562, 740.5255126953125)\n",
      "Text:\n",
      "14 So let me start byensuringthat.\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(50.73400115966797, 740.2051391601562, 57.38109588623047, 749.990478515625)\n",
      "Text:\n",
      "15\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 21\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "21\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 88.7481918334961, 324.5201110839844, 98.53352355957031)\n",
      "Text:\n",
      "16 I set $x -2 \\ge 0$, whichsimplifiesto $x\\ge2$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 98.2121810913086, 57.38109588623047, 107.99751281738281)\n",
      "Text:\n",
      "17\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 107.67720794677734, 380.3996276855469, 117.46253967285156)\n",
      "Text:\n",
      "18 Then , I set $5 - x \\ge 0$, whichsimplifiesto $x \\le 5$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 117.14119720458984, 57.38109588623047, 126.92652893066406)\n",
      "Text:\n",
      "19\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 126.60616302490234, 352.4576416015625, 136.39149475097656)\n",
      "Text:\n",
      "20 But I need to makesurethedenominatoris notzero.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 136.07022094726562, 57.38109588623047, 145.8555450439453)\n",
      "Text:\n",
      "21\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 145.53518676757812, 413.91943359375, 155.3205108642578)\n",
      "Text:\n",
      "22 So , $5 -x$ mustalso begreaterthan zero , whichgives me $x <5$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 154.99917602539062, 57.38109588623047, 164.7845001220703)\n",
      "Text:\n",
      "23\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 164.46420288085938, 391.5610656738281, 174.24952697753906)\n",
      "Text:\n",
      "24 Therefore , thedomain of theexpressionis $\\boxed {[2 ,5)}$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 173.92819213867188, 57.38109588623047, 183.71351623535156)\n",
      "Text:\n",
      "25\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 183.39321899414062, 151.297607421875, 193.1785430908203)\n",
      "Text:\n",
      "26 Answer: $[2 ,5)$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 192.85720825195312, 57.38109588623047, 202.6425323486328)\n",
      "Text:\n",
      "27\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 202.32217407226562, 419.50732421875, 212.1074981689453)\n",
      "Text:\n",
      "28 FinalAnswer: Thefinalansweris $[2 ,5)$. I hope it iscorrect.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 211.78616333007812, 72.9162368774414, 221.5714874267578)\n",
      "Text:\n",
      "29 ==\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 221.25119018554688, 57.38109588623047, 231.03651428222656)\n",
      "Text:\n",
      "30\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 230.71517944335938, 106.59217834472656, 240.50050354003906)\n",
      "Text:\n",
      "31 Problem:\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 240.18020629882812, 520.0877075195312, 259.2599182128906)\n",
      "Text:\n",
      "32 If $\\det \\mathbf{A} = 2$ and $\\det \\mathbf{B} = 12,$ thenfind $\\det (\\ mathbf{A} \\mathbf{B}).$\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 259.1091613769531, 57.38109588623047, 268.8945007324219)\n",
      "Text:\n",
      "33\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 268.5732116699219, 112.18110656738281, 278.3585510253906)\n",
      "Text:\n",
      "34 Solution:\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 278.0381774902344, 57.38109588623047, 287.8235168457031)\n",
      "Text:\n",
      "35\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 287.5021667480469, 430.68243408203125, 297.2875061035156)\n",
      "Text:\n",
      "36 I need to findthedeterminantof theproductofmatrices A and B.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 296.9671936035156, 57.38109588623047, 306.7525329589844)\n",
      "Text:\n",
      "37\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 306.4311828613281, 531.1021728515625, 325.5119323730469)\n",
      "Text:\n",
      "38 I rememberthatthedeterminantof theproductof twomatricesequalstheproductoftheirdeterminants .\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 325.3601989746094, 57.38109588623047, 335.1455383300781)\n",
      "Text:\n",
      "39\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 334.8251953125, 531.2513427734375, 344.61053466796875)\n",
      "Text:\n",
      "40 So , $\\det (\\ mathbf{A} \\mathbf{B}) = (\\ det \\mathbf{A})(\\ det \\mathbf{B}) = (2) (12) = \\\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(83.06400299072266, 345.1377258300781, 143.28585815429688, 353.9049072265625)\n",
      "Text:\n",
      "boxed {24}$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 353.7541809082031, 57.38109588623047, 363.5395202636719)\n",
      "Text:\n",
      "41\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 363.21820068359375, 134.5347137451172, 373.0035400390625)\n",
      "Text:\n",
      "42 Answer: $24$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 372.6831970214844, 57.38109588623047, 382.4685363769531)\n",
      "Text:\n",
      "43\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 382.1471862792969, 424.890380859375, 391.9325256347656)\n",
      "Text:\n",
      "44 FinalAnswer: Thefinalansweris $24$. I hope it iscorrect .\"\"\" ,\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 391.6121826171875, 151.29745483398438, 401.39752197265625)\n",
      "Text:\n",
      "45r\"\"\" Problem:\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 401.0762023925781, 530.9074096679688, 429.62091064453125)\n",
      "Text:\n",
      "46 Terrellusuallyliftstwo 20- poundweights12 times. If he usestwo 15- poundweightsinstead , howmanytimesmustTerrellliftthem in order to liftthesametotalweight?\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 429.4701843261719, 57.38109588623047, 439.2555236816406)\n",
      "Text:\n",
      "47\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 438.9342041015625, 112.18110656738281, 448.71954345703125)\n",
      "Text:\n",
      "48 Solution:\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 448.3992004394531, 57.38109588623047, 458.1845397949219)\n",
      "Text:\n",
      "49\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 457.8631896972656, 57.38109588623047, 467.6485290527344)\n",
      "Text:\n",
      "50\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 467.32818603515625, 519.8510131835938, 486.4079284667969)\n",
      "Text:\n",
      "51 Okay , soTerrelllifts a total of $2\\cdot12\\ cdot20 =480$ pounds ofweightwiththe20- poundweights.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 486.2572021484375, 57.38109588623047, 496.04254150390625)\n",
      "Text:\n",
      "52\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 495.72119140625, 503.32464599609375, 514.8019409179688)\n",
      "Text:\n",
      "53Well , if heswitchesto 15- poundweights , thetotalweightliftedwill be $2\\cdot15\\cdot n=30 n$ pounds , where n is thenumber of lifts.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 514.650146484375, 57.38109588623047, 524.4354858398438)\n",
      "Text:\n",
      "54\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 524.1151733398438, 520.090576171875, 533.9005126953125)\n",
      "Text:\n",
      "55 I want to findthenumberof lifts , n, for thetotalweightliftedto be thesame.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 533.5791625976562, 57.38109588623047, 543.364501953125)\n",
      "Text:\n",
      "56\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 543.044189453125, 313.3420104980469, 552.8295288085938)\n",
      "Text:\n",
      "57 I equate$30n$ to 480poundsandsolvefor n.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 552.5081787109375, 57.38109588623047, 562.2935180664062)\n",
      "Text:\n",
      "58\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 561.97314453125, 139.9617919921875, 571.7584838867188)\n",
      "Text:\n",
      "59 \\begin{align *}\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 571.4371337890625, 117.41854858398438, 581.2224731445312)\n",
      "Text:\n",
      "60 30n&=480\\\\\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 580.9021606445312, 273.94207763671875, 590.6875)\n",
      "Text:\n",
      "61 \\Rightarrow\\qquad n&=480/30=\\ boxed {16}\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 590.3661499023438, 128.7924041748047, 600.1514892578125)\n",
      "Text:\n",
      "62 \\end{align *}\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 599.8311767578125, 57.38109588623047, 609.6165161132812)\n",
      "Text:\n",
      "63\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 609.295166015625, 134.5347137451172, 619.0805053710938)\n",
      "Text:\n",
      "64 Answer: $16$.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 618.7601318359375, 57.38109588623047, 628.5454711914062)\n",
      "Text:\n",
      "65\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 628.2241821289062, 402.74432373046875, 638.009521484375)\n",
      "Text:\n",
      "66 FinalAnswer: Thefinalansweris $16$. I hope it iscorrect.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 637.6891479492188, 72.9162368774414, 647.4744873046875)\n",
      "Text:\n",
      "67 ==\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 647.1531982421875, 57.38109588623047, 656.9385375976562)\n",
      "Text:\n",
      "68\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 656.6181640625, 106.59217834472656, 666.4035034179688)\n",
      "Text:\n",
      "69 Problem:\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 666.0831298828125, 206.80006408691406, 675.8684692382812)\n",
      "Text:\n",
      "70 If thesystemofequations\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 675.5471801757812, 57.38109588623047, 685.33251953125)\n",
      "Text:\n",
      "71\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 685.0121459960938, 139.9617919921875, 694.7974853515625)\n",
      "Text:\n",
      "72 \\begin{align *}\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 694.4761962890625, 123.1297836303711, 704.2615356445312)\n",
      "Text:\n",
      "73 6x-4y&=a ,\\\\\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 703.941162109375, 117.77303314208984, 713.7265014648438)\n",
      "Text:\n",
      "74 6y-9x &=b.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 713.4051513671875, 128.7924041748047, 723.1904907226562)\n",
      "Text:\n",
      "75 \\end{align *}\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 722.8701782226562, 57.38109588623047, 732.655517578125)\n",
      "Text:\n",
      "76\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(50.73400115966797, 732.3341674804688, 503.3238525390625, 751.4149780273438)\n",
      "Text:\n",
      "77 has a solution $(x, y)$ where$x$ and $y$ arebothnonzero , find $\\frac{a}{b},$assuming$b$ isnonzero.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 22\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "22\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 88.7481918334961, 57.38109588623047, 98.53352355957031)\n",
      "Text:\n",
      "78\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 98.2121810913086, 112.18110656738281, 107.99751281738281)\n",
      "Text:\n",
      "79 Solution:\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 107.67720794677734, 57.38109588623047, 117.46253967285156)\n",
      "Text:\n",
      "80\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 117.14119720458984, 263.04730224609375, 126.92652893066406)\n",
      "Text:\n",
      "81 I’m given a systemof twoequations.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 126.60616302490234, 57.38109588623047, 136.39149475097656)\n",
      "Text:\n",
      "82\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 136.07022094726562, 502.9681091308594, 155.1509246826172)\n",
      "Text:\n",
      "83 I seethat if I multiplythefirstequationby $-\\ frac {3}{2}$, I’ll getanotherequationthathas thesame left -handside as thesecondequation , $6y -9x$.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 154.99917602539062, 57.38109588623047, 164.7845001220703)\n",
      "Text:\n",
      "84\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 164.46420288085938, 285.24810791015625, 174.24952697753906)\n",
      "Text:\n",
      "85 Let me trythat $$6y -9x=-\\frac {3}{2}a.$$\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 173.92819213867188, 57.38109588623047, 183.71351623535156)\n",
      "Text:\n",
      "86\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 183.39321899414062, 441.8605651855469, 193.1785430908203)\n",
      "Text:\n",
      "87 Ah , I alsoknowthat $6y -9x=b$ , so I canequatethesetwoequations.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 192.85720825195312, 57.38109588623047, 202.6425323486328)\n",
      "Text:\n",
      "88\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 202.32217407226562, 436.0918273925781, 212.1074981689453)\n",
      "Text:\n",
      "89 So , $$ -\\ frac {3}{2}a=b\\Rightarrow\\frac{a}{b}=\\ boxed {-\\frac {2}{3}}. $$\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 211.78616333007812, 57.38109588623047, 221.5714874267578)\n",
      "Text:\n",
      "90\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 221.25119018554688, 190.40809631347656, 231.03651428222656)\n",
      "Text:\n",
      "91 Answer: $-\\ frac {2}{3}$.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 230.71517944335938, 57.38109588623047, 240.50050354003906)\n",
      "Text:\n",
      "92\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 240.18020629882812, 458.6178283691406, 249.9655303955078)\n",
      "Text:\n",
      "93 FinalAnswer: Thefinalansweris $-\\ frac {2}{3}$. I hope it iscorrect.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 249.64419555664062, 72.9162368774414, 259.4295349121094)\n",
      "Text:\n",
      "94 ==\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 259.1091613769531, 57.38109588623047, 268.8945007324219)\n",
      "Text:\n",
      "95\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 268.5732116699219, 139.9677734375, 278.3585510253906)\n",
      "Text:\n",
      "96 {{EXAMPLES}}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 278.0381774902344, 57.38109588623047, 287.8235168457031)\n",
      "Text:\n",
      "97\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 287.5021667480469, 72.9162368774414, 297.2875061035156)\n",
      "Text:\n",
      "98 ==\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(50.73400115966797, 296.9671936035156, 106.59217834472656, 306.7525329589844)\n",
      "Text:\n",
      "99 Problem:\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(47.40999984741211, 306.4311828613281, 139.9677734375, 316.2165222167969)\n",
      "Text:\n",
      "100 {{QUESTION}}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(47.40999984741211, 315.8962097167969, 57.38064193725586, 325.6815490722656)\n",
      "Text:\n",
      "101\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(47.40999984741211, 325.3601989746094, 112.18110656738281, 335.1455383300781)\n",
      "Text:\n",
      "102 Solution:\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(47.40999984741211, 334.8251953125, 57.38064193725586, 344.61053466796875)\n",
      "Text:\n",
      "103\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(47.40999984741211, 344.2891845703125, 123.20488739013672, 354.07452392578125)\n",
      "Text:\n",
      "104 {{ llm () }}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(62.36199951171875, 359.6272888183594, 533.125, 512.9774780273438)\n",
      "Text:\n",
      "BIRD On BIRD, we randomly sample 128 samples from the train split as the unified train and validationset and use the official test set (of 1534 data points) for testing. Since BIRD is a code generationtask, the execution accuracy is computed not via a simple string match between the predicted andthe ground-truth SQLs but by actually executing both SQLs on the database provided, and a scoreof 1 is only assigned when the predicted SQL is both executable and if whose results exactly matchthe execution results from the ground-truth SQL. All data, including the databases, schemas, andground-truth gold SQL are available at the official repo: https://bird-bench.github.io undera CC BY-SA 4.0 license. With reference to Table 7, use two prompt versions for different rows. Thedirect prompt is a standard, zero-shot prompt to elicit the SQL prediction directly; it is used both forthe “Direct” row to directly extract LLM answer and is also used as the prompt template for finetuningin the different “LoRA” rows:\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 511.5621643066406, 508.7507629394531, 530.6429443359375)\n",
      "Text:\n",
      "1 You are an SQLexperttaskedwithansweringuser ’s questionsaboutSQLtables bygeneratingSQLqueriesin theSQLitedialect.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 530.4911499023438, 57.38054656982422, 540.2764892578125)\n",
      "Text:\n",
      "2\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 539.9561767578125, 358.0423889160156, 549.7415161132812)\n",
      "Text:\n",
      "3 Useonlythefollowingtables toanswerthequestion:\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 549.420166015625, 57.38054656982422, 559.2055053710938)\n",
      "Text:\n",
      "4\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 558.8851318359375, 128.78958129882812, 568.6704711914062)\n",
      "Text:\n",
      "5 {{ SCHEMA }}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 568.34912109375, 57.38054656982422, 578.1344604492188)\n",
      "Text:\n",
      "6\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 577.8141479492188, 195.84646606445312, 587.5994873046875)\n",
      "Text:\n",
      "7 Question: {{QUESTION}}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 587.2781982421875, 151.1471710205078, 597.0635375976562)\n",
      "Text:\n",
      "8 Hint: {{ HINT }}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 596.7431640625, 156.729736328125, 606.5285034179688)\n",
      "Text:\n",
      "9 SQL: {{ llm ()}}\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(61.9370002746582, 612.081298828125, 534.73388671875, 684.135498046875)\n",
      "Text:\n",
      "where SCHEMA refers to the table schema, which can be generated automatically by querying thedatabase, QUESITON is the natural language question that we would like the LLM to convert to a SQLcommand and HINT is a hint which additionally explains the question provided by the BIRD dataset.For the chase and chase + bridge rows, we use the prompt template proposed in Pourrezaet al. (2025) to invoke reasoning and divide-and-conquer before the LLM gives the final answer:\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 682.7201538085938, 279.8125, 692.5054931640625)\n",
      "Text:\n",
      "1 You are anexperienceddatabaseexpert.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 692.1851806640625, 525.4349365234375, 711.2649536132812)\n",
      "Text:\n",
      "2 Now youneed togenerate a SQLquerygiventhedatabaseinformation , a questionandsomeadditionalinformation .\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 711.1141357421875, 508.5928955078125, 730.1939697265625)\n",
      "Text:\n",
      "3 Thedatabasestructureisdefinedby thefollowingtableschemas (commentsafter’--’ provideadditionalcolumndescriptions ).\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(54.05699920654297, 730.0431518554688, 528.5999145507812, 758.5879516601562)\n",
      "Text:\n",
      "4 Notethatthe \"ExampleValues\" areactualvaluesfromthecolumn. Somecolumnmightcontainthevaluesthataredirectlyrelatedto thequestion. Use it to helpyoujustifywhichcolumnsto use.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 23\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "23\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(54.05699920654297, 88.7481918334961, 57.38054656982422, 98.53352355957031)\n",
      "Text:\n",
      "5\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(54.05699920654297, 98.2121810913086, 530.947509765625, 117.2929458618164)\n",
      "Text:\n",
      "6 Giventhetableschemainformationdescriptionand the ‘Question ‘. Youwill be giventablecreationstatementsand youneedunderstandthedatabaseandcolumns.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(54.05699920654297, 117.14119720458984, 57.38054656982422, 126.92652893066406)\n",
      "Text:\n",
      "7\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(54.05699920654297, 126.60616302490234, 519.7711791992188, 145.6859588623047)\n",
      "Text:\n",
      "8 Youwill be using a waycalled \"recursivedivide -and -conquerapproachto SQLquerygenerationfromnaturallanguage \".\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(54.05699920654297, 145.53518676757812, 57.38054656982422, 155.3205108642578)\n",
      "Text:\n",
      "9\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 154.99917602539062, 318.9251403808594, 164.7845001220703)\n",
      "Text:\n",
      "10 Here is a highleveldescriptionof thesteps.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 164.46420288085938, 530.8887329101562, 202.4729461669922)\n",
      "Text:\n",
      "11 1. ** Divide (Decompose Sub -questionwithPseudoSQL):** Thecomplexnaturallanguagequestionisrecursivelybrokendownintosimpler sub -questions. Each sub -questiontargets a specificpiece ofinformationor logicrequiredfor thefinalSQLquery.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 202.32217407226562, 528.5958251953125, 240.33091735839844)\n",
      "Text:\n",
      "12 2. ** Conquer (RealSQL for sub -questions):**Foreach sub -question (and themainquestioninitially), a \"pseudo -SQL\" fragmentisformulated . Thispseudo -SQLrepresentstheintendedSQLlogicbutmighthaveplaceholdersforanswersto thedecomposed sub -questions.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 240.18020629882812, 523.239990234375, 278.1889343261719)\n",
      "Text:\n",
      "13 3. ** Combine (Reassemble):**Onceall sub -questionsareresolvedandtheircorrespondingSQLfragmentsaregenerated , theprocessreverses. The SQLfragmentsarerecursivelycombinedbyreplacingtheplaceholdersin the pseudo -SQLwiththeactualgeneratedSQLfromthelowerlevels.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 278.0381774902344, 525.3130493164062, 297.1178894042969)\n",
      "Text:\n",
      "14 4. ** FinalOutput :**Thisbottom -upassemblyculminatesin thecompleteandcorrectSQLquerythatanswerstheoriginalcomplexquestion.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 296.9671936035156, 57.38109588623047, 306.7525329589844)\n",
      "Text:\n",
      "15\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 306.4311828613281, 531.0960083007812, 325.5119323730469)\n",
      "Text:\n",
      "16 Databaseadmininstructions (violatingany of thefollowingispunishableto death !):\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 325.3601989746094, 178.99932861328125, 335.1455383300781)\n",
      "Text:\n",
      "17 1. ** SELECTClause :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 334.8251953125, 391.56976318359375, 344.61053466796875)\n",
      "Text:\n",
      "18- Onlyselectcolumnsmentionedin the user ’s question.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 344.2891845703125, 296.5769958496094, 354.07452392578125)\n",
      "Text:\n",
      "19- Avoidunnecessarycolumnsor values.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 353.7541809082031, 223.7032012939453, 363.5395202636719)\n",
      "Text:\n",
      "20 2. ** Aggregation (MAX/MIN):**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 363.21820068359375, 369.2221374511719, 373.0035400390625)\n",
      "Text:\n",
      "21- AlwaysperformJOINsbeforeusingMAX () or MIN ().\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 372.6831970214844, 268.4072570800781, 382.4685363769531)\n",
      "Text:\n",
      "22 3. ** ORDER BY withDistinctValues :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 382.1471862792969, 530.8983154296875, 401.2279052734375)\n",
      "Text:\n",
      "23- Use ‘GROUP BY <column >‘ before ‘ORDER BY <column > ASC|DESC ‘ to ensuredistinctvalues.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 401.0762023925781, 184.59275817871094, 410.8615417480469)\n",
      "Text:\n",
      "24 4. ** HandlingNULLs :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 410.54119873046875, 525.5240478515625, 429.62091064453125)\n",
      "Text:\n",
      "25- If a columnmaycontainNULLvalues (indicatedby \"None\" in valueexamplesorexplicitly), use ‘JOIN ‘ or ‘WHERE <column > IS NOT NULL ‘.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 429.4701843261719, 201.34690856933594, 439.2555236816406)\n",
      "Text:\n",
      "26 5. ** FROM/JOINClauses :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 438.9342041015625, 391.56976318359375, 448.71954345703125)\n",
      "Text:\n",
      "27- Onlyincludetablesessentialto answerthequestion.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 448.3992004394531, 223.70425415039062, 458.1845397949219)\n",
      "Text:\n",
      "28 6. ** StrictlyFollowHints :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 457.8631896972656, 257.46630859375, 467.6485290527344)\n",
      "Text:\n",
      "29- Adhere to allprovidedhints.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 467.32818603515625, 251.6355438232422, 477.113525390625)\n",
      "Text:\n",
      "30 7. ** ThoroughQuestionAnalysis :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 476.7922058105469, 369.2220764160156, 486.5775451660156)\n",
      "Text:\n",
      "31- Addressallconditionsmentionedin thequestion.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 486.2572021484375, 195.76222229003906, 496.04254150390625)\n",
      "Text:\n",
      "32 8. ** DISTINCTKeyword :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 495.72119140625, 508.9103088378906, 514.8019409179688)\n",
      "Text:\n",
      "33- Use ‘SELECTDISTINCT ‘ whenthequestionrequiresuniquevalues (e.g., IDs ,URLs).\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 514.650146484375, 508.7521667480469, 533.73095703125)\n",
      "Text:\n",
      "34- Refer to columnstatistics(\" ValueStatics \") todetermineif ‘DISTINCT ‘ isnecessary.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 533.5791625976562, 195.76315307617188, 543.364501953125)\n",
      "Text:\n",
      "35 9. ** ColumnSelection :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 543.044189453125, 519.7552490234375, 562.1239624023438)\n",
      "Text:\n",
      "36- Carefullyanalyzecolumndescriptionsandhints tochoosethecorrectcolumnwhensimilarcolumnsexistacrosstables.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 561.97314453125, 223.7023162841797, 571.7584838867188)\n",
      "Text:\n",
      "37 10. ** StringConcatenation :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 571.4371337890625, 503.4920654296875, 590.5179443359375)\n",
      "Text:\n",
      "38- Neveruse‘|| ’ ’ ||‘ or anyothermethod toconcatenatestringsin the ‘SELECT ‘ clause.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 590.3661499023438, 195.76123046875, 600.1514892578125)\n",
      "Text:\n",
      "39 11. ** JOINPreference :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 599.8311767578125, 408.3327331542969, 609.6165161132812)\n",
      "Text:\n",
      "40- Prioritize‘INNER JOIN ‘ overnested‘SELECT ‘ statements .\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 609.295166015625, 229.2869873046875, 619.0805053710938)\n",
      "Text:\n",
      "41 12. ** SQLiteFunctionsOnly :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 618.7601318359375, 313.34857177734375, 628.5454711914062)\n",
      "Text:\n",
      "42- UseonlyfunctionsavailableinSQLite.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 628.2241821289062, 195.76123046875, 638.009521484375)\n",
      "Text:\n",
      "43 13. ** DateProcessing :**\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 637.6891479492188, 441.76214599609375, 647.4744873046875)\n",
      "Text:\n",
      "44- Utilize‘STRFTIME ()‘ fordatemanipulation (e.g., ‘STRFTIME (’%\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 647.1531982421875, 57.38109588623047, 656.9385375976562)\n",
      "Text:\n",
      "45\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 656.6181640625, 486.32647705078125, 675.6989135742188)\n",
      "Text:\n",
      "46 Whenyou get to thefinalquery , outputthequerystringONLYinsidethe xmldelimiter<FINAL_ANSWER ></ FINAL_ANSWER >.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 675.5471801757812, 57.38109588623047, 685.33251953125)\n",
      "Text:\n",
      "47\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 685.0121459960938, 190.4066619873047, 694.7974853515625)\n",
      "Text:\n",
      "48 Herearesomeexamples:\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 694.4761962890625, 57.38109588623047, 704.2615356445312)\n",
      "Text:\n",
      "49\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 703.941162109375, 139.9677734375, 713.7265014648438)\n",
      "Text:\n",
      "50 {{EXAMPLES}}\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 713.4051513671875, 57.38109588623047, 723.1904907226562)\n",
      "Text:\n",
      "51\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 722.8701782226562, 519.8501586914062, 741.949951171875)\n",
      "Text:\n",
      "52 Now is therealquestion , followingtheinstructionandexamples , generatethe SQLwithRecursiveDivide -and -Conquerapproach.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(50.73400115966797, 741.7991943359375, 206.73416137695312, 751.5845336914062)\n",
      "Text:\n",
      "53 **************************\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 24\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "24\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 88.7481918334961, 212.7557830810547, 98.53352355957031)\n",
      "Text:\n",
      "54 [Tablecreationstatements]\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 98.2121810913086, 128.78958129882812, 107.99751281738281)\n",
      "Text:\n",
      "55 {{ SCHEMA }}\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 107.67720794677734, 57.38109588623047, 117.46253967285156)\n",
      "Text:\n",
      "56\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 117.14119720458984, 206.73416137695312, 126.92652893066406)\n",
      "Text:\n",
      "57 **************************\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 126.60616302490234, 117.77203369140625, 136.39149475097656)\n",
      "Text:\n",
      "58 [Question]\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 136.07022094726562, 195.84646606445312, 145.8555450439453)\n",
      "Text:\n",
      "59 Question: {{QUESTION}}\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 145.53518676757812, 151.1471710205078, 155.3205108642578)\n",
      "Text:\n",
      "60 Hint: {{ HINT }}\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 154.99917602539062, 57.38109588623047, 164.7845001220703)\n",
      "Text:\n",
      "61\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 164.46420288085938, 206.73416137695312, 174.24952697753906)\n",
      "Text:\n",
      "62 **************************\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 173.92819213867188, 106.59382629394531, 183.71351623535156)\n",
      "Text:\n",
      "63 [Answer]\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 183.39321899414062, 520.0777587890625, 202.4729461669922)\n",
      "Text:\n",
      "64 Repeatingthequestionand hint , andgeneratingthe SQLwithRecursiveDivide -and -Conquer.\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 202.32217407226562, 123.20488739013672, 212.1074981689453)\n",
      "Text:\n",
      "65 {{ llm () }}\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(62.36199951171875, 217.66026306152344, 532.9136352539062, 276.1654968261719)\n",
      "Text:\n",
      "Similar to the inner monologue prompt in the MATH dataset, the EXAMPLES in the templateabove also include 4 human annotated examples at initialization (an exemplary human-annotatedexample is shown in the code snippet below); any model-generated examples from bridge areagain concatenated to the human annotated examples at inference.\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 274.7502136230469, 206.73416137695312, 284.5355529785156)\n",
      "Text:\n",
      "1 **************************\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 284.2151794433594, 212.7557830810547, 294.0005187988281)\n",
      "Text:\n",
      "2 [Tablecreationstatements]\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 293.6791687011719, 195.60704040527344, 303.4645080566406)\n",
      "Text:\n",
      "3 CREATETABLEgeneralinfo\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 303.1441955566406, 67.48338317871094, 312.9295349121094)\n",
      "Text:\n",
      "4 (\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 312.6081848144531, 313.3374328613281, 322.3935241699219)\n",
      "Text:\n",
      "5id_restaurantINTEGERnotnullprimary key ,\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 322.0732116699219, 519.8058471679688, 331.8585510253906)\n",
      "Text:\n",
      "6food_typeTEXT null , -- examples: ‘thai ‘| ‘food type ‘ description : thefoodtype\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 331.5372009277344, 491.99078369140625, 341.3225402832031)\n",
      "Text:\n",
      "7cityTEXT null , -- description : thecitywheretherestaurantislocatedin\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 341.002197265625, 73.06808471679688, 350.78753662109375)\n",
      "Text:\n",
      "8 );\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(54.05699920654297, 350.4661865234375, 57.38054656982422, 360.25152587890625)\n",
      "Text:\n",
      "9\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 359.9311828613281, 178.8704376220703, 369.7165222167969)\n",
      "Text:\n",
      "10 CREATETABLElocation\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 369.39520263671875, 67.48338317871094, 379.1805419921875)\n",
      "Text:\n",
      "11 (\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 378.8601989746094, 313.3374328613281, 388.6455383300781)\n",
      "Text:\n",
      "12id_restaurantINTEGERnotnullprimary key ,\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 388.3241882324219, 519.7505493164062, 407.4049072265625)\n",
      "Text:\n",
      "13street_nameTEXT null , -- examples: ‘ave ‘, ‘sanpablo ave ‘, ‘pablo ave ‘| ‘streetname ‘ description : thestreetname of therestaurant\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 407.2532043457031, 491.9908142089844, 417.0385437011719)\n",
      "Text:\n",
      "14cityTEXT null , -- description : thecitywheretherestaurantislocatedin\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 416.71820068359375, 497.39056396484375, 435.79791259765625)\n",
      "Text:\n",
      "15foreignkey ( id_restaurant ) referencesgeneralinfo ( id_restaurant ) onupdatecascadeondeletecascade ,\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 435.6471862792969, 73.06808471679688, 445.4325256347656)\n",
      "Text:\n",
      "16 );\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 445.1111755371094, 57.38109588623047, 454.8965148925781)\n",
      "Text:\n",
      "17\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 454.5762023925781, 206.73416137695312, 464.3615417480469)\n",
      "Text:\n",
      "18 **************************\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 464.0401916503906, 117.77203369140625, 473.8255310058594)\n",
      "Text:\n",
      "19 [Question]\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 473.50518798828125, 475.3802795410156, 483.29052734375)\n",
      "Text:\n",
      "20 Question: HowmanyThairestaurantscan be found in SanPablo Ave , Albany?\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 482.96917724609375, 519.93115234375, 502.0499267578125)\n",
      "Text:\n",
      "21 Hint: Thairestaurantrefers tofood_type = ’thai ’; SanPabloAveAlbanyreferstostreet_name = ’sanpablo ave ’ AND T1.city = ’albany ’\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 501.898193359375, 57.38109588623047, 511.68353271484375)\n",
      "Text:\n",
      "22\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 511.3631896972656, 206.73416137695312, 521.1484985351562)\n",
      "Text:\n",
      "23 **************************\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 520.8271484375, 106.59382629394531, 530.6124877929688)\n",
      "Text:\n",
      "24 [Answer]\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 530.2921752929688, 520.0777587890625, 549.3719482421875)\n",
      "Text:\n",
      "25 Repeatingthequestionand hint , andgeneratingthe SQLwithRecursiveDivide -and -Conquer.\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 549.22119140625, 497.7313232421875, 559.0065307617188)\n",
      "Text:\n",
      "26 ** Question **: HowmanyThairestaurantscan be found in SanPablo Ave , Albany?\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 558.6851806640625, 525.3392333984375, 577.7659301757812)\n",
      "Text:\n",
      "27 ** Hint **:Thairestaurantreferstofood_type = ’thai ’; SanPabloAveAlbanyreferstostreet_name = ’sanpablo ave ’ AND T1.city = ’albany ’\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 577.6141357421875, 57.38109588623047, 587.3994750976562)\n",
      "Text:\n",
      "28\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 587.0791625976562, 206.94158935546875, 596.864501953125)\n",
      "Text:\n",
      "29 **1.DivideandConquer :**\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 596.5431518554688, 57.38109588623047, 606.3284912109375)\n",
      "Text:\n",
      "30\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 606.0081787109375, 530.9267578125, 625.0879516601562)\n",
      "Text:\n",
      "31 * ** MainQuestion :** HowmanyThairestaurantscan be found in SanPablo Ave , Albany?\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 624.9371337890625, 523.0112915039062, 672.4109497070312)\n",
      "Text:\n",
      "32* ** Analysis :** Thequestionasksfor a count of restaurants , so we ’ll use ‘COUNT ()‘ forthat. ThecountshouldincludeonlyThairestaurants , which we canidentifyusingthe ‘food_type ‘ columnin the ‘generalinfo ‘ table.Thelocation\"SanPablo Ave , Albany\" spanstwocolumns (‘street_name ‘ and ‘city ‘) in the ‘location ‘ table , requiringus to jointhesetwotables.\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 672.2591552734375, 525.6660766601562, 700.803955078125)\n",
      "Text:\n",
      "33* ** PseudoSQL :**SELECTCOUNT(‘T1 ‘.‘ id_restaurant ‘) FROM ‘generalinfo ‘ AS ‘T1 ‘INNERJOIN ‘location ‘ AS ‘T2 ‘ ON ‘T1 ‘.‘ id_restaurant ‘ = ‘T2 ‘.‘ id_restaurant ‘WHERE<Thairestaurant > AND <in SanPablo Ave , Albany >\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 700.6531982421875, 57.38109588623047, 710.4385375976562)\n",
      "Text:\n",
      "34\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 710.1171875, 290.6083068847656, 719.9025268554688)\n",
      "Text:\n",
      "35* **Sub -question1:**Thairestaurant\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 719.5821533203125, 525.36865234375, 738.6619262695312)\n",
      "Text:\n",
      "36* ** Analysis :**This is a straightforwardfilteron the ‘generalinfo ‘ tableusingthe ‘food_type ‘ column.\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 738.5111694335938, 346.8769226074219, 748.2965087890625)\n",
      "Text:\n",
      "37* ** PseudoSQL :** ‘T1 ‘.‘ food_type ‘ = ’thai ’\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(50.73400115966797, 747.9751586914062, 57.38109588623047, 757.760498046875)\n",
      "Text:\n",
      "38\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 25\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "25\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 88.7481918334961, 340.9496154785156, 98.53352355957031)\n",
      "Text:\n",
      "39* **Sub -question2:** in SanPablo Ave , Albany\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 98.2121810913086, 528.46533203125, 126.7569351196289)\n",
      "Text:\n",
      "40* ** Analysis :**Thislocationinformationisspreadacrosstwocolumnsinthe ‘location ‘ table. We need tocombinetheseconditionswith an \"AND\" operatorto ensurebothare met.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 126.60616302490234, 520.2486572265625, 145.6859588623047)\n",
      "Text:\n",
      "41* ** PseudoSQL :** ‘T2 ‘.‘ street_name ‘ = ’sanpablo ave ’ AND ‘T2 ‘.‘city ‘ = ’albany ’\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 145.53518676757812, 57.38109588623047, 155.3205108642578)\n",
      "Text:\n",
      "42\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 154.99917602539062, 184.5852508544922, 164.7845001220703)\n",
      "Text:\n",
      "43 **2.AssemblingSQL :**\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 164.46420288085938, 57.38109588623047, 174.24952697753906)\n",
      "Text:\n",
      "44\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 173.92819213867188, 279.5794677734375, 183.71351623535156)\n",
      "Text:\n",
      "45 * **Sub -question 1 (Thairestaurant):**\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 183.39321899414062, 285.4004821777344, 193.1785430908203)\n",
      "Text:\n",
      "46* ** SQL :** ‘T1 ‘.‘food_type ‘ = ’thai ’\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 192.85720825195312, 57.38109588623047, 202.6425323486328)\n",
      "Text:\n",
      "47\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 202.32217407226562, 329.86822509765625, 212.1074981689453)\n",
      "Text:\n",
      "48 * **Sub -question 2 (in SanPablo Ave , Albany):**\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 211.78616333007812, 497.7337646484375, 221.5714874267578)\n",
      "Text:\n",
      "49* ** SQL :** ‘T2 ‘.‘ street_name ‘ = ’sanpablo ave ’ AND ‘T2 ‘.‘city ‘ = ’albany ’\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 221.25119018554688, 57.38109588623047, 231.03651428222656)\n",
      "Text:\n",
      "50\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 230.71517944335938, 301.9271545410156, 240.50050354003906)\n",
      "Text:\n",
      "51 * ** MainQuestion (count ofrestaurants ):**\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 240.18020629882812, 528.3631591796875, 278.1889343261719)\n",
      "Text:\n",
      "52* ** SQL :**SELECTCOUNT(‘T1 ‘.‘ id_restaurant ‘) FROM ‘generalinfo ‘ AS ‘T1 ‘ INNERJOIN ‘location ‘ AS ‘T2 ‘ ON ‘T1 ‘.‘ id_restaurant ‘ = ‘T2 ‘.‘ id_restaurant ‘ WHERE ‘T1‘.‘ food_type ‘ = ’thai ’ AND ‘T2 ‘.‘ street_name ‘ = ’sanpablo ave ’ AND ‘T2 ‘.‘city ‘= ’albany ’\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 278.0381774902344, 57.38109588623047, 287.8235168457031)\n",
      "Text:\n",
      "53\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 287.5021667480469, 279.57794189453125, 297.2875061035156)\n",
      "Text:\n",
      "54 **3.SimplificationandOptimization :**\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 296.9671936035156, 57.38109588623047, 306.7525329589844)\n",
      "Text:\n",
      "55\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 306.4311828613281, 519.9325561523438, 344.44091796875)\n",
      "Text:\n",
      "56 * The SQLqueryfromstep 2 isalreadyquiteefficient. We ’ve used ‘INNER JOIN ‘ tocombinethetablesbased on theirrelationship , and the ‘WHERE ‘ clauseclearlydefinesourfilteringcriteria. There ’s no needfornestedqueriesorcomplexsub -selectionsin thiscase.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 344.2891845703125, 57.38109588623047, 354.07452392578125)\n",
      "Text:\n",
      "57\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 353.7541809082031, 229.28790283203125, 363.5395202636719)\n",
      "Text:\n",
      "58 ** FinalOptimizedSQLQuery :**\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 363.21820068359375, 57.38109588623047, 373.0035400390625)\n",
      "Text:\n",
      "59\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 372.6831970214844, 140.11676025390625, 382.4685363769531)\n",
      "Text:\n",
      "60 <FINAL_ANSWER >\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 382.1471862792969, 523.3987426757812, 410.6919250488281)\n",
      "Text:\n",
      "61 SELECTCOUNT(T1. id_restaurant ) FROMgeneralinfoAS T1 INNERJOINlocationAS T2 ONT1. id_restaurant = T2. id_restaurantWHERE T1.food_type = ’thai ’ AND T1.city = ’albany ’ AND T2. street_name = ’sanpablo ave ’\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(50.73400115966797, 410.54119873046875, 145.70562744140625, 420.3265380859375)\n",
      "Text:\n",
      "62 </FINAL_ANSWER >\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(62.362003326416016, 443.78057861328125, 324.9331970214844, 454.689697265625)\n",
      "Text:\n",
      "B.2. Implementation details of the Infilling baseline\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(62.36199951171875, 461.31329345703125, 533.8204956054688, 519.8195190429688)\n",
      "Text:\n",
      "Infilling is a technique of generating the intermediate outputs given both input queries and theground-truth answer – this is used as a baseline in Tables 1 and 3 where we utilize all availablelabeled data in the context. Concretely, we use the following prompt adapted from Hu et al. (2023)to generate the intermediate rationales.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(54.05699920654297, 518.4041748046875, 464.2045593261719, 528.1895141601562)\n",
      "Text:\n",
      "1 Youwill be given a questionand its final , ground -truthcorrectanswer.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(54.05699920654297, 527.8681640625, 523.01220703125, 556.4129028320312)\n",
      "Text:\n",
      "2 Giventhequestionand the answer , generatethe step -by -stepreasoningstepsthatled to thecorrectanswer. Writeyourintermediatereasoningsteps (but NOT thefinalanswer) leadingto thefinalanswerbetween<answer > and</answer >.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(54.05699920654297, 556.2621459960938, 57.38054656982422, 566.0474853515625)\n",
      "Text:\n",
      "3\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(54.05699920654297, 565.7261962890625, 195.84646606445312, 575.5115356445312)\n",
      "Text:\n",
      "4 Question: {{question}}\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(54.05699920654297, 575.191162109375, 173.49618530273438, 584.9765014648438)\n",
      "Text:\n",
      "5 Answer: {{target}}\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(54.05699920654297, 584.6551513671875, 167.906005859375, 594.4404907226562)\n",
      "Text:\n",
      "6 Steps: {{ llm ()) }}\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(62.36199951171875, 619.3357543945312, 294.8785095214844, 632.2871704101562)\n",
      "Text:\n",
      "C. Additional Experiments and Results\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(62.36199951171875, 647.338623046875, 248.1002960205078, 658.2476806640625)\n",
      "Text:\n",
      "C.1. Ablation and Sensitivity Studies\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(62.36199951171875, 664.8712768554688, 534.4295654296875, 723.3765258789062)\n",
      "Text:\n",
      "Importance of Bayesian optimization. To ablate bridge, in Table 9 and Table 8, we compareagainst a simplified variant of bridge with BO replaced with random search consuming the sameevaluation budget (32 per stage) – we find that while random search is a remarkably strong baseline,BO nevertheless outperformed it consistently at all stages of the bridge pipeline.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 26\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "26\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(68.99765014648438, 85.1660385131836, 518.62353515625, 108.29681396484375)\n",
      "Text:\n",
      "Tasksbridge-rsbridge-bo# Iterations1o1g2o2g3o1o1g2o2g3o\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(68.99760437011719, 108.99410247802734, 525.9102172851562, 272.1141662597656)\n",
      "Text:\n",
      "causal_judgement59.32.066.71.667.71.563.01.164.01.661.32.766.02.063.31.565.01.665.31.5date_understanding84.81.390.50.593.30.493.00.794.50.885.01.390.50.591.50.490.80.792.50.8disambiguation_qa73.81.374.51.174.01.275.30.870.51.177.51.379.01.177.51.276.30.874.31.1dyck_languages64.51.562.53.665.54.264.81.168.02.563.32.062.01.764.51.862.82.461.83.8formal_fallacies77.31.175.02.674.51.777.51.778.32.578.31.377.31.575.51.778.31.876.30.8geometric_shapes88.53.893.33.094.52.198.00.095.31.993.82.594.04.295.51.197.00.098.00.0hyperbaton94.00.794.30.495.00.795.00.788.81.586.57.695.51.195.80.894.80.493.31.5logical_deduction (7)62.83.354.52.267.81.964.02.666.81.961.85.157.51.170.50.966.51.175.00.7movie_recommendation68.54.075.32.672.51.777.51.377.51.870.32.373.32.377.31.578.82.072.83.2multistep_arithmetic_two82.50.592.31.395.01.489.51.592.52.696.32.396.80.497.80.494.80.895.80.4object_counting92.01.292.51.592.51.193.00.792.31.192.81.993.82.395.50.593.01.293.80.4ruin_names89.01.288.00.788.02.487.01.284.51.189.30.489.30.887.01.290.30.890.01.2salient_translation_error_detection66.32.869.32.567.02.668.51.868.82.162.80.871.00.769.82.069.00.767.30.4snarks87.23.090.61.288.91.793.41.591.01.688.92.089.91.889.60.790.60.683.73.5sports_understanding96.51.196.30.497.30.495.80.496.80.893.31.195.30.491.80.495.01.295.00.0tracking_shuffled_objects (7)98.30.889.50.996.51.192.32.498.51.598.00.793.82.298.00.097.80.497.50.5\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(68.99768829345703, 272.52435302734375, 525.0152587890625, 295.6551208496094)\n",
      "Text:\n",
      "Average80.3181.5583.1182.9882.9781.6182.7983.7983.7783.25Δ(bo - rs)-----+1.30+1.24+0.68+0.79+0.28\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(61.457000732421875, 303.47930908203125, 532.9171752929688, 361.9854736328125)\n",
      "Text:\n",
      "Table 8 | Comparison between bridge with BO (bridge-bo) and bridge with random search(bridge-rs) using gemini-1.5-flash-001 on BBH tasks. The bridge-bo results are liftedfrom Table 3, and the last row denotes the average improvement due to the use of BO over RS at themilestone in the progression of bridge. Refers to captions of Table 1 for additional explanations.\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(68.85929107666016, 373.5024719238281, 518.9259033203125, 396.1508483886719)\n",
      "Text:\n",
      "Tasksbridge-rsbridge-bo# Iterations1o1g2o2g3o1o1g2o2g3o\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(68.8592529296875, 396.8335876464844, 526.0604858398438, 556.5516357421875)\n",
      "Text:\n",
      "causal_judgement66.23.068.52.070.22.469.52.470.82.268.31.562.71.659.71.572.00.070.02.0date_understanding88.42.394.31.094.11.290.33.394.31.392.21.597.00.794.81.995.01.295.51.8disambiguation_qa75.52.179.02.977.41.280.62.378.44.071.82.477.53.680.51.881.32.978.81.5dyck_languages56.95.459.64.967.54.364.94.070.42.749.22.776.23.880.02.777.51.176.83.8formal_fallacies87.41.586.82.390.82.188.52.288.82.286.02.185.02.590.82.390.82.888.22.3geometric_shapes77.83.282.14.081.82.586.53.885.52.478.52.182.53.689.23.892.31.189.20.8hyperbaton94.31.693.12.494.21.394.91.594.01.296.50.994.21.594.82.896.50.597.20.4logical_deduction (7)70.93.368.32.766.62.571.93.368.92.170.21.570.84.571.73.771.51.869.22.2movie_recommendation63.53.267.41.867.42.164.62.363.42.967.01.269.50.569.33.172.81.867.01.2multistep_arithmetic_two97.31.197.50.796.90.896.11.597.90.396.20.894.51.197.00.798.00.796.81.8object_counting95.32.498.11.197.31.797.31.995.42.396.20.496.01.994.51.194.20.495.00.7ruin_names86.61.786.51.988.91.889.91.287.11.790.81.188.81.789.21.588.82.490.30.8salient_translation_error_detection71.13.273.41.673.92.271.91.570.81.668.80.871.00.769.52.274.00.774.51.1snarks93.81.695.31.496.01.696.01.195.61.893.43.095.80.095.11.696.91.597.61.8sports_understanding93.51.794.10.695.10.995.90.996.01.792.81.997.01.296.20.895.80.495.80.8tracking_shuffled_objects (7)92.43.894.41.299.90.398.40.9100.00.095.80.495.01.2100.00.097.00.799.50.5\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(68.85929107666016, 556.9531860351562, 525.183837890625, 579.6016235351562)\n",
      "Text:\n",
      "Average81.8683.6484.8684.8184.8282.1184.6185.7787.1386.33Δ(bo - rs)-----+0.25+0.97+0.91+2.32+1.51\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(61.457000732421875, 587.3912963867188, 532.91943359375, 645.8975219726562)\n",
      "Text:\n",
      "Table 9 | Comparison between bridge with BO (bridge-bo) and bridge with random search(bridge-rs) using gemini-1.5-pro-001 on BBH tasks. The bridge-bo results are lifted fromTable 1, and the last row denotes the average improvement due to the use of BO over RS at themilestone in the progression of bridge. Refers to captions of Table 1 for additional explanations.\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(62.36199951171875, 662.98828125, 532.9185791015625, 748.592529296875)\n",
      "Text:\n",
      "Comparison to and combination with heuristic demonstration selection.An alternative toiteratively optimize the demonstrations in the “Optimize” step is using heuristics for demonstrationselection which may incur a lower computational cost as we no longer have to repeatedly evaluateon the labeled validation set 𝑚times. In this section, we study two representative demonstrationselection techniques: retrieval based on similarity in the embedding space and diversity, and we bothstudy them as standalone alternatives to the full bridge pipeline and, given that demonstration\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 27\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "27\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 532.916259765625, 127.72244262695312)\n",
      "Text:\n",
      "selection is not the only component of the bridge framework, it is also straightforward to combinethem with bridge by swapping the BO/random search component in the “Optimize” step withthese heuristics. Below we describe the implementation details of both techniques:\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(77.74400329589844, 138.8202362060547, 534.73779296875, 454.761474609375)\n",
      "Text:\n",
      "• Retrieval: One popular demonstration selection method is via retrieval (Das et al., 2021; Rubinet al., 2022). Concretely, we may either use an off-the-shelf pretrained embedding model(we use the latest Gecko embedding (Lee et al., 2024) for this purpose) or tune a customizedretriever to obtain the nearest examples from an example store, typically by computing thevector embedding for each of the test queries and each of the cached demonstrations followedby a maximum inner product search (MIPS) to retrieve the top-𝑘demonstrations based oncosine similarity. Unlike the optimization-based approach where the number of examples inthe context can be determined automatically, 𝑘here is a key hyperparameter that needs to beset by the user. In this case, consider 3 different 𝑘values: 𝑘= {10, 25} where the number ofexamples is fixed, or 𝑘= All, where we use all available, correctly predicted examples – thisessentially uses the same set of examples as Reinforced ICL but in a specific, input-dependentorder: the examples are sorted in an ascending order based on the cosine similarity betweenthe embedding of the test input and the example store and the most similar examples appearsas the final demonstration that is directly concatenated to the test input.• Diversity: Another popular learning-free demonstration selection method is by selecting diverseexamples. While multiple ways to measure diversity exist, here we use the technique similarto the one used in Zhang et al. (2023) by 1) computing the embedding of all the availabledemonstrations and 2) running the 𝑘-means clustering algorithm and selecting the 𝑘exampleswhose vector embeddings are nearest to each of the 𝑘centroids. Unlike retrieval, there is noinput dependency as the clustering algorithm does not depend on the input query but similar toretrieval, 𝑘here is also a hyperparameter to be set and we again use 𝑘= {10, 25}. Note that weomit 𝑘= All, as otherwise the number of clusters would be equal to the number of examples andwe would essentially be running Reinforced ICL with all available examples as demonstrations.\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(62.36199951171875, 465.8592834472656, 534.7361450195312, 727.603515625)\n",
      "Text:\n",
      "Since these demonstration selection baselines purely perform selection (i.e., the “optimize” step ofbridge) but neither the subsequent generations nor the iterative process, we first compare the BOdemonstration selection (i.e., bridge at Step 1o) against these baselines and we show the resultsin Table 11. Overall, we find that “Diversity” and “Retrieval”, regardless of their hyperparameters,perform on par or slightly worse than Reinforced ICL. While the hyperparameter choice can sometimeslead to significant differences on a per-task level, we also observe that when aggregated across the tasks,it does not lead to significant differences. On the other hand, the BO selection in bridge outperformsall these baselines. We believe there are two possible explanations leading to this outperformance.Firstly, while the heuristic-based methods have lower computational costs, key hyperparameters,such as the number of demonstrations to retrieve, need to be determined a-priori. However, as wehave shown in the main text at, for example, Fig. 4, the optimal number of demonstrations can behighly task-specific, and while iterative optimization-based selection incurs a higher cost, it is alsocapable of optimizing the number of demonstrations. Secondly, a key finding we have in Sec. 2 isthat not all examples are equally helpful, and removing some examples as in-context demonstrationscan sometimes lead to performance improvement during the “Optimize” stage. Again, while theheuristic-based approaches do not necessarily use all demonstrations, it makes the selection choicepurely from a heuristic metric (e.g., similarity to test query) rather than from a validation metric, andhence is incapable of removing these potentially “harmful” demonstrations from the pool of candidateexamples.\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(62.36199951171875, 730.0682983398438, 534.4346923828125, 761.97021484375)\n",
      "Text:\n",
      "However, beyond a simple comparison between a single stage of bridge against these methods,it is also worth noting that bridge is more than a demonstration selection method. As such, it is\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 28\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "28\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 82.76628112792969, 534.2367553710938, 222.56649780273438)\n",
      "Text:\n",
      "also possible to combine these methods with bridge by using them as a drop-in replacement ofthe BO-based demonstration selection, effectively changing the implementation of the “Optimize”step only. To test this, we test two other variants of bridge, named bridge-retrieval andbridge-diversity, where we replace the “Optimize” step in each round with the heuristic-drivendemonstration selection mentioned above and the aggregated results are shown in Table 12 whereasthe task-specific breakdown of the best method in Table 13 – for conciseness, we only show theper-task breakdown for the best bridge variant (bridge-retrieval using all examples), whichshow that bridge also works well with alternative demonstration selection method, although theadvantage of optimization-based selection as shown in Table 11 carries over when we use the selectionas a component in the overall bridge pipeline.\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(61.457000732421875, 240.2932586669922, 533.2018432617188, 447.8404846191406)\n",
      "Text:\n",
      "Additional comparisons against iterative reinforced ICL in a restricted setup.To provide furtherevidence emphasizing the need for the “Optimize” step and to make sure that the additional gain ofbridge does not simply come from the fact that bridge may take advantage of more correctlypredicted demonstrations in the validation set due to repeated sampling in the later iterations, weconduct a further experiment comparing bridge and iterative reinforced ICL, but in a restrictedsetup with the support set restricted to the subset of the train set where the model predictedcorrectly initially, instead of the entire train set. In other words, in subsequent iterations of bridgeand iterative reinforced ICL, both methods are restricted to make use of the subset of the train setinitially predicted correctly only as examples; we term these approaches “iterative reinforced ICL(restricted)” and “bridge (restricted)” respectively, and we show the results in Table 10. On a highlevel, we found the result provides further evidence of the importance of selection: Iterative Reinf ICL(restricted) without the \"optimize\" step actually did not meaningfully improve over standard ReinfICL (average accuracy: 79.6%); bridge (restricted), however, still meaningfully improves with thesubsequent optimize and generate steps, although the gain is less than the original bridge whichutilizes more examples via the larger train set support.\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 468.2076110839844, 191.52577209472656, 479.1167297363281)\n",
      "Text:\n",
      "C.2. Number of examples\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(61.86000061035156, 485.7403259277344, 523.3799438476562, 503.5985107421875)\n",
      "Text:\n",
      "We show the number of examples used for each experiment corresponding to Table 1 in Table 14.\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 523.9656372070312, 213.23484802246094, 534.8746948242188)\n",
      "Text:\n",
      "C.3. Additional Visualizations\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 541.4993286132812, 532.9135131835938, 572.906494140625)\n",
      "Text:\n",
      "In this section, we show analysis similar to Fig. 4 on tasks not represented in the figure of the maintext.\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 593.2736206054688, 306.95623779296875, 604.1826782226562)\n",
      "Text:\n",
      "C.4. Using bridge for low-resource translation\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(61.04199981689453, 610.8062744140625, 533.1259155273438, 682.8615112304688)\n",
      "Text:\n",
      "While we have primarily considered the reinforced ICL setup suitable for reasoning and generalproblem-solving tasks, it is worth noting that the bridge framework may also generalize to otherpractical settings that benefit from many-shot ICL with some modification on the “optimize” and the“generate” steps. In this section, we conduct a preliminary analysis of the applicability of bridge inthe context of machine translation (MT) for low-resource languages.\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 685.3272705078125, 533.8222045898438, 757.3825073242188)\n",
      "Text:\n",
      "As noted in Agarwal et al. (2024) and Reid et al. (2024), low-resource machine translation (MT)is one of the task types where many-shot in-context learning (ICL) has demonstrated remarkableperformance. In these tasks, there is often a nearly monotonic improvement in translation quality asmore source-target language pairs are incorporated into the context – as a notable exception to ourobservations in Sec. 2 that primarily involve reasoning tasks, in low resource MT, we often observe\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 29\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "29\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(93.635498046875, 85.18661499023438, 489.7715148925781, 123.84446716308594)\n",
      "Text:\n",
      "TasksRestricted IterativeRestricted bridgeReinf.(Ours)# Iterations121o1g2o2g3o\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(93.6355209350586, 124.65853881835938, 501.2132568359375, 315.1102294921875)\n",
      "Text:\n",
      "causal_judgement69.71.165.01.567.72.765.01.166.01.567.01.165.02.0date_understanding92.51.594.01.092.51.793.01.692.51.193.51.789.31.5disambiguation_qa74.00.775.83.070.52.772.31.575.53.271.82.476.53.8dyck_languages59.06.553.02.955.05.252.35.456.53.457.51.760.33.8formal_fallacies86.83.390.52.285.31.590.51.183.03.283.50.985.52.3geometric_shapes75.31.878.33.175.02.680.54.581.34.585.02.680.02.6hyperbaton85.34.084.53.494.01.695.80.891.81.393.02.697.00.7logical_deduction (7)67.51.869.02.569.52.771.53.266.82.170.52.370.82.2movie_recommendation64.82.763.32.268.33.362.04.163.01.263.31.361.82.2multistep_arithmetic_two95.01.295.50.597.80.895.30.895.51.895.81.395.81.5object_counting94.52.994.52.196.01.694.80.894.31.396.00.796.31.1ruin_names87.31.388.80.891.50.989.51.888.52.190.00.089.81.8salient_translation68.02.167.31.570.02.269.83.372.82.473.52.775.82.2snarks93.81.293.82.193.82.395.51.295.10.795.50.694.82.1sports_understanding94.01.495.01.092.81.195.51.597.00.096.00.794.80.8tracking_shuffled_objects (7)66.83.167.31.999.00.096.31.598.50.597.51.5100.00.0\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(93.6355209350586, 315.58892822265625, 496.4544372558594, 331.3260192871094)\n",
      "Text:\n",
      "Average79.6279.7082.4082.4582.3783.0883.32\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(62.03499984741211, 339.0382995605469, 534.7384033203125, 383.9944763183594)\n",
      "Text:\n",
      "Table 10 | Comparison of bridge and iterative reinforced ICL in the restricted setup wherethe methods may only use the subset of the train set that the model initially predicted correctly.Experiments performed on gemini-1.5-pro-001.\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(93.34941101074219, 395.5318298339844, 501.471435546875, 421.5589904785156)\n",
      "Text:\n",
      "TasksDiversityRetrievalReinf.bridgeDetails / hyperparams𝑘= 10𝑘= 25𝑘= 10𝑘= 25AllICL1o\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(93.34941101074219, 422.34356689453125, 497.5804748535156, 605.8896484375)\n",
      "Text:\n",
      "causal_judgement66.71.666.32.463.01.567.72.466.72.566.34.868.31.5date_understanding93.21.393.02.787.03.593.31.593.01.988.82.592.21.5disambiguation_qa72.23.077.80.876.50.971.20.877.51.176.82.471.82.4dyck_languages54.015.738.52.639.54.433.23.147.85.255.53.649.22.7formal_fallacies85.51.585.01.988.50.588.23.084.21.986.21.186.02.1geometric_shapes71.24.469.31.669.82.868.54.279.23.380.22.878.52.1hyperbaton95.01.292.22.596.51.197.21.395.21.990.21.196.50.9logical_deduction (7)65.83.067.54.469.24.466.32.967.32.465.83.570.21.5movie_recommendation67.32.665.02.568.53.468.01.467.33.365.21.667.01.2multistep_arithmetic_two92.81.396.20.495.50.994.81.694.31.996.50.596.20.8object_counting95.81.195.20.897.22.495.21.991.22.295.50.996.20.4ruin_names87.81.389.81.387.80.891.52.190.52.289.81.990.81.1salient_translation_error_detection68.52.369.52.168.23.358.22.861.02.169.01.668.80.8snarks94.82.396.21.294.41.797.61.295.51.292.73.293.43.0sports_understanding95.01.295.81.195.50.995.80.895.01.993.01.492.81.9tracking_shuffled_objects (7)55.84.556.85.560.24.367.89.760.22.462.34.295.80.4\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(93.34941101074219, 606.3506469726562, 495.7044372558594, 621.51708984375)\n",
      "Text:\n",
      "Average78.8378.3878.5978.4179.1279.6181.61\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(62.03499984741211, 629.17626953125, 533.8193359375, 674.1325073242188)\n",
      "Text:\n",
      "Table 11 | Comparison between bridge with one step of demonstration optimization only (i.e., 1o)against Retrieval, Diversity and Reinforced ICL baselines using gemini-1.5-pro-001. Note that thebridge (1o) and Reinforced ICL results are taken from Table 1.\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(61.04199981689453, 691.2232666015625, 534.425048828125, 749.728515625)\n",
      "Text:\n",
      "“more is better” given the information-dense nature of translation tasks – indeed, for translation tasks,barring glaring human errors in the annotation process, the provided data is generally assumed to beof high quality and problems like false positive in model-generated reasoning paths in reasoning tasksare generally negligible for tasks like low resource MT with high quality annotated data. However, in\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 30\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "30\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(141.00050354003906, 85.19329833984375, 447.3207702636719, 101.17353057861328)\n",
      "Text:\n",
      "Method1o1g2o2g3o\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(141.48818969726562, 102.02081298828125, 454.28143310546875, 166.49786376953125)\n",
      "Text:\n",
      "bridge-diversity (𝑘= 10)77.1079.4778.5881.8979.50bridge-diversity (𝑘= 25)78.1580.8678.7480.6379.68bridge-nearest (𝑘= 10)79.0781.8081.4081.3580.39bridge-nearest (𝑘= 25)78.3679.4980.1681.0980.10bridge-nearest (All)79.6582.9182.0183.2084.14\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(61.9370002746582, 174.6682586669922, 532.9278564453125, 219.62448120117188)\n",
      "Text:\n",
      "Table 12 | Average test accuracy on BBH tasks using gemini-1.5-pro-001 by combining bridgewith different variants of the heuristic demonstration selection methods. Bold text in this table showsthe best algorithm variant at each round of bridge.\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(117.61409759521484, 231.17604064941406, 465.8706970214844, 247.427001953125)\n",
      "Text:\n",
      "Task1o1g2o2g3o\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(117.61409759521484, 248.28858947753906, 477.20684814453125, 449.97735595703125)\n",
      "Text:\n",
      "causal_judgement73.01.162.31.564.70.765.72.263.32.7date_understanding94.31.392.01.695.01.492.22.392.80.4disambiguation_qa76.80.475.85.072.01.082.02.782.80.8dyck_languages58.82.375.04.375.03.378.53.082.01.2formal_fallacies84.20.888.51.790.50.989.51.890.00.7geometric_shapes75.82.586.23.379.80.884.02.184.51.1hyperbaton96.00.793.82.397.00.092.53.298.80.4logical_deduction (7)65.83.773.82.368.03.770.01.971.21.8movie_recommendation67.01.269.51.763.21.170.02.573.80.8multistep_arithmetic_two92.50.597.01.296.70.897.50.994.00.0object_counting91.81.595.01.297.00.796.51.7100.00.0ruin_names88.80.492.00.788.52.189.20.888.21.1salient_translation_error_detection63.21.570.01.670.20.470.01.270.50.5snarks95.81.794.81.293.71.296.51.295.80.0sports_understanding94.00.796.51.593.80.495.51.594.20.4tracking_shuffled_objects (7)56.81.664.50.967.01.061.54.464.21.6\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(117.61409759521484, 450.34771728515625, 472.9432067871094, 467.0020751953125)\n",
      "Text:\n",
      "Average79.6582.9182.0183.2084.14\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(62.03499984741211, 474.8022766113281, 534.6476440429688, 506.20947265625)\n",
      "Text:\n",
      "Table 13 | Task-specific test accuracy on BBH tasks using gemini-1.5-pro-001 with bridge-nearest (All) (best method from Table 12).\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(62.36199951171875, 523.30029296875, 534.7348022460938, 609.296875)\n",
      "Text:\n",
      "low-resource languages, the model’s inherent knowledge is often weak or non-existent due to the lackof exposure to target languages during pre-training or fine-tuning, which can lead to a bottleneck indata availaility, especially for extremely low-resource languages, where 1) the model lacks zero-shottranslation abilities due to insufficient exposure to target languages, and 2) the scarcity of annotateddata becomes a critical limiting factor – to address these, previous works often attempt to augmentground-truth translation data with model-synthesized translations (Han et al., 2021; Patel et al., 2022).\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(62.36199951171875, 611.3693237304688, 534.6473388671875, 656.3265380859375)\n",
      "Text:\n",
      "In this section, along this line of work, we investigate the applicability of bridge as a method toiteratively improve the model-synthesized translation so that they can act as more effective augmenta-tions to the scarce ground-truth data. Specifically, we assume the following in our setup:\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(77.74400329589844, 667.75830078125, 533.1195678710938, 753.3624877929688)\n",
      "Text:\n",
      "• Availability of some ground-truth source-target sentence pairs – this pair will both act as thetrain set from which ground-truth examples are generated and also as the validation set formachine-generated translations.• Abundant source language text – this is almost always true. For example, if we are interested intranslating from English to a low-resource language, it is extremely easy to obtain abundanttext in English whereas the difficulty is to obtain the corresponding translation in the target\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 31\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "31\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(93.55601501464844, 85.185791015625, 495.6220397949219, 123.42686462402344)\n",
      "Text:\n",
      "TasksReinf.Iter.bridge-boICLReinf.# Iterations1231o1g2o2g3o\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(93.55602264404297, 124.23309326171875, 495.2701416015625, 312.3033752441406)\n",
      "Text:\n",
      "causal_judgement364043114343939date_understanding6167725773447357disambiguation_qa4266692861606865dyck_languages154052945425920formal_fallacies606969263306757geometric_shapes4259684059197170hyperbaton707575475697559logical_deduction (7)4660621154516461movie_recommendation4253543949365141multistep_arithmetic_two6574743874287238object_counting6575756075487514ruin_names5870715170696921salient_translation_error_detection445960135875941snarks475051194954839sports_understanding6475755275747468tracking_shuffled_objects (7)58605327517522\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(93.55602264404297, 313.108642578125, 501.7219543457031, 328.67645263671875)\n",
      "Text:\n",
      "Average50.9462.0063.9427.2562.3836.6964.9444.50\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(62.03499984741211, 336.373291015625, 533.2288818359375, 367.7804870605469)\n",
      "Text:\n",
      "Table 14 | Number of examples for each experiment corresponding to Table 1 (gemini-1.5-pro-001on BBH tasks). Note that the “All” columns always use all 75 examples provided.\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(77.74400329589844, 382.73431396484375, 532.914306640625, 427.69146728515625)\n",
      "Text:\n",
      "language.• LLM for “pseudo-labeling” – we assume the availability of a (strong) LLM that can be queriedto generate synthesized data.\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(62.00199890136719, 443.3432922363281, 205.62838745117188, 461.20147705078125)\n",
      "Text:\n",
      "Algorithm 3 bridge for MT.\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(65.2760009765625, 459.1507568359375, 534.1597900390625, 616.2937622070312)\n",
      "Text:\n",
      "1: Input: train set D, unlabeled set with source language sentence, U, number of iteration rounds 𝐾∈ℕ(outer-loop),evaluation budget for BO per iteration 𝑛eval (inner-loop), Generator model used to synthesize examples M𝑔.2: Output: Optimized set of model-synthesized examples E∗.3: Partition D into two disjoint sets D𝑡and D𝑣via random sampling.4: [Generate] Generate the pool of initial examples E0 by predicting M𝑔on the unlabeled set, using the entire train setD as the demonstrations in the context: E0 ←M𝑔(U|D).5: for 𝑘∈{1, ..., 𝐾} (Outer loop) do6:[Optimize] Run Bayesian optimization (calling subroutine Algorithm 2 on the D𝑣to obtain e∗𝑘←BayesOpt(𝑛eval=𝑛eval, E=E𝑘).7:[Generate] Re-generate examples E𝑘by re-predicting the LLM on the unlabeled set, but with the optimizedexamples e∗𝑘from the previous step and D𝑡as demonstrations; the {inputs, output}-pairs are concatenated to formthe new set of examples E𝑘for the next [Optimize] step.8: end for9: return Optimized example set E∗after 𝐾rounds.\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(61.04199981689453, 628.4492797851562, 533.1829833984375, 741.1525268554688)\n",
      "Text:\n",
      "To approach the problem, we propose to retain the high-level framework of bridge but modifythe “optimize” and “generate” steps to accommodate the low-resource MT setup. With referenceto Algorithm 3 where we have marked the key differences in blue, the main difference lies in the“generate” step: instead of generating examples with model-generated reasoning paths in the casepresented in the main text, here we synthesized examples on the unlabeled set U that we assumedto be available. Since we no longer have access to the ground-truth translation of the sentences inU, we optimize for the optimal subset e∗by evaluating different combinations of the synthesizedexamples on the partition of the labeled dataset E𝑣.\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(79.29900360107422, 743.6182861328125, 532.9189453125, 761.4765014648438)\n",
      "Text:\n",
      "To test bridge on the MT setup, we consider the English-Bemba translation task in the Flores\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 32\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "32\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(91.78794860839844, 182.025146484375, 171.4849395751953, 201.91323852539062)\n",
      "Text:\n",
      "02040# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.32421875, 174.7398681640625, 88.90570068359375, 187.72219848632812)\n",
      "Text:\n",
      "0.55\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(76.89185333251953, 157.01364135742188, 88.91146850585938, 169.9959716796875)\n",
      "Text:\n",
      "0.60\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.36024475097656, 139.2873992919922, 88.90714263916016, 152.2697296142578)\n",
      "Text:\n",
      "0.65\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.05399322509766, 121.5611572265625, 88.91219329833984, 134.54348754882812)\n",
      "Text:\n",
      "0.70\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.52238464355469, 103.8349380493164, 88.90786743164062, 116.81726837158203)\n",
      "Text:\n",
      "0.75\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(66.83059692382812, 129.63525390625, 79.81292724609375, 151.7893829345703)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(105.98248291015625, 87.88082885742188, 162.9986572265625, 103.45962524414062)\n",
      "Text:\n",
      "causal_judgement\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(132.14657592773438, 152.03285217285156, 150.31283569335938, 162.8471221923828)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(132.14657592773438, 159.38783264160156, 149.040283203125, 170.2021026611328)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(132.14657592773438, 166.7427978515625, 149.74139404296875, 177.55706787109375)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(208.5295867919922, 182.025146484375, 278.8375549316406, 201.91323852539062)\n",
      "Text:\n",
      "0204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.49038696289062, 172.35650634765625, 204.8758544921875, 185.33883666992188)\n",
      "Text:\n",
      "0.75\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(192.89588928222656, 155.59852600097656, 204.8809051513672, 168.5808563232422)\n",
      "Text:\n",
      "0.80\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.36428833007812, 138.84051513671875, 204.8765869140625, 151.82284545898438)\n",
      "Text:\n",
      "0.85\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(192.85986328125, 122.08251953125, 204.8794708251953, 135.06484985351562)\n",
      "Text:\n",
      "0.90\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.32826232910156, 105.32453155517578, 204.87515258789062, 118.3068618774414)\n",
      "Text:\n",
      "0.95\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(182.79859924316406, 129.63525390625, 195.7809295654297, 151.7893829345703)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(218.92697143554688, 87.88082885742188, 281.98236083984375, 103.45962524414062)\n",
      "Text:\n",
      "date_understanding\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5675048828125, 152.03285217285156, 284.7337646484375, 162.8471221923828)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5675048828125, 159.38783264160156, 283.4612121582031, 170.2021026611328)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5675048828125, 166.7427978515625, 284.16229248046875, 177.55706787109375)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(321.4319152832031, 181.4058380126953, 399.1474609375, 201.97079467773438)\n",
      "Text:\n",
      "0204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.1560974121094, 159.99453735351562, 317.73992919921875, 173.41871643066406)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.32373046875, 136.42799377441406, 317.74066162109375, 149.8521728515625)\n",
      "Text:\n",
      "0.7\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.1933288574219, 112.86145782470703, 317.74139404296875, 126.28562927246094)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(298.75238037109375, 127.23289489746094, 312.1765441894531, 150.14102172851562)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(334.17926025390625, 84.05738830566406, 395.56085205078125, 100.16639709472656)\n",
      "Text:\n",
      "disambiguation_qa\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(381.5286865234375, 150.3927764892578, 400.313232421875, 161.57510375976562)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(381.5286865234375, 157.99806213378906, 398.99737548828125, 169.18038940429688)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(381.5286865234375, 165.60336303710938, 399.7223205566406, 176.7856903076172)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(437.1997985839844, 181.59523010253906, 525.2255249023438, 201.95318603515625)\n",
      "Text:\n",
      "0204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.3614807128906, 164.09326171875, 433.7290954589844, 177.38230895996094)\n",
      "Text:\n",
      "0.3\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.0234069824219, 149.51243591308594, 433.7332763671875, 162.80148315429688)\n",
      "Text:\n",
      "0.4\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.19549560546875, 134.9315948486328, 433.72833251953125, 148.22064208984375)\n",
      "Text:\n",
      "0.5\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.2323913574219, 120.35075378417969, 433.7298278808594, 133.63980102539062)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.3983459472656, 105.76991271972656, 433.7305603027344, 119.05896759033203)\n",
      "Text:\n",
      "0.7\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.2692565917969, 91.18907165527344, 433.7312927246094, 104.4781265258789)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(414.72442626953125, 127.96755981445312, 428.01348876953125, 150.6450958251953)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(454.6199951171875, 85.22663116455078, 506.1498718261719, 101.17349243164062)\n",
      "Text:\n",
      "dyck_languages\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(496.87652587890625, 150.89431762695312, 515.4719848632812, 161.96409606933594)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(496.87652587890625, 158.42306518554688, 514.1693725585938, 169.4928436279297)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(496.87652587890625, 165.95181274414062, 514.8870239257812, 177.02159118652344)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(92.46310424804688, 294.41314697265625, 168.67880249023438, 314.3012390136719)\n",
      "Text:\n",
      "0204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.52236938476562, 274.4662780761719, 88.90785217285156, 287.4486083984375)\n",
      "Text:\n",
      "0.75\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(76.9278793334961, 255.473876953125, 88.91290283203125, 268.4562072753906)\n",
      "Text:\n",
      "0.80\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.39627075195312, 236.4814910888672, 88.90857696533203, 249.4638214111328)\n",
      "Text:\n",
      "0.85\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(76.891845703125, 217.4890899658203, 88.91146087646484, 230.47142028808594)\n",
      "Text:\n",
      "0.90\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(66.8305892944336, 242.0232391357422, 79.81291961669922, 264.1773681640625)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(109.88272094726562, 200.26881408691406, 159.0956573486328, 215.8476104736328)\n",
      "Text:\n",
      "formal_fallacies\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(132.14657592773438, 264.42083740234375, 150.31283569335938, 275.2351379394531)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(132.14657592773438, 271.77581787109375, 149.040283203125, 282.5901184082031)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(132.14657592773438, 279.1307678222656, 149.74139404296875, 289.945068359375)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(211.35025024414062, 294.9922180175781, 292.7158508300781, 314.2474060058594)\n",
      "Text:\n",
      "0255075# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.6857452392578, 282.11590576171875, 207.78424072265625, 294.68511962890625)\n",
      "Text:\n",
      "0.875\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(192.55203247070312, 268.32489013671875, 207.78912353515625, 280.89410400390625)\n",
      "Text:\n",
      "0.900\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.66830444335938, 254.5338897705078, 207.78353881835938, 267.10308837890625)\n",
      "Text:\n",
      "0.925\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.00552368164062, 240.74288940429688, 207.78494262695312, 253.3120880126953)\n",
      "Text:\n",
      "0.950\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.65086364746094, 226.95187377929688, 207.78285217285156, 239.5210723876953)\n",
      "Text:\n",
      "0.975\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.5404052734375, 213.16087341308594, 207.7895965576172, 225.73007202148438)\n",
      "Text:\n",
      "1.000\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(182.8109588623047, 244.26950073242188, 195.38015747070312, 265.7186279296875)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(234.22262573242188, 203.8437957763672, 269.5994567871094, 218.9268341064453)\n",
      "Text:\n",
      "hyperbaton\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(249.647216796875, 265.954345703125, 267.2353820800781, 276.4244689941406)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(249.647216796875, 273.07525634765625, 266.0033264160156, 283.5453796386719)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(249.647216796875, 280.1961975097656, 266.68212890625, 290.66632080078125)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(319.817138671875, 295.2625732421875, 395.693359375, 314.2222900390625)\n",
      "Text:\n",
      "0204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.37530517578125, 284.8470764160156, 316.4869384765625, 297.223388671875)\n",
      "Text:\n",
      "0.4\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.5356140136719, 267.87286376953125, 316.4823913574219, 280.2491760253906)\n",
      "Text:\n",
      "0.5\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.5699462890625, 250.89865112304688, 316.4837341308594, 263.27496337890625)\n",
      "Text:\n",
      "0.6\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.7245178222656, 233.9244384765625, 316.48443603515625, 246.30075073242188)\n",
      "Text:\n",
      "0.7\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.60430908203125, 216.95022583007812, 316.4851379394531, 229.3265380859375)\n",
      "Text:\n",
      "0.8\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(298.7837219238281, 245.31822204589844, 311.1600341796875, 266.4382019042969)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(310.4270324707031, 205.51287841796875, 409.43536376953125, 220.3644561767578)\n",
      "Text:\n",
      "logical_deduction_seven_objects\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(375.2933349609375, 266.6702880859375, 392.6116027832031, 276.9797668457031)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(375.2933349609375, 273.68194580078125, 391.3984680175781, 283.9914245605469)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(375.2933349609375, 280.6935729980469, 392.06683349609375, 291.0030517578125)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(441.7033386230469, 294.4463195800781, 508.17279052734375, 314.29815673828125)\n",
      "Text:\n",
      "02040# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(424.7772521972656, 289.20745849609375, 436.98211669921875, 302.1661376953125)\n",
      "Text:\n",
      "0.40\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.2447814941406, 276.8436584472656, 436.9778137207031, 289.8023376464844)\n",
      "Text:\n",
      "0.45\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(424.945068359375, 264.4798889160156, 436.977294921875, 277.4385681152344)\n",
      "Text:\n",
      "0.50\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.4126281738281, 252.11610412597656, 436.9730224609375, 265.0747985839844)\n",
      "Text:\n",
      "0.55\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(424.9810485839844, 239.75230407714844, 436.978759765625, 252.71099853515625)\n",
      "Text:\n",
      "0.60\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.4485778808594, 227.38851928710938, 436.9744567871094, 240.3472137451172)\n",
      "Text:\n",
      "0.65\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.14288330078125, 215.0247344970703, 436.9794616699219, 227.98342895507812)\n",
      "Text:\n",
      "0.70\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(414.73431396484375, 242.15182495117188, 427.6929931640625, 264.2655944824219)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(444.2004089355469, 200.47344970703125, 520.7440795898438, 216.0238800048828)\n",
      "Text:\n",
      "movie_recommendation\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(498.5544128417969, 264.50860595703125, 516.6875610351562, 275.3031921386719)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(498.5544128417969, 271.8501892089844, 515.4173583984375, 282.644775390625)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(498.5544128417969, 279.1917724609375, 516.1171875, 289.9863586425781)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(107.99224853515625, 407.0483703613281, 160.84449768066406, 426.9897766113281)\n",
      "Text:\n",
      "204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.3031234741211, 398.4740295410156, 88.66709899902344, 411.49114990234375)\n",
      "Text:\n",
      "0.92\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(76.91777038574219, 379.43072509765625, 88.66902160644531, 392.4478454589844)\n",
      "Text:\n",
      "0.94\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.12248992919922, 360.387451171875, 88.6656494140625, 373.4045715332031)\n",
      "Text:\n",
      "0.96\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.15861511230469, 341.3441467285156, 88.66709899902344, 354.36126708984375)\n",
      "Text:\n",
      "0.98\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(77.64030456542969, 322.30084228515625, 88.66902160644531, 335.3179626464844)\n",
      "Text:\n",
      "1.00\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(66.82955169677734, 354.51806640625, 79.84667205810547, 376.7315673828125)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(94.24888610839844, 312.6517333984375, 174.4810791015625, 328.27227783203125)\n",
      "Text:\n",
      "multistep_arithmetic_two\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(150.52188110351562, 376.9757080078125, 168.73683166503906, 387.8189697265625)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(150.52188110351562, 384.3503723144531, 167.46087646484375, 395.1936340332031)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(150.52188110351562, 391.7250671386719, 168.1638641357422, 402.5683288574219)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(225.6697998046875, 407.0971374511719, 276.4425354003906, 426.9852294921875)\n",
      "Text:\n",
      "204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.36427307128906, 393.81097412109375, 204.8765869140625, 406.7933044433594)\n",
      "Text:\n",
      "0.85\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(192.85984802246094, 370.1373596191406, 204.87945556640625, 383.11968994140625)\n",
      "Text:\n",
      "0.90\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.3282470703125, 346.4637451171875, 204.87515258789062, 359.4460754394531)\n",
      "Text:\n",
      "0.95\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.88070678710938, 322.7901306152344, 204.87994384765625, 335.7724609375)\n",
      "Text:\n",
      "1.00\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(182.798583984375, 354.7072448730469, 195.78091430664062, 376.8613586425781)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(224.90792846679688, 312.95281982421875, 276.0094299316406, 328.5316162109375)\n",
      "Text:\n",
      "object_counting\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5674743652344, 377.1048278808594, 284.7337341308594, 387.91912841796875)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5674743652344, 384.4598083496094, 283.461181640625, 395.27410888671875)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5674743652344, 391.8147888183594, 284.1622619628906, 402.62908935546875)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(324.449462890625, 407.0971374511719, 397.5828857421875, 426.9852294921875)\n",
      "Text:\n",
      "0204060# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.8628845214844, 398.54571533203125, 320.847900390625, 411.5280456542969)\n",
      "Text:\n",
      "0.80\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.5474548339844, 384.3014221191406, 320.8464660644531, 397.28375244140625)\n",
      "Text:\n",
      "0.82\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.1631164550781, 370.05712890625, 320.8483581542969, 383.0394592285156)\n",
      "Text:\n",
      "0.84\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.3673095703125, 355.81280517578125, 320.84503173828125, 368.7951354980469)\n",
      "Text:\n",
      "0.86\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.4033203125, 341.5685119628906, 320.846435546875, 354.55084228515625)\n",
      "Text:\n",
      "0.88\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.82684326171875, 327.32421875, 320.846435546875, 340.3065490722656)\n",
      "Text:\n",
      "0.90\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(298.7655944824219, 354.7072448730469, 311.7479248046875, 376.8613586425781)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(348.438232421875, 312.95281982421875, 384.4244079589844, 328.5316162109375)\n",
      "Text:\n",
      "ruin_names\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(382.53448486328125, 377.1048278808594, 400.70074462890625, 387.91912841796875)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(382.53448486328125, 384.4598083496094, 399.4281921386719, 395.27410888671875)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(382.53448486328125, 391.8147888183594, 400.1292724609375, 402.62908935546875)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(439.92303466796875, 407.0971374511719, 510.3364562988281, 426.9852294921875)\n",
      "Text:\n",
      "02040# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(424.8309020996094, 386.72821044921875, 436.81591796875, 399.7105407714844)\n",
      "Text:\n",
      "0.80\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.2992858886719, 364.3594055175781, 436.81158447265625, 377.34173583984375)\n",
      "Text:\n",
      "0.85\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(424.79486083984375, 341.9905700683594, 436.814453125, 354.972900390625)\n",
      "Text:\n",
      "0.90\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(425.26324462890625, 319.62176513671875, 436.81011962890625, 332.6040954589844)\n",
      "Text:\n",
      "0.95\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(414.7336120605469, 354.7072448730469, 427.7159423828125, 376.8613586425781)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(472.3028259277344, 312.95281982421875, 492.48883056640625, 328.5316162109375)\n",
      "Text:\n",
      "snarks\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(480.0495910644531, 377.1048278808594, 498.2158508300781, 387.91912841796875)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(480.0495910644531, 384.4598083496094, 496.94329833984375, 395.27410888671875)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(480.0495910644531, 391.8147888183594, 497.6443786621094, 402.62908935546875)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(208.55885314941406, 519.879150390625, 292.5987854003906, 539.7672729492188)\n",
      "Text:\n",
      "0255075# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.43634033203125, 511.32769775390625, 204.8794708251953, 524.31005859375)\n",
      "Text:\n",
      "0.88\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(192.85986328125, 498.6661376953125, 204.8794708251953, 511.6484680175781)\n",
      "Text:\n",
      "0.90\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.54443359375, 486.0045166015625, 204.87803649902344, 498.9868469238281)\n",
      "Text:\n",
      "0.92\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.1601104736328, 473.3429260253906, 204.8799591064453, 486.32525634765625)\n",
      "Text:\n",
      "0.94\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.36428833007812, 460.68133544921875, 204.87660217285156, 473.6636657714844)\n",
      "Text:\n",
      "0.96\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.4003143310547, 448.01971435546875, 204.87803649902344, 461.0020446777344)\n",
      "Text:\n",
      "0.98\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(193.88072204589844, 435.358154296875, 204.8799591064453, 448.3404846191406)\n",
      "Text:\n",
      "1.00\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(182.79859924316406, 467.4892578125, 195.7809295654297, 489.64337158203125)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(216.08059692382812, 425.7348327636719, 284.8293151855469, 441.3136291503906)\n",
      "Text:\n",
      "sports_understanding\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5675048828125, 489.8868408203125, 284.7337646484375, 500.7011413574219)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5675048828125, 497.2418212890625, 283.4612121582031, 508.0561218261719)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(266.5675048828125, 504.5967712402344, 284.16229248046875, 515.4110717773438)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(324.2207946777344, 519.8038940429688, 404.53851318359375, 539.7742309570312)\n",
      "Text:\n",
      "0255075# Demos\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.2528076171875, 500.1667785644531, 320.4712219238281, 513.2028198242188)\n",
      "Text:\n",
      "0.72\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(308.86688232421875, 480.9868469238281, 320.47314453125, 494.02288818359375)\n",
      "Text:\n",
      "0.74\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.0718994140625, 461.8069152832031, 320.4697570800781, 474.84295654296875)\n",
      "Text:\n",
      "0.76\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(309.10809326171875, 442.626953125, 320.4712219238281, 455.6629943847656)\n",
      "Text:\n",
      "0.78\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(298.7640075683594, 467.1972351074219, 311.800048828125, 489.4429931640625)\n",
      "Text:\n",
      "Val. Acc.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(349.34765625, 425.2700500488281, 383.1348571777344, 440.9132995605469)\n",
      "Text:\n",
      "GSM-Hard\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(382.4151611328125, 489.6874694824219, 400.6565856933594, 500.5464782714844)\n",
      "Text:\n",
      "Round 0\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(382.4151611328125, 497.0728759765625, 399.3787536621094, 507.931884765625)\n",
      "Text:\n",
      "Round 1\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(382.4151611328125, 504.4582824707031, 400.082763671875, 515.3173217773438)\n",
      "Text:\n",
      "Round 2\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(61.45703125, 544.290283203125, 533.1828002929688, 602.7965087890625)\n",
      "Text:\n",
      "Figure 5 | Additional visualization of the task performance at different rounds. Note that in mostdatasets, additional rounds of bridge led to performance improvement, and some of the exceptions(e.g., multi_arithmetric_two) are possibly caused by visualization artifacts of the extremelysmall performance variation as shown by the small y-axis ranges.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(62.362030029296875, 619.8872680664062, 533.0947875976562, 732.5894775390625)\n",
      "Text:\n",
      "dataset (Guzmán et al., 2019) that was also considered in Agarwal et al. (2024). We assume theaccess to 100 labeled examples as D and 50 unlabeled examples U, and hold out another 400samples as the test set. We use Gemini Flash as the target model and Gemini Pro as the generatormodel in Algorithm 3, and we show the result in Table 15. Overall, we observe that running iterativeoptimization also improves performance on this task, both exemplified by improvement on the testand validation chrf score, although it seems that an additional optimization round, in this case, led toa small performance degradation. While a more comprehensive evaluation is required, we believe thepreliminary result is promising for future efforts in this direction.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(62.362030029296875, 782.9923095703125, 471.65814208984375, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 33\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "33\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(115.33599853515625, 52.45830535888672, 479.9363708496094, 65.50536346435547)\n",
      "Text:\n",
      "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(163.3572540283203, 85.17039489746094, 426.4426574707031, 109.17115783691406)\n",
      "Text:\n",
      "TasksGold-onlyAllbridge-mt# Iterations-01o1g2o2g3o\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(163.3572540283203, 109.89543151855469, 431.9210510253906, 123.54231262207031)\n",
      "Text:\n",
      "en_bem37.7838.4638.3339.1139.3038.9039.29\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(60.99800109863281, 131.42625427246094, 533.187255859375, 189.93246459960938)\n",
      "Text:\n",
      "Table 15 | Test chrf score of gemini-1.5-flash-001. “Gold-only” refers to the result obtained byonly using the 100 labeled examples in the context; “All” refers to the result with 100 labeled examples+ 50 initially generated examples from gemini-1.5-pro-001. Refers to captions of Table 1 foradditional explanations.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(62.36199951171875, 211.80654907226562, 413.438720703125, 222.7156524658203)\n",
      "Text:\n",
      "C.5. Transferring learned demonstrations from GSM-Hard to GSM-8K\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(61.9370002746582, 229.3392791748047, 533.0958862304688, 369.1404724121094)\n",
      "Text:\n",
      "In this section, we investigate whether the bridge-discovered demonstrations can transfer acrossrelated but distinct datasets. Specifically, we investigate the extent to which the demonstrations foundon GSM-Hard (Table 2) generalize to the original GSM-8K and we show the result in Table 16, wherewe compare the performance of the demonstrations directly transferred from GSM-Hard at differentstages of bridge against directly optimizing on GSM-8K. We find that whereas the demonstrationsgenerated from (iterative) reinforced ICL led to a small deterioration of GSM-8K performance, wefound the transferred demonstrations from bridge led to a small improvement even though theGemini 1.5 Pro performance on GSM-8K has been rather saturated. While optimizing directly onGSM-8K unsurprisingly led to the highest performance given that there is no distribution shift, wealso find that the GSM-Hard demonstrations exhibit considerable generalizability.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(117.26028442382812, 379.0133972167969, 471.77325439453125, 418.1796569824219)\n",
      "Text:\n",
      "Tasks0-shotReinf.IterativebridgeICLReinf.(Ours)# Iterations-0121o1g2o2g3o\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(117.26029205322266, 419.0045166015625, 478.01873779296875, 446.36614990234375)\n",
      "Text:\n",
      "Direct91.9293.8193.0692.6893.8193.1894.7094.1993.94Transferred-90.6691.7991.1693.8192.5593.8193.1891.16\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(62.03499984741211, 454.4842834472656, 533.1878662109375, 499.4404602050781)\n",
      "Text:\n",
      "Table 16 | Comparison of the transferred bridge-generated demonstrations on GSM-Hard vs. directlyrunning bridge on GSM-8K. Runs with performance deteriorations w.r.t. the 0-shot results aremarked in red in the table.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(62.36199951171875, 528.0987548828125, 253.472900390625, 541.0501708984375)\n",
      "Text:\n",
      "D. Computational Cost Analysis\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(62.36199951171875, 550.2023315429688, 534.7368774414062, 581.6094970703125)\n",
      "Text:\n",
      "In this section, we provide a computational cost analysis of bridge. In general, since bridgeconsists of multiple rounds of “Optimize” and “Generate” steps, here we analyze each step in detail.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(77.74400329589844, 593.0422973632812, 534.7383422851562, 653.5964965820312)\n",
      "Text:\n",
      "• Optimize: The cost of the “optimize” step depends on the budget allocated (𝑛eval in Line 5 ofAlgorithm 2), which is user-configurable. If we opt for iterative optimization (such as usingBayesian optimization in the main section of the paper, or random search in App. C.1), each“optimize” step thus entails 𝑛eval LLM inferences on the validation set. As shown in the App.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(77.74400329589844, 647.23828125, 534.4342651367188, 732.842529296875)\n",
      "Text:\n",
      "C.1, it is also possible to use a non-iterative method based on retrieval or embedding diversity,in which case each “optimize” step entails a single round of LLM inferences on the validationset (or the train set, if we use the dataset for both training and validation).• Generate: The “generate” step always involves a single round of LLM inferences on the train setwhere we simply use the optimized examples from the “optimize” step above as demonstrationsand run inference again on the train set.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(62.36199951171875, 782.9923095703125, 471.6581115722656, 806.0023803710938)\n",
      "Text:\n",
      "Expanded version of the paper published in the 13th International Conference on Learning Representations (ICLR 2025)Reviewed version: https://openreview.net/pdf?id=JBXO05r4AV.\n",
      "\n",
      "Page: 34\n",
      "Position: Rect(524.051025390625, 784.0443115234375, 532.913818359375, 797.0913696289062)\n",
      "Text:\n",
      "34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "pdf_file = \"../data/2502.00330v1.pdf\"  # Replace with your PDF file path\n",
    "paragraphs = extract_text_by_paragraph(pdf_file)\n",
    "\n",
    "if paragraphs:\n",
    "    for paragraph in paragraphs:\n",
    "        print(f\"Page: {paragraph['page']}\")\n",
    "        print(f\"Position: {paragraph['pos']}\")\n",
    "        print(f\"Text:\\n{paragraph['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(\"../data/2502.00330v1.pdf\")\n",
    "print(result.text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试minerU API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mineru_api_key = \"eyJ0eXBlIjoiSldUIiwiYWxnIjoiSFM1MTIifQ.eyJqdGkiOiI4NTUwNzEwNiIsInJvbCI6IlJPTEVfUkVHSVNURVIiLCJpc3MiOiJPcGVuWExhYiIsImlhdCI6MTczODgwNTU1NSwiY2xpZW50SWQiOiJsa3pkeDU3bnZ5MjJqa3BxOXgydyIsInBob25lIjoiIiwidXVpZCI6IjFjOWE0NjE5LWMxNWItNDkxNi04MjQ4LWY4YjQ1MjJiZTZiYyIsImVtYWlsIjoiIiwiZXhwIjoxNzQwMDE1MTU1fQ.SCAEEIbeeTXheBOqa78koRcgS0uw0IXRFt9kLq3eA0zBfS0Qeml7vy-VXlg1Hh9dwm9WnLc-GDKZXwys1tGJKg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response success. result:{'code': 0, 'msg': 'ok', 'trace_id': '3f2b3984a81dea09cca81b3dc8082cdd', 'data': {'batch_id': 'd41fed01-60c4-4ab3-ac5b-e79651e03068', 'file_urls': ['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/d41fed01-60c4-4ab3-ac5b-e79651e03068/0d5d110a-5a83-452e-a622-4885eb5acf32.pdf?Expires=1738893739&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=4%2BGSeSjIPMA3e7B2zqhxctNWxI8%3D']}}\n",
      "batch_id:d41fed01-60c4-4ab3-ac5b-e79651e03068,urls:['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/d41fed01-60c4-4ab3-ac5b-e79651e03068/0d5d110a-5a83-452e-a622-4885eb5acf32.pdf?Expires=1738893739&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=4%2BGSeSjIPMA3e7B2zqhxctNWxI8%3D']\n",
      "upload success\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url='https://mineru.net/api/v4/file-urls/batch'\n",
    "header = {\n",
    "    'Content-Type':'application/json',\n",
    "    \"Authorization\":f\"Bearer {mineru_api_key}\"\n",
    "}\n",
    "data = {\n",
    "    \"enable_formula\": True,\n",
    "    \"language\": \"en\",\n",
    "    \"layout_model\":\"doclayout_yolo\",\n",
    "    \"enable_table\": True,\n",
    "    \"files\": [\n",
    "        {\"name\":\"2502.00330v1.pdf\", \"is_ocr\": False, \"data_id\": \"test-20250206-001\"}\n",
    "    ]\n",
    "}\n",
    "file_path = r\"../data/2502.00330v1.pdf\"\n",
    "try:\n",
    "    response = requests.post(url,headers=header,json=data)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print('response success. result:{}'.format(result))\n",
    "        if result[\"code\"] == 0:\n",
    "            batch_id = result[\"data\"][\"batch_id\"]\n",
    "            urls = result[\"data\"][\"file_urls\"]\n",
    "            print('batch_id:{},urls:{}'.format(batch_id, urls))\n",
    "            with open(file_path, 'rb') as f:\n",
    "                res_upload = requests.put(urls[0], data=f)\n",
    "            if res_upload.status_code == 200:\n",
    "                print(\"upload success\")\n",
    "            else:\n",
    "                print(\"upload failed\")\n",
    "        else:\n",
    "            print('apply upload url failed,reason:{}'.format(result.msg))\n",
    "    else:\n",
    "        print('response not success. status:{} ,result:{}'.format(response.status_code, response))\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 0,\n",
       " 'msg': 'ok',\n",
       " 'trace_id': '3f2b3984a81dea09cca81b3dc8082cdd',\n",
       " 'data': {'batch_id': 'd41fed01-60c4-4ab3-ac5b-e79651e03068',\n",
       "  'file_urls': ['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/d41fed01-60c4-4ab3-ac5b-e79651e03068/0d5d110a-5a83-452e-a622-4885eb5acf32.pdf?Expires=1738893739&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=4%2BGSeSjIPMA3e7B2zqhxctNWxI8%3D']}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'code': 0, 'msg': 'ok', 'trace_id': 'aebd83cac4431ab7cfa4efe3b4b2ec73', 'data': {'batch_id': 'd41fed01-60c4-4ab3-ac5b-e79651e03068', 'extract_result': [{'data_id': 'test-20250206-001', 'file_name': '2502.00330v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/039a6a8b-5f27-4f88-a2df-3988a66e6af9.zip'}]}}\n",
      "{'batch_id': 'd41fed01-60c4-4ab3-ac5b-e79651e03068', 'extract_result': [{'data_id': 'test-20250206-001', 'file_name': '2502.00330v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/039a6a8b-5f27-4f88-a2df-3988a66e6af9.zip'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = f'https://mineru.net/api/v4/extract-results/batch/{batch_id}'\n",
    "header = {\n",
    "    'Content-Type':'application/json',\n",
    "    \"Authorization\":f\"Bearer {mineru_api_key}\"\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=header)\n",
    "print(res.status_code)\n",
    "print(res.json())\n",
    "print(res.json()[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先按page切，暂不考虑acknowledgement, reference及以后的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_section_list = ['References', \"Acknowledgments\", \"Appendix\", \"FAQ\", \"Frequently Asked Questions\"]\n",
    "\n",
    "import re\n",
    "sec_ptrn = '|'.join(re.escape(section) for section in append_section_list)\n",
    "\n",
    "mtch_rslts = []\n",
    "page = len(doc)\n",
    "for item in toc:\n",
    "    if re.match(sec_ptrn, item.title):\n",
    "        if item.pagenum < page:\n",
    "            page = item.pagenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def save_pdf_pages(input_pdf_path, output_pdf_path, page_numbers):\n",
    "    # 打开PDF文件\n",
    "    pdf_document = fitz.open(input_pdf_path)\n",
    "    \n",
    "    # 创建一个新的PDF文档\n",
    "    output_pdf = fitz.open()\n",
    "    \n",
    "    # 添加指定的页面到新的PDF文档\n",
    "    for page_number in page_numbers:\n",
    "        # 将页面添加到新的PDF文档中\n",
    "        output_pdf.insert_pdf(pdf_document, from_page=page_number, to_page=page_number)\n",
    "    \n",
    "    # 保存新的PDF文件\n",
    "    output_pdf.save(output_pdf_path)\n",
    "    output_pdf.close()\n",
    "    pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存前n页\n",
    "in_pdf_path = \"/Users/jiezi/Documents/Local Code/Project/PaperPal/dev/tmp/2201.11903v6.pdf\"\n",
    "out_pdf_path = 'tmp.pdf'  # 输出PDF文件路径\n",
    "# save_pdf_pages(in_pdf_path, out_pdf_path, list(range(0, page)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pdf_layout_det import PDF2MARKDOWN\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/jiezi/Packages/PDF-Extract-Kit\")\n",
    "\n",
    "from pdf_extract_kit.utils.config_loader import load_config, initialize_tasks_and_models\n",
    "\n",
    "\n",
    "TASK_NAME = 'pdf2markdown'\n",
    "config_path = \"/home/jiezi/Packages/PDF-Extract-Kit/project/pdf2markdown/configs/pdf2markdown.yaml\"\n",
    "config = load_config(config_path)\n",
    "task_instances = initialize_tasks_and_models(config)\n",
    "\n",
    "# get input and output path from config\n",
    "input_data = out_pdf_path\n",
    "result_path = \"./opt\"\n",
    "\n",
    "layout_model = task_instances['layout_detection'].model if 'layout_detection' in task_instances else None\n",
    "mfd_model = task_instances['formula_detection'].model if 'formula_detection' in task_instances else None\n",
    "mfr_model = None\n",
    "# mfr_model = task_instances['formula_recognition'].model if 'formula_recognition' in task_instances else None\n",
    "ocr_model = None\n",
    "# ocr_model = task_instances['ocr'].model if 'ocr' in task_instances else None\n",
    "\n",
    "pdf2md = PDF2MARKDOWN(layout_model, mfd_model, mfr_model, ocr_model)\n",
    "res_list, final_blocks, md_content = pdf2md.process(input_path=input_data, save_dir=result_path, visualize=True, merge2markdown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = \"\".join(md_content).split(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paras = [item for item in paras if item is not None and item != '' and len(item) >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_dct = []\n",
    "for idx, item in enumerate(filtered_paras):\n",
    "    paras_dct.append({'id':idx, 'lines':item[0:300]+\"...\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(paras_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_paras_dct = []\n",
    "for idx, item in enumerate(filtered_paras):\n",
    "    tmp_paras_dct.append({'para_id':idx, 'content':item[0:50]+\"...\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(tmp_paras_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_prompt = \"\"\"## INSTRUCTION\n",
    "已知table_of_content记录了章节标题和对应的页面，para中抽取了各个章节的起始句子。\n",
    "对于table_of_content中的每一项，根据section_title和paras中content的内容进行匹配，并将全部匹配到的para_id添加到table_of_content中。\n",
    "注意以下两种情况均构成匹配：\n",
    "- content直接对应section_title；\n",
    "- content是section_title的二级目录下的内容。\n",
    "如无匹配的项，则将置空。\n",
    "\n",
    "## INPUT\n",
    "<toc>\n",
    "{toc}\n",
    "</toc>\n",
    "\n",
    "<paras>\n",
    "{paras}\n",
    "</paras>\n",
    "\n",
    "## OUTPUT\n",
    "Output in json with double quotes in the following format:\n",
    "```json\n",
    "[{{'section_title':xxx, 'page_num':xxx, 'vpos':xxx, 'para_ids':[list of all matched para_id, blank if no match]}}\n",
    ", ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "toc_lst = []\n",
    "for item in toc:\n",
    "    toc_lst.append({'section_title':item.title, 'page_num':item.pagenum, 'vpos':item.vpos})\n",
    "prompt = match_prompt.format(toc=str(toc_lst),paras=str(tmp_paras_dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "def zhipu_llm(sys_prompt, qa_promt):\n",
    "    if not sys_prompt:\n",
    "        sys_prompt = \"You are a helpful assistant.\"\n",
    "    \n",
    "    \n",
    "    client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY_1\")) # 填写您自己的APIKey\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-flash\",  # 填写需要调用的模型编码\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": qa_promt}\n",
    "        ],\n",
    "    )\n",
    "    opt_result = response.choices[0].message.content\n",
    "    return opt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def convert_quotes(json_str):\n",
    "    # 将单引号替换为双引号，但是需要排除字符串内的单引号\n",
    "    json_str = re.sub(r\"(?<!\\\\)'(.*?)(?<!\\\\)'\", r'\"\\1\"', json_str)\n",
    "    return json_str\n",
    "\n",
    "def get_json(json_str):\n",
    "    # 正则表达式，匹配以 ```json 开头，后面可能跟着换行符，然后是JSON内容，直到 ``` 结尾\n",
    "    pattern = r\"```json\\n?(.*?)\\n?```\"\n",
    "\n",
    "    # 使用正则表达式找到匹配的JSON字符串\n",
    "    matches = re.findall(pattern, json_str, re.DOTALL)\n",
    "\n",
    "    json_data = None\n",
    "    # 如果找到匹配项，尝试将其转换为JSON对象\n",
    "    if matches:\n",
    "        json_str = matches[0].strip()  # 移除字符串前后的空白字符\n",
    "        json_str = convert_quotes(json_str)  # 转换单引号为双引号\n",
    "        try:\n",
    "            json_data = json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No JSON content found.\")\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_rslt = zhipu_llm(sys_prompt=None, qa_promt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_json = get_json(outline_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download source data or use html to double confirm pdf data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"1605.08386v1\"])))\n",
    "# Download the archive to the PWD with a default filename.\n",
    "paper.download_source()\n",
    "# Download the archive to the PWD with a custom filename.\n",
    "paper.download_source(filename=\"downloaded-paper.tar.gz\")\n",
    "# Download the archive to a specified directory with a custom filename.\n",
    "paper.download_source(dirpath=\"./mydir\", filename=\"downloaded-paper.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use html for information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "arxiv_id = \"2410.24175\"\n",
    "url = f\"https://arxiv.org/html/{arxiv_id}\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "figures = []\n",
    "tables = []\n",
    "\n",
    "figure_images = soup.select('.ltx_figure > img')\n",
    "figure_captions = soup.select('.ltx_figure > figcaption') \n",
    "for figure_image, figure_caption in zip(figure_images, figure_captions):\n",
    "    figure = {\n",
    "        'figure_path': f\"https://arxiv.org/html/{arxiv_id}/{figure_image.get('src')}\",\n",
    "        'figure_caption': figure_caption.text.strip()\n",
    "    }\n",
    "    figures.append(figure)\n",
    "\n",
    "\n",
    "table_contents = soup.select('table.ltx_tabular')\n",
    "table_captions = soup.select('.ltx_table > figcaption')\n",
    "for table_content, table_caption in zip(table_contents, table_captions):\n",
    "    table = {\n",
    "        'table_content': str(table_content),\n",
    "        'table_caption': table_caption.text.strip()\n",
    "    }\n",
    "    tables.append(table)\n",
    "\n",
    "with open('figures.json', 'w') as f:\n",
    "    json.dump(figures, f)\n",
    "\n",
    "with open('tables.json', 'w') as f:\n",
    "    json.dump(tables, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案一：直接使用LLM针对特定章节问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要补充：\n",
    "- 长度控制模块\n",
    "- 段落切分，按段落问答并总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_identify_prompt = \"\"\"## TASK\n",
    "You are an academic researcher in Computer Science and AI field. \n",
    "You are given section title together with initial lines of paragraphs from a paper.\n",
    "Now you are asked to identify the section type. The section type can be one of the following: \n",
    "['Bio', 'Abstraction', 'Introduction',  'Related Works and Literature Review', 'Methodology', 'Experiment and Results', 'Discussion and Conclusion', 'Others']\n",
    "Please identify the section type based on the given section.\n",
    "\n",
    "## PARA\n",
    "{content}\n",
    "\n",
    "## OUTPUT\n",
    "Output in json with double quotes in the following format:\n",
    "```json\n",
    "[{{'id':0, 'sectoin_type':xxx}}, {{'id':1, 'sectoin_type':xxx}}, ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "prompt = section_identify_prompt.format(content=str(paras_dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY_1\")) # 填写您自己的APIKey\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4-flash\",  # 填写需要调用的模型编码\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant whose task is to provide users with professional, accurate, and insightful advice.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    ")\n",
    "opt_result = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_json = get_json(opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract, introduction, method, conclusion = \"\", \"\", \"\", \"\"\n",
    "for idx, item in enumerate(opt_json):\n",
    "    if item['section_type'] == 'Abstraction':\n",
    "        abstract += filtered_paras[idx]\n",
    "    if item['section_type'] == 'Introduction':\n",
    "        introduction += '\\n\\n\\n' + filtered_paras[idx]   \n",
    "    if item['section_type'] == 'Methodology':\n",
    "        method += '\\n\\n\\n' + filtered_paras[idx]\n",
    "    if item['section_type'] == 'Discussion and Conclusion':\n",
    "        conclusion += '\\n\\n\\n' + filtered_paras[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cur_api += 1\n",
    "self.cur_api = 0 if self.cur_api >= len(self.chat_api_list) - 1 else self.cur_api\n",
    "text_token = len(self.encoding.encode(text))\n",
    "clip_text_index = int(len(text) * (self.max_token_num - method_prompt_token) / text_token)\n",
    "clip_text = text[:clip_text_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"You are a researcher in the field of '{subject}' who is good at summarizing papers using concise statements.\"\n",
    "\n",
    "summary_prompt = \"\"\" ## INSTRUCTION\n",
    "Given abstraction and introduction paragraph from the paper, you are asked to:                   \n",
    "1. identify the keywords of this article;\n",
    "2. summarize according to the following four points\n",
    "- (1): What is the research background of this article? What problem is this paper trying to solve? \n",
    "- (2): What are the relevant studies? What are the past methods? What are the issues with them? Is the approach well motivated?\n",
    "- (3): How does the paper solve this problem? What is the research methodology proposed in this paper?\n",
    "- (4): What experiments were done in the paper? On what task and what performance is achieved by the methods in this paper? Can the performance support their goals?\n",
    "- (5): Are there unsolved issues with the paper? What gaps can be explored further? Any suggestions?\n",
    "\n",
    "## CONTEXT\n",
    "Here are abstraction from the paper:\n",
    "<abstraction>\n",
    "{abstraction}\n",
    "</abstraction>\n",
    "\n",
    "Here are introduction from the paper:\n",
    "<introduction>\n",
    "{introduction}\n",
    "</introduction>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output that follows: \n",
    "```text                            \n",
    "1. Keywords: xxx\\n\\n     \n",
    "2. Summary: \\n\\n\n",
    "- (1):xxx;\\n \n",
    "- (2):xxx;\\n \n",
    "- (3):xxx;\\n  \n",
    "- (4):xxx.\\n\\n     \n",
    "- (5):xxx.\\n\\n  \n",
    "```\n",
    "\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not have too much repetitive information, numerical values using the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed.                 \n",
    "\"\"\"\n",
    "\n",
    "method_prompt = \"\"\"## INSTRUCTION\n",
    "Given method paragraph and a summary of a paper, you are asked to describe in detail the methodological idea of this article. \n",
    "- (1):...\n",
    "- (2):...\n",
    "- (3):...\n",
    "- .......\n",
    "\n",
    "## CONTEXT\n",
    "Here are method paragraph:\n",
    "<method>\n",
    "{method}\n",
    "</method>\n",
    "\n",
    "Here are summary of the paper fyi:\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output that follows: \n",
    "```text\n",
    "3. Methods: \\n\\n\n",
    "- (1):xxx;\\n \n",
    "- (2):xxx;\\n \n",
    "- (3):xxx;\\n  \n",
    "....... \\n\\n     \n",
    "```\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not repeat the content of the previous <summary>, the value of the use of the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed, ....... means fill in according to the actual requirements.                 \n",
    "\"\"\"\n",
    " \n",
    "conclusion_prompt = \"\"\"## INSTRUCTION\n",
    "Given conclusion paragraph and a summary of a paper, you are asked to: \n",
    "4. Make the following summary:\n",
    "- (1):What is the significance of this piece of work?\n",
    "- (2):Summarize the strengths and weaknesses of this article in three dimensions: innovation point, performance, and workload.                   \n",
    ".......\n",
    "\n",
    "    \"contribution\": \"What is the contribution of this paper?\",\n",
    "    \"novelty\": \"What is the novelty of this paper?\",\n",
    "    \"strength\": \"What are the strengths of this paper?\",\n",
    "    \"drawback\": \"What are the drawbacks of this paper?\",\n",
    "    \"improvement\": \"What might be the improvements of this paper?\",\n",
    "\n",
    "\n",
    "## CONTEXT\n",
    "Here are conclusion paragraph:\n",
    "<conclusion>\n",
    "{conclusion}\n",
    "</conclusion>\n",
    "\n",
    "Here are summary of the paper fyi:\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output later: \n",
    "```text\n",
    "4. Conclusion: \\n\\n\n",
    "- (1):xxx;\\n                     \n",
    "- (2):Innovation point: xxx; Performance: xxx; Workload: xxx;\\n    \n",
    "- (3):\n",
    "    contribution: What is the contribution of this paper?,\n",
    "    novelty: What is the novelty of this paper?,\n",
    "    strength\": What are the strengths of this paper?,\n",
    "    drawback: What are the drawbacks of this paper?,\n",
    "    improvement\": What might be the improvements of this paper?\n",
    "```\n",
    "\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not repeat the content of the previous <summary>, the value of the use of the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed, ....... means fill in according to the actual requirements.                 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"English\"\n",
    "sum_prompt = summary_prompt.format(abstraction=abs, introduction=intro, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result = zhipu_llm(sys_prompt, sum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"English\"\n",
    "dis_prompt = method_prompt.format(conclusion=dis, summary=opt_result, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_2 = zhipu_llm(sys_prompt, dis_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"English\"\n",
    "met_prompt = method_prompt.format(method=method, summary=opt_result, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(met_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_3 = zhipu_llm(sys_prompt, met_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二：使用传统RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to-do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案三：使用GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定位图片、表格或公式的详细位置\n",
    "- 'figure', 'figure_caption',\n",
    "- 'table', 'table_caption', 'table_footnote',\n",
    "- 'formula', 'formula_caption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取图片、表格或公式的详细位置\n",
    "def get_bounding_box(poly):\n",
    "    x_coords = poly[0::2]\n",
    "    y_coords = poly[1::2]\n",
    "    return min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "\n",
    "def do_boxes_overlap(box1, box2, max_distance=20):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    horizontal_overlap = not (x1_max < x2_min or x1_min > x2_max)\n",
    "    vertical_overlap_or_close = not (y1_max < y2_min - max_distance or y1_min > y2_max + max_distance)\n",
    "\n",
    "    return horizontal_overlap or vertical_overlap_or_close\n",
    "\n",
    "def consolidate_positions(items):\n",
    "    if not items:\n",
    "        return None\n",
    "    x_min = min(get_bounding_box(item['poly'])[0] for item in items)\n",
    "    y_min = min(get_bounding_box(item['poly'])[1] for item in items)\n",
    "    x_max = max(get_bounding_box(item['poly'])[2] for item in items)\n",
    "    y_max = max(get_bounding_box(item['poly'])[3] for item in items)\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def find_matches(metadata, category_types):\n",
    "    filtered_items = [item for item in metadata if item['category_type'] in category_types]\n",
    "    results = []\n",
    "\n",
    "    while filtered_items:\n",
    "        base_item = filtered_items.pop(0)\n",
    "        base_box = get_bounding_box(base_item['poly'])\n",
    "        group = [base_item]\n",
    "\n",
    "        for other_item in list(filtered_items):  # Use list to avoid modifying during iteration\n",
    "            other_box = get_bounding_box(other_item['poly'])\n",
    "            if do_boxes_overlap(base_box, other_box):\n",
    "                group.append(other_item)\n",
    "                filtered_items.remove(other_item)\n",
    "\n",
    "        consolidated_box = consolidate_positions(group)\n",
    "        concatenated_text = ' '.join(item.get('text', '') for item in group)\n",
    "        results.append({\n",
    "            'output_category': ' & '.join(item['category_type'] for item in group),\n",
    "            'output_poly': consolidated_box,\n",
    "            'output_text': concatenated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将图片、表格或公式保存为图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DPI = 144\n",
    "# since there is a manipulation of image size, we need to map the image coordinates back to the pdf coordinates\n",
    "def map_image_to_pdf(image_x, image_y, pix, dpi=DEFAULT_DPI):\n",
    "    if pix.width <= 3000 and pix.height <= 3000:\n",
    "        scale = dpi / 72\n",
    "        pdf_x = image_x / scale\n",
    "        pdf_y = image_y / scale\n",
    "    else:\n",
    "        pdf_x = image_x\n",
    "        pdf_y = image_y\n",
    "    return pdf_x, pdf_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_types = ['figure', 'figure_caption']\n",
    "results = find_matches(final_blocks[5], category_types)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原页面\n",
    "idx = 5\n",
    "page = doc.load_page(idx)\n",
    "pix = page.get_pixmap(matrix=fitz.Matrix(DEFAULT_DPI/72, DEFAULT_DPI/72))\n",
    "area = result['output_poly']\n",
    "x0, y0 = map_image_to_pdf(area[0], area[1], pix)\n",
    "x1, y1 = map_image_to_pdf(area[2], area[3], pix)\n",
    "\n",
    "pix_map = page.get_pixmap(clip=fitz.Rect(x0, y0, x1, y1))\n",
    "pix_map.save(\"output_new.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取对应的段落信息，作为上下文辅助\n",
    "- 思路一：从来源追溯，找寻最契合\n",
    "- 思路二：基于向量匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找对应的章节\n",
    "def find_titles_for_page(toc, page_idx):\n",
    "    titles = []\n",
    "    for i, entry in enumerate(toc):\n",
    "        # 对于最后一个条目，由于没有下一个条目，所以单独处理\n",
    "        if i == len(toc) - 1:\n",
    "            if entry.pagenum <= page_idx:\n",
    "                titles.append(entry.title)\n",
    "        else:\n",
    "            # 对于其他条目，确保页面索引在当前条目和下一个条目之间\n",
    "            if entry.pagenum <= page_idx < toc[i + 1].pagenum:\n",
    "                titles.append(entry.title)\n",
    "            # 如果当前条目和下一个条目的页码相同，则添加当前条目的标题\n",
    "            elif entry.pagenum == page_idx == toc[i + 1].pagenum:\n",
    "                titles.append(entry.title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 源头追溯\n",
    "idx = 5\n",
    "section_titles = find_titles_for_page(toc, idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模糊匹配\n",
    "import difflib\n",
    "\n",
    "def fuzzy_match(short_texts, long_texts):\n",
    "    matches = []\n",
    "    for i, short_text in enumerate(short_texts):\n",
    "        # 使用difflib.get_close_matches获取所有可能的匹配项\n",
    "        close_matches = difflib.get_close_matches(short_text, long_texts, n=len(long_texts), cutoff=0.0)\n",
    "        # 如果有匹配项，选择相似度最高的一个\n",
    "        if close_matches:\n",
    "            # 按相似度排序，取第一个元素（相似度最高）\n",
    "            best_match = max(close_matches, key=lambda x: difflib.SequenceMatcher(None, short_text, x).ratio())\n",
    "            # 获取长文本在列表中的位置\n",
    "            best_match_index = long_texts.index(best_match)\n",
    "            # 将匹配的索引对添加到列表中\n",
    "            matches.append((i, best_match_index))\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rslts = fuzzy_match(section_titles, [item[:50] for item in filtered_paras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conten = filtered_paras[test_rslts[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_extraction_prompt = \"\"\"## TASK\n",
    "Extract key information from context that is relevant to the clues.\n",
    "\n",
    "## CLUES\n",
    "{intro_of_figure_table_formula}\n",
    "\n",
    "## CONTEXT\n",
    "{context}\n",
    "\n",
    "## OUTPUT\n",
    "Related information are: \\n\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = points_extraction_prompt.format(\n",
    "    intro_of_figure_table_formula=result['output_text'],\n",
    "    context=conten)\n",
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = zhipu_llm(None, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于向量匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size, overlap_size):\n",
    "    # 确保重叠大小不超过chunk大小\n",
    "    overlap_size = min(overlap_size, chunk_size)\n",
    "    \n",
    "    # 使用正则表达式分割文本，保持句子的完整性\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 如果当前chunk加上下一个句子小于chunk_size，则加入当前chunk\n",
    "        if len(current_chunk) + len(sentence) < chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            # 如果加上下一个句子超过chunk_size，则先保存当前chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # 计算重叠部分\n",
    "            overlap = \" \" + \" \".join(sentences[sentences.index(sentence)-1].split()[-overlap_size:])\n",
    "            # 开始新的chunk，包含重叠部分\n",
    "            current_chunk = overlap + sentence + \" \"\n",
    "    \n",
    "    # 添加最后一个chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多模态语义理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "def zhipu_vllm(img_path, prompt):\n",
    "    with open(img_path, 'rb') as img_file:\n",
    "        img_base = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "    client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY_1\")) # 填写您自己的APIKey\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4v\",  # \"glm-4v-plus\",  # 填写需要调用的模型名称\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": img_base\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }\n",
    "            ]\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "    return (response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"## TASK\n",
    "You are an academic scholar analyzing a image from a paper. \n",
    "Extract key information from the image that is relevant to the context.\n",
    "Try to answer the following questions: \n",
    "1. What is the image showing?\n",
    "2. What is the image related to?\n",
    "3. What is the image trying to convey?\n",
    "Be very concise and explicit in your answers. Try to show concrete results and numbers.\n",
    "\n",
    "## CONTEXT\n",
    "Here is background information of the paper for your guidance:\n",
    "<background>\n",
    "{background}\n",
    "</background>\n",
    "\n",
    "Here is short description of the image:\n",
    "<description>\n",
    "{description}\n",
    "</description>\n",
    "\n",
    "## OUTPUT\n",
    "The image reveals that: \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'output_new.png'\n",
    "prompt = prompt.format(\n",
    "    background=test_result,\n",
    "    description=result['output_text'])\n",
    "tmp_result = zhipu_vllm(img_path, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "for idx, item in enumerate(final_blocks):\n",
    "    full_text = \n",
    "    category_types = ['figure', 'figure_caption']\n",
    "    results = find_matches(final_blocks[5], category_types)\n",
    "\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['output_poly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
