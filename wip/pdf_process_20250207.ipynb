{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"Training Large Language Models to Reason in a Continuous Latent Space\", \n",
    "          \"Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought\",\n",
    "          \"s1: Simple test-time scaling\",\n",
    "          \"From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\",\n",
    "          \"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search\"]\n",
    "\n",
    "\n",
    "pdf_pathes = [\"./data/2412.06769v2.pdf\",\n",
    "              \"./data/2501.04682v1.pdf\",\n",
    "              \"./data/2501.19393v2.pdf\",\n",
    "              \"./data/2502.00330v1.pdf\",\n",
    "              \"./data/2502.02508v1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前脚本所在目录的父目录 (即 my_project)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# 将父目录添加到 sys.path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Downloads a file from the given URL and saves it as filename.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"Successfully downloaded: {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(original_zip_file, destination_folder):\n",
    "    assert os.path.splitext(original_zip_file)[-1] == '.zip'\n",
    "    with zipfile.ZipFile(original_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.arxiv_tool import ArxivKit\n",
    "from apis.semanticscholar_tool import SemanticScholarKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:41:03,925 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Training+Large+Language+Models+to+Reason+in+a+Continuous+Latent+Space&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-07 10:41:11,305 - INFO - Got first page: 100 of 2652267 total results\n",
      "2025-02-07 10:41:16,311 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Towards+System+2+Reasoning+in+LLMs%3A+Learning+How+to+Think+With+Meta+Chain-of-Thought&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-07 10:41:18,723 - INFO - Got first page: 100 of 2641739 total results\n",
      "2025-02-07 10:41:23,730 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=s1%3A+Simple+test-time+scaling&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-07 10:41:26,283 - INFO - Got first page: 100 of 297166 total results\n",
      "2025-02-07 10:41:31,289 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=From+Few+to+Many%3A+Self-Improving+Many-Shot+Reasoners+Through+Iterative+Optimization+and+Generation&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-07 10:41:36,934 - INFO - Got first page: 100 of 2645263 total results\n",
      "2025-02-07 10:41:41,940 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Satori%3A+Reinforcement+Learning+with+Chain-of-Action-Thought+Enhances+LLM+Reasoning+via+Autoregressive+Search&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-07 10:41:44,511 - INFO - Got first page: 100 of 2046048 total results\n"
     ]
    }
   ],
   "source": [
    "arxiv = ArxivKit()\n",
    "\n",
    "arxiv_metadata = []\n",
    "for title in titles:\n",
    "    candit_arxiv_metadata = arxiv.retrieve_metadata_by_paper(query_term=title, max_cnt=3)\n",
    "    arxiv_metadata.append(candit_arxiv_metadata)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:40:25,144 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Training%20Large%20Language%20Models%20to%20Reason%20in%20a%20Continuous%20Latent%20Space&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n",
      "2025-02-07 10:40:31,719 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Towards%20System%202%20Reasoning%20in%20LLMs:%20Learning%20How%20to%20Think%20With%20Meta%20Chain-of-Thought&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n",
      "2025-02-07 10:40:39,461 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=s1:%20Simple%20test-time%20scaling&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n",
      "2025-02-07 10:40:46,329 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=From%20Few%20to%20Many:%20Self-Improving%20Many-Shot%20Reasoners%20Through%20Iterative%20Optimization%20and%20Generation&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n",
      "2025-02-07 10:40:52,820 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Satori:%20Reinforcement%20Learning%20with%20Chain-of-Action-Thought%20Enhances%20LLM%20Reasoning%20via%20Autoregressive%20Search&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "ss = SemanticScholarKit()\n",
    "\n",
    "ss_metadata = []\n",
    "for title in titles:\n",
    "    candit_ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)\n",
    "    ss_metadata.append(candit_ss_metadata)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference and Citedby Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673fbdd957cada770d10dffca5e45b53da43a3c6\n"
     ]
    }
   ],
   "source": [
    "paper_ss_id = ss_metadata[0][0].get('paperId')\n",
    "print(paper_ss_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:46:15,478 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-02-07 10:46:46,765 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-02-07 10:47:18,262 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 429 \"\n",
      "2025-02-07 10:47:50,088 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reference_metadata = ss.get_semanticscholar_references(paper_id=paper_ss_id, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:47:52,816 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/citations?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "citedby_metadata = ss.get_semanticscholar_citedby(paper_id=paper_ss_id, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(citedby_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage MinerU for PDF Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minerU API from https://mineru.net/apiManage/docs\n",
    "# Note: monitor_batch_status need to be further tested\n",
    "import os\n",
    "import uuid\n",
    "import copy\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "TASK_URL = \"https://mineru.net/api/v4/extract/task\"\n",
    "BATCH_URL = \"https://mineru.net/api/v4/file-urls/batch\"\n",
    "BATCH_STATUS_URL = \"https://mineru.net/api/v4/extract-results/batch\"\n",
    "\n",
    "def detect_lang(string):\n",
    "    \"\"\"\n",
    "    检查整个字符串是否包含中文\n",
    "    :param string: 需要检查的字符串\n",
    "    :return: bool\n",
    "    \"\"\"\n",
    "\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return 'zh'\n",
    "    return 'en'\n",
    "\n",
    "class MinerUKit:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.task_url = TASK_URL\n",
    "        self.batch_url = BATCH_URL\n",
    "        self.batch_status_url = BATCH_STATUS_URL\n",
    "        self.header = {\n",
    "                    'Content-Type':'application/json',\n",
    "                    \"Authorization\":f\"Bearer {self.api_key}\"\n",
    "                 }\n",
    "        self.config = {\n",
    "            \"enable_formula\": True,\n",
    "            \"language\": \"en\",\n",
    "            \"layout_model\":\"doclayout_yolo\",\n",
    "            \"enable_table\": True\n",
    "        }\n",
    "\n",
    "    def single_process_request(self, pdf_url, if_ocr, lang):\n",
    "        \"\"\"apply MinerU API to process single PDF\n",
    "        \"\"\"\n",
    "        data = copy.deepcopy(self.config)\n",
    "        data['url'] = pdf_url\n",
    "        data['is_ocr'] = if_ocr\n",
    "        data['language'] = lang\n",
    "        response = requests.post(url=self.task_url, headers=self.header, json=data)\n",
    "        print(response.status_code)\n",
    "        return response\n",
    "    \n",
    "    def batch_process_files(self, pdf_files:List[str], if_ocr:Optional[bool]=False, lang:Optional[str]='en'):\n",
    "        \"\"\"apply MinerU API to process multiple PDF in local path\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for file in pdf_files:\n",
    "            files.append({\"name\": os.path.basename(file),\n",
    "                          \"data_id\": str(uuid.uuid1())})\n",
    "        data = copy.deepcopy(self.config)\n",
    "        data['is_ocr'] = if_ocr\n",
    "        data['language'] = lang\n",
    "        data['files'] = files\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url=self.batch_url,headers=self.header,json=data)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                print('response success. result:{}'.format(result))\n",
    "                if result[\"code\"] == 0:\n",
    "                    batch_id = result[\"data\"][\"batch_id\"]\n",
    "                    urls = result[\"data\"][\"file_urls\"]\n",
    "                    print('batch_id:{},urls:{}'.format(batch_id, urls))\n",
    "\n",
    "                    for idx, file_path in enumerate(pdf_files):\n",
    "                        with open(file_path, 'rb') as f:\n",
    "                            res_upload = requests.put(urls[idx], data=f)\n",
    "                        if res_upload.status_code == 200:\n",
    "                            print(\"upload success\")\n",
    "                        else:\n",
    "                            print(\"upload failed\")\n",
    "                else:\n",
    "                    print('apply upload url failed,reason:{}'.format(result.msg))\n",
    "            else:\n",
    "                print('response not success. status:{} ,result:{}'.format(response.status_code, response))\n",
    "            return response\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def batch_process_urls(self, pdf_urls:List[str], if_ocr:Optional[bool]=False, lang:Optional[str]='en'):\n",
    "        \"\"\"apply MinerU API to process multiple PDF urls\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        for pdf_url in pdf_urls:\n",
    "            files.append({\"url\": pdf_url,\n",
    "                          \"data_id\": str(uuid.uuid1())})\n",
    "        data = copy.deepcopy(self.config)\n",
    "        data['is_ocr'] = if_ocr\n",
    "        data['language'] = lang\n",
    "        data['files'] = files\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url=self.batch_url, headers=self.header, json=data)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                print('response success. result:{}'.format(result))\n",
    "                if result[\"code\"] == 0:\n",
    "                    batch_id = result[\"data\"][\"batch_id\"]\n",
    "                    print('batch_id:{}'.format(batch_id))\n",
    "                else:\n",
    "                    print('submit task failed,reason:{}'.format(result.msg))\n",
    "            else:\n",
    "                print('response not success. status:{} ,result:{}'.format(response.status_code, response))\n",
    "            return response\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def batch_status_check(self, batch_id):\n",
    "        \"\"\"check status code of batch task\n",
    "        \"\"\"\n",
    "        url = f'{self.batch_status_url}/{batch_id}'\n",
    "        res = requests.get(url=url, headers=self.header)\n",
    "        print(res.status_code)\n",
    "        # print(res.json())\n",
    "        return res\n",
    "    \n",
    "    def download_and_unzip(self, zip_url, download_file_name, unzip_folder_name):\n",
    "        \"\"\"download and unzip MinerU processed files\"\"\"\n",
    "        download_file(zip_url, download_file_name)\n",
    "        unzip_file(download_file_name, unzip_folder_name)\n",
    "\n",
    "    def monitor_batch_status(self, batch_id, save_path, interval=10, max_retries=10):\n",
    "        \"\"\"\n",
    "        monitor batch run status, try to download with max_retries\n",
    "\n",
    "        Args:\n",
    "            batch_id: batch id\n",
    "            save_path: path to save processed files (in folder whose name aligned with orginal pdf)\n",
    "            interval: time interval for next check (in seconds)\n",
    "            max_retries: max retries\n",
    "        \"\"\"\n",
    "        downloaded_files = set()  # 记录已下载的文件名，避免重复下载\n",
    "\n",
    "        for _ in range(max_retries):\n",
    "            running_res = self.batch_status_check(batch_id)\n",
    "            if running_res.json().get('msg') == 'ok':\n",
    "                results = running_res.json().get('data', {}).get('extract_result', [])\n",
    "                for item in results:\n",
    "                    if item.get('state') == 'done':\n",
    "                        file_name = item.get('file_name')\n",
    "                        if file_name not in downloaded_files:  # 检查是否已下载\n",
    "                            file_name_nosuffix = file_name.rsplit('.', 1)[0]\n",
    "                            zip_url = item.get('full_zip_url')\n",
    "                            download_file_name = os.path.join(save_path, file_name_nosuffix + \".zip\")\n",
    "                            unzip_folder_name = os.path.join(save_path, file_name_nosuffix)\n",
    "\n",
    "                            # 使用线程下载并解压，避免阻塞主线程\n",
    "                            thread = threading.Thread(\n",
    "                                target=self.download_and_unzip,\n",
    "                                args=(zip_url, download_file_name, unzip_folder_name)\n",
    "                            )\n",
    "                            thread.start()\n",
    "\n",
    "                            downloaded_files.add(file_name)  # 标记为已下载\n",
    "                \n",
    "                # 检查是否全部完成\n",
    "                all_done = all(item.get('state') == 'done' for item in results)\n",
    "                if all_done:\n",
    "                    print(f\"Batch {batch_id} complte\")\n",
    "                    return\n",
    "\n",
    "            print(f\"Batch {batch_id} running, recheck in next {interval} seconds...\")\n",
    "            time.sleep(interval)\n",
    "\n",
    "        print(f\"Exit as batch {batch_id} reached max retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response success. result:{'code': 0, 'msg': 'ok', 'trace_id': '70e0b490acbd3e669f7417132a6c0aaf', 'data': {'batch_id': '1c431751-178f-45f0-b69b-b35faca8bcde', 'file_urls': ['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/443db9c4-b324-4c27-83c1-7ae5cccd0d94.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=grLBcVRj4acEWaGHNFevsVRmOHE%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/15163ff9-fa5f-418e-90a0-fb37003d5bcc.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=XFlNA4fsl8%2FsOnTpAxnwV8NMkF8%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/28265148-7c3e-4fe5-937d-6b833754709e.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=kiAClxMx31U8Nf1G5hBSZ%2FXHMwY%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/9c7daf9c-d07b-4fa2-a224-b181b03393b4.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=d1B2UrxrHvVuX4Ve3qaNXw9rTt0%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/d2920324-318c-466b-8a32-bd06997bb590.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=agPYDVNpDE9RzuYZT%2BytF%2BO41aU%3D']}}\n",
      "batch_id:1c431751-178f-45f0-b69b-b35faca8bcde,urls:['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/443db9c4-b324-4c27-83c1-7ae5cccd0d94.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=grLBcVRj4acEWaGHNFevsVRmOHE%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/15163ff9-fa5f-418e-90a0-fb37003d5bcc.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=XFlNA4fsl8%2FsOnTpAxnwV8NMkF8%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/28265148-7c3e-4fe5-937d-6b833754709e.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=kiAClxMx31U8Nf1G5hBSZ%2FXHMwY%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/9c7daf9c-d07b-4fa2-a224-b181b03393b4.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=d1B2UrxrHvVuX4Ve3qaNXw9rTt0%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/1c431751-178f-45f0-b69b-b35faca8bcde/d2920324-318c-466b-8a32-bd06997bb590.pdf?Expires=1738983651&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=agPYDVNpDE9RzuYZT%2BytF%2BO41aU%3D']\n",
      "upload success\n",
      "upload success\n",
      "upload success\n",
      "upload success\n",
      "upload success\n"
     ]
    }
   ],
   "source": [
    "mineru_api_key = os.getenv('MINERU_API_KEY_1')\n",
    "mineru = MinerUKit(api_key=mineru_api_key)\n",
    "upload_res = mineru.batch_process_files(pdf_files=pdf_pathes, if_ocr=False, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'code': 0, 'msg': 'ok', 'trace_id': 'bc7f2824f223f0fbcfc47c060988cca5', 'data': {'batch_id': '1c431751-178f-45f0-b69b-b35faca8bcde', 'extract_result': [{'data_id': 'bc80f48e-e4ff-11ef-9430-7413ea7e2ff2', 'file_name': '2412.06769v2.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/14aa1800-9750-4ad2-9941-afdbfd1a32d0.zip'}, {'data_id': 'bc80f51a-e4ff-11ef-9430-7413ea7e2ff2', 'file_name': '2501.04682v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/bd6b47a3-b65a-4670-9bac-158aaf8d32bf.zip'}, {'data_id': 'bc80f542-e4ff-11ef-9430-7413ea7e2ff2', 'file_name': '2501.19393v2.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/b3fbc814-da53-4ba7-b19a-854fd32c8e7e.zip'}, {'data_id': 'bc80f556-e4ff-11ef-9430-7413ea7e2ff2', 'file_name': '2502.00330v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/039a6a8b-5f27-4f88-a2df-3988a66e6af9.zip'}, {'data_id': 'bc80f574-e4ff-11ef-9430-7413ea7e2ff2', 'file_name': '2502.02508v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/2af9928d-81a2-4d15-a166-d0e8144c0ca9.zip'}]}}\n"
     ]
    }
   ],
   "source": [
    "batch_id = upload_res.json().get('data', {}).get('batch_id')\n",
    "running_res = mineru.batch_status_check(batch_id=batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: ./tmp/2412.06769v2.zip\n",
      "Successfully downloaded: ./tmp/2501.04682v1.zip\n",
      "Successfully downloaded: ./tmp/2501.19393v2.zip\n",
      "Successfully downloaded: ./tmp/2502.00330v1.zip\n",
      "Successfully downloaded: ./tmp/2502.02508v1.zip\n"
     ]
    }
   ],
   "source": [
    "temp_path = \"./tmp\"\n",
    "\n",
    "if running_res.json().get('msg') == 'ok':\n",
    "    results = running_res.json().get('data', {}).get('extract_result', []) \n",
    "    for item in results:\n",
    "        if item.get('state') == 'done':\n",
    "            file_name_nosuffix = item.get('file_name').rsplit('.', 1)[0] \n",
    "            zip_url = item.get('full_zip_url')\n",
    "            download_file_name = os.path.join(temp_path, file_name_nosuffix+\".zip\") \n",
    "            unzip_folder_name = os.path.join(temp_path, file_name_nosuffix) \n",
    "            download_file(zip_url, download_file_name)\n",
    "            unzip_file(download_file_name, unzip_folder_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def pdf_outline_detection(pdf_path, excpert_len:Optional[int]=300):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    toc_infos = doc.get_toc(simple=False) or []\n",
    "\n",
    "    pdf_toc = []\n",
    "    for item in toc_infos:\n",
    "        lvl = item[0] if len(item) > 0 else None\n",
    "        title = item[1] if len(item) > 1 else None\n",
    "        start_page = item[2] if len(item) > 2 else None\n",
    "        end_pos = item[3].get('to') if len(item) > 3 and item[3] else None\n",
    "        nameddest = item[3].get('nameddest') if len(item) > 3 and item[3] else None\n",
    "        if_collapse = item[3].get('collapse', False) if len(item) > 3 and item[3] else None\n",
    "\n",
    "        if start_page is not None:\n",
    "            page = doc[start_page-1]\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "\n",
    "            lines = \"\"\n",
    "            for block in blocks:\n",
    "                x0, y0, x1, y1, text, _, _ = block\n",
    "                if len(lines) < excpert_len:\n",
    "                    if end_pos and x0 >= end_pos[0]:\n",
    "                        lines += text\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            pdf_toc.append({\n",
    "                \"level\": lvl,\n",
    "                \"title\": title,\n",
    "                \"page\": start_page,\n",
    "                \"position\": end_pos,\n",
    "                \"nameddest\": nameddest,\n",
    "                'if_collapse': if_collapse,\n",
    "                \"excerpt\": lines + \"...\"\n",
    "            })\n",
    "    return pdf_toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_toc = pdf_outline_detection(pdf_path=pdf_pathes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level': 1,\n",
       "  'title': 'Introduction',\n",
       "  'page': 1,\n",
       "  'position': Point(70.866, 378.949),\n",
       "  'nameddest': 'section.1',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'Training Large Language Models to Reason in a\\nContinuous Latent Space\\nShibo Hao1,2,∗, Sainbayar Sukhbaatar1, DiJia Su1, Xian Li1, Zhiting Hu2, Jason Weston1, Yuandong Tian1\\n1FAIR at Meta, 2UC San Diego\\n∗Work done at Meta\\nLarge language models (LLMs) are restricted to reason in the “language space”, where they typically\\nexpress the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem.\\nHowever, we argue that language space may not always be optimal for reasoning. For example, most\\nword tokens are primarily for textual coherence and not essential for reasoning, while some critical\\ntokens require complex planning and pose huge challenges to LLMs. To explore the potential of\\nLLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new\\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM\\nas a representation of the reasoning state (termed “continuous thought”). Rather than decoding this\\ninto a word token, we feed it back to the LLM as the subsequent input embedding directly in the\\ncontinuous space. Experiments show that Coconut can effectively augment the LLM on several\\nreasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns:\\nthe continuous thought can encode multiple alternative next reasoning steps, allowing the model to\\nperform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a\\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that\\nrequire substantial backtracking during planning, with fewer thinking tokens during inference. These\\nfindings demonstrate the promise of latent reasoning and offer valuable insights for future research.\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Related Work',\n",
       "  'page': 2,\n",
       "  'position': Point(70.866, 235.616),\n",
       "  'nameddest': 'section.2',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '2\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Coconut: Chain of Continuous Thought',\n",
       "  'page': 3,\n",
       "  'position': Point(70.866, 351.688),\n",
       "  'nameddest': 'section.3',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'Ht = Transformer(Et)\\nM(xt+1 | x≤t) = softmax(Wht)\\n3\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Experiments',\n",
       "  'page': 4,\n",
       "  'position': Point(70.866, 121.688),\n",
       "  'nameddest': 'section.4',\n",
       "  'if_collapse': True,\n",
       "  'excerpt': '1If a language reasoning chain is shorter than k steps, then all the language thoughts will be removed.\\n4\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Reasoning Tasks',\n",
       "  'page': 5,\n",
       "  'position': Point(70.866, 654.762),\n",
       "  'nameddest': 'subsection.4.1',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '5\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Experimental Setup',\n",
       "  'page': 5,\n",
       "  'position': Point(70.866, 393.291),\n",
       "  'nameddest': 'subsection.4.2',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '5\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Baselines and Variants of Coconut',\n",
       "  'page': 5,\n",
       "  'position': Point(70.866, 137.798),\n",
       "  'nameddest': 'subsection.4.3',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '5\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Results and Discussion',\n",
       "  'page': 6,\n",
       "  'position': Point(70.866, 353.621),\n",
       "  'nameddest': 'subsection.4.4',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'Method\\nGSM8k\\nProntoQA\\nProsQA\\nAcc. (%)\\n# Tokens\\nAcc. (%)\\n# Tokens\\nAcc. (%)\\n# Tokens\\nCoT\\n42.9 ±0.2\\n25.0\\n98.8 ±0.8\\n92.5\\n77.5 ±1.9\\n49.4\\nNo-CoT\\n16.5 ±0.5\\n2.2\\n93.8 ±0.7\\n3.0\\n76.7 ±1.0\\n8.2\\niCoT\\n30.0∗\\n2.2\\n99.8 ±0.3\\n3.0\\n98.2 ±0.3\\n8.2\\nPause Token\\n16.4 ±1.8\\n2.2\\n77.7 ±21.0\\n3.0\\n75.9 ±0.7\\n8.2\\nCoconut (Ours)\\n34.1 ±1.5\\n8.2\\n99.8 ±0.2\\n9.0\\n97.0 ±0.3\\n14.2\\n- w/o curriculum\\n14.4 ±0.8\\n8.2\\n52.4 ±0.4\\n9.0\\n76.1 ±0.2\\n14.2\\n- w/o thought\\n21.6 ±0.5\\n2.3\\n99.9 ±0.1\\n3.0\\n95.5 ±1.1\\n8.2\\n- pause as thought\\n24.1 ±0.7\\n2.2\\n100.0 ±0.1\\n3.0\\n96.6 ±0.8\\n8.2\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Understanding the Latent Reasoning in Coconut',\n",
       "  'page': 7,\n",
       "  'position': Point(70.866, 112.585),\n",
       "  'nameddest': 'section.5',\n",
       "  'if_collapse': True,\n",
       "  'excerpt': 'Figure 4 A case study where we decode the continuous\\nthought into language tokens.\\n2We discuss the case of larger c in Appendix C.\\n7\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Experimental Setup',\n",
       "  'page': 8,\n",
       "  'position': Point(70.866, 408.338),\n",
       "  'nameddest': 'subsection.5.1',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '8\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Interpolating between Latent and Language Reasoning',\n",
       "  'page': 9,\n",
       "  'position': Point(70.866, 252.643),\n",
       "  'nameddest': 'subsection.5.2',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '9\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Interpreting the Latent Search Tree',\n",
       "  'page': 10,\n",
       "  'position': Point(70.866, 392.467),\n",
       "  'nameddest': 'subsection.5.3',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '10\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Why is a Latent Space Better for Planning?',\n",
       "  'page': 11,\n",
       "  'position': Point(70.866, 680.456),\n",
       "  'nameddest': 'subsection.5.4',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'Figure 9 The correlation between prediction probabil-\\nity of concepts and their heights.\\n11\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Conclusion',\n",
       "  'page': 11,\n",
       "  'position': Point(70.866, 257.59),\n",
       "  'nameddest': 'section.6',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'Figure 9 The correlation between prediction probabil-\\nity of concepts and their heights.\\n11\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Datasets',\n",
       "  'page': 15,\n",
       "  'position': Point(70.866, 711.856),\n",
       "  'nameddest': 'appendix.A',\n",
       "  'if_collapse': True,\n",
       "  'excerpt': 'GSM8k\\nQuestion = \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4\\ninches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay\\nper year?\"\\nSteps = [\"«4-2=2»\", \"«2/.5=4»\", \"«12/4=3»\", \"«100*3=300»\"]\\nAnswer = \"300\"\\nProntoQA\\nQuestion = \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus.\\nGorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are\\nnot floral. Lempuses are cold. Brimpuses are impuses. Every lorpus is floral. Every rompus\\nis transparent.\\nGrimpuses are muffled.\\nRompuses are yumpuses.\\nRompuses are wumpuses.\\nZumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus.\\nYumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses.\\nEvery lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not\\nfloral.\"\\nSteps = [\"Stella is a zumpus.\\nZumpuses are gorpuses.\", \"Stella is a gorpus.\\nGorpuses are\\nrompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus\\nis a lorpus.\", \"Stella is a lorpus. Every lorpus is floral.\", \"Stella is floral.\"]\\nAnswer = \"False\"\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Examples',\n",
       "  'page': 15,\n",
       "  'position': Point(70.866, 673.46),\n",
       "  'nameddest': 'subsection.A.1',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'GSM8k\\nQuestion = \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4\\ninches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay\\nper year?\"\\nSteps = [\"«4-2=2»\", \"«2/.5=4»\", \"«12/4=3»\", \"«100*3=300»\"]\\nAnswer = \"300\"\\nProntoQA\\nQuestion = \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus.\\nGorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are\\nnot floral. Lempuses are cold. Brimpuses are impuses. Every lorpus is floral. Every rompus\\nis transparent.\\nGrimpuses are muffled.\\nRompuses are yumpuses.\\nRompuses are wumpuses.\\nZumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus.\\nYumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses.\\nEvery lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not\\nfloral.\"\\nSteps = [\"Stella is a zumpus.\\nZumpuses are gorpuses.\", \"Stella is a gorpus.\\nGorpuses are\\nrompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus\\nis a lorpus.\", \"Stella is a lorpus. Every lorpus is floral.\", \"Stella is floral.\"]\\nAnswer = \"False\"\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Construction of ProsQA',\n",
       "  'page': 15,\n",
       "  'position': Point(70.866, 216.127),\n",
       "  'nameddest': 'subsection.A.2',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'GSM8k\\nQuestion = \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4\\ninches he cuts it back down to 2 inches. It cost $100 to get his grass cut. How much does he pay\\nper year?\"\\nSteps = [\"«4-2=2»\", \"«2/.5=4»\", \"«12/4=3»\", \"«100*3=300»\"]\\nAnswer = \"300\"\\nProntoQA\\nQuestion = \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus.\\nGorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are\\nnot floral. Lempuses are cold. Brimpuses are impuses. Every lorpus is floral. Every rompus\\nis transparent.\\nGrimpuses are muffled.\\nRompuses are yumpuses.\\nRompuses are wumpuses.\\nZumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus.\\nYumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses.\\nEvery lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not\\nfloral.\"\\nSteps = [\"Stella is a zumpus.\\nZumpuses are gorpuses.\", \"Stella is a gorpus.\\nGorpuses are\\nrompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus\\nis a lorpus.\", \"Stella is a lorpus. Every lorpus is floral.\", \"Stella is floral.\"]\\nAnswer = \"False\"\\n...'},\n",
       " {'level': 2,\n",
       "  'title': 'Statistics',\n",
       "  'page': 16,\n",
       "  'position': Point(70.866, 132.32),\n",
       "  'nameddest': 'subsection.A.3',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '# Nodes\\n# Edges\\nLen. of Shortest Path\\n# Shortest Paths\\n23.0\\n36.0\\n3.8\\n1.6\\nTable 2 Statistics of the graph structure in ProsQA.\\nedges ←{}\\nnodes ←{0, 1}\\nlabels ←{0 : 1, 1 : 2}\\n▷Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).\\ngroups ←{0 : {}, 1 : {0}, 2 : {1}, 3 : {}}\\nidx ←2\\nwhile idx < N do\\n▷For each new node, randomly add edges from existing nodes\\nn_in_nodes ←poisson(1.5)\\nrand ←random()\\nif rand ≤0.35 then\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Clock-Time Reasoning Efficiency Metric',\n",
       "  'page': 16,\n",
       "  'position': Point(70.866, 87.979),\n",
       "  'nameddest': 'appendix.B',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': '# Nodes\\n# Edges\\nLen. of Shortest Path\\n# Shortest Paths\\n23.0\\n36.0\\n3.8\\n1.6\\nTable 2 Statistics of the graph structure in ProsQA.\\nedges ←{}\\nnodes ←{0, 1}\\nlabels ←{0 : 1, 1 : 2}\\n▷Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).\\ngroups ←{0 : {}, 1 : {0}, 2 : {1}, 3 : {}}\\nidx ←2\\nwhile idx < N do\\n▷For each new node, randomly add edges from existing nodes\\nn_in_nodes ←poisson(1.5)\\nrand ←random()\\nif rand ≤0.35 then\\n...'},\n",
       " {'level': 1,\n",
       "  'title': 'Using More Continuous Thoughts',\n",
       "  'page': 17,\n",
       "  'position': Point(70.866, 447.201),\n",
       "  'nameddest': 'appendix.C',\n",
       "  'if_collapse': False,\n",
       "  'excerpt': 'Dataset\\nTraining\\nValidation\\nTest\\nGSM8k\\n385,620\\n500\\n1319\\nProntoQA\\n9,000\\n200\\n800\\nProsQA\\n17,886\\n300\\n500\\nTable 3 Statistics of the datasets.\\nMethod\\nGSM8k\\nProntoQA\\nProsQA\\nNo-CoT\\n0.03\\n0.03\\n0.08\\nCoT\\n0.26\\n0.85\\n0.47\\nCoconut\\n0.09\\n0.11\\n0.15\\nTable 4 Inference time (in seconds) comparison across tasks and methods.\\n...'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_toc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following would use 2412.06769v2 for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_nosuffix = \"2412.06769v2\"\n",
    "file_path = os.path.join(temp_path, file_name_nosuffix)\n",
    "\n",
    "from pathlib import Path  \n",
    " \n",
    "for file in Path(file_path).glob('*'): \n",
    "    file_nm = os.path.basename(file)\n",
    "    if \"_origin.pdf\" in file_nm:\n",
    "        os.remove(file) \n",
    "    elif \"_content_list.json\" in file_nm:\n",
    "        os.rename(file, os.path.join(file_path, \"content_list.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = os.path.join(file_path, \"full.md\")\n",
    "content_json_file = os.path.join(file_path, \"content_list.json\")\n",
    "layout_json_file = os.path.join(file_path, \"layout.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- update markdown title\n",
    "- update markdown table and image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def restore_md_toc(md_content, pdf_toc):\n",
    "    \"\"\"\n",
    "    Align markdown title with pdf table of content (generated from fitz)\n",
    "\n",
    "    Args:\n",
    "        md_file: Path to the markdown file.\n",
    "        pdf_toc: pdf toc from pdf_outline_detection function\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a section\n",
    "        with 'level', 'section_num', 'title', and 'text' keys.\n",
    "        Returns an empty list if the file doesn't exist.\n",
    "        Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    # if not os.path.exists(md_file):\n",
    "    #     return []\n",
    "\n",
    "    # try:\n",
    "    #     with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    #         markdown_content = f.read()\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error reading file: {e}\")\n",
    "    #     return None\n",
    "\n",
    "\n",
    "    if pdf_toc:\n",
    "        modified_lines = []  # 用于存储修改后的行的列表\n",
    "\n",
    "        title_pattern = r\"^#{1,}\\s*.*$\"  # patttern of markdown title\n",
    "        md_titles = []\n",
    "\n",
    "        for idx, line in enumerate(md_content.splitlines()):  # iterate markdown lines\n",
    "            match = re.search(title_pattern, line)\n",
    "            if match:  # find markdown title\n",
    "                sec_title = line\n",
    "                flag = 0\n",
    "\n",
    "                for x in pdf_toc:  # iterate pdf toc, refine markdown title based on toc title\n",
    "                    toc_title = x['title'] \n",
    "                    toc_level = int(x['level'])  \n",
    "                    if toc_title in line:  \n",
    "                        sec_title = \"#\"*toc_level + \" \" + toc_title + \"  \"\n",
    "                        flag = 1\n",
    "                        break\n",
    "                \n",
    "                if flag == 0:  # markdown title not exit in toc\n",
    "                    for item in ['Acknowledgement', 'Reference', 'Appendix']:\n",
    "                        if item in line:\n",
    "                            sec_title = line\n",
    "                            flag = 1\n",
    "                \n",
    "                if flag == 0:\n",
    "                    if len(md_titles) > 0:\n",
    "                        if re.match('^#{1,}', md_titles[-1]):\n",
    "                            pre_level = re.match('^#{1,}', md_titles[-1]).group(0) + \"#\"\n",
    "                            sec_title = re.sub('^#{1,}', pre_level, line)\n",
    "                        else:\n",
    "                            sec_title = \"#\" + line\n",
    "\n",
    "                modified_lines.append(sec_title)\n",
    "                md_titles.append(sec_title)  # get markdown title\n",
    "\n",
    "            else:\n",
    "                modified_lines.append(line)\n",
    "    return \"\\n\".join(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_lines(text, sentence_length):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # 使用正则表达式分割句子\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|;|!)\\s', text) # 更精确的断句正则\n",
    "\n",
    "    result = \"\"\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.strip()\n",
    "\n",
    "        if cleaned_sentence:\n",
    "            result += cleaned_sentence + \" \"\n",
    "            current_length = len(result.strip())\n",
    "\n",
    "            if current_length >= sentence_length:\n",
    "                return result.strip()\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_charts_and_tables(content_json):\n",
    "    \"\"\"assign title and ids to images/ charts, tables, and equations\n",
    "    \"\"\"\n",
    "    img_lst, tbl_lst, formula_lst = [], [], []\n",
    "    i, j, k = 1, 1, 1\n",
    "    for x in content_json:\n",
    "        if x['type'] == 'image':\n",
    "\n",
    "            desc = \"\\n\".join(x.get('img_caption', [])) + \"\\n\".join(x.get('img_footnote', []))\n",
    "            ptrn = r\"(pic|picture|img|image|chart|figure|fig)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "            mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "            img_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                img_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "            if len(img_ids) == 0:\n",
    "                img_ids = [f\"Image_Number_{i}\"]\n",
    "                i += 1\n",
    "            x['id'] = img_ids[0]\n",
    "            x['related_ids'] = img_ids[1:]\n",
    "            x['img_title'] = get_first_lines(desc, 10)\n",
    "\n",
    "            img_lst.append(x)\n",
    "\n",
    "        elif x['type'] == 'table':\n",
    "\n",
    "            desc = \"\\n\".join(x.get('table_caption', [])) + \"\\n\".join(x.get('table_footnote', []))\n",
    "            ptrn = r\"(tbl|table|chart|figure|fig)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "            mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "            tbl_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                tbl_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "            if len(tbl_ids) == 0:\n",
    "                tbl_ids = [f\"Table_Number_{j}\"]\n",
    "                j += 1\n",
    "            x['id'] = tbl_ids[0]\n",
    "            x['related_ids'] = tbl_ids[1:]\n",
    "            x['table_title'] = get_first_lines(desc, 10)\n",
    "\n",
    "            tbl_lst.append(x)\n",
    "\n",
    "        elif x['type'] == 'equation':\n",
    "\n",
    "            desc = x.get('text')\n",
    "            ptrn = r\"(formula|equation|notation|syntax)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "            mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "            equation_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                equation_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "            if len(equation_ids) == 0:\n",
    "                equation_ids = [f\"Equation_Number_{k}\"]\n",
    "                k += 1\n",
    "            x['id'] = equation_ids[0]\n",
    "            x['related_ids'] = equation_ids[1:]\n",
    "\n",
    "            formula_lst.append(x)\n",
    "    return img_lst, tbl_lst, formula_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./tmp/2412.06769v2/content_list.json\") as json_data:\n",
    "    content_json = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst, tbl_lst, formula_lst = restore_charts_and_tables(content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = \"./tmp/2412.06769v2/full.md\"\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'image',\n",
       "  'img_path': 'images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg',\n",
       "  'img_caption': ['Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 1,\n",
       "  'id': 'Figure 1',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT).'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg',\n",
       "  'img_caption': ['Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 3,\n",
       "  'id': 'Figure 2',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 2 Training procedure of Chain of Continuous Thought (Coconut).'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg',\n",
       "  'img_caption': ['Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 5,\n",
       "  'id': 'Figure 3',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts.'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg',\n",
       "  'img_caption': ['Figure 4 A case study where we decode the continuous thought into language tokens. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 6,\n",
       "  'id': 'Figure 4',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 4 A case study where we decode the continuous thought into language tokens.'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg',\n",
       "  'img_caption': ['Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 7,\n",
       "  'id': 'Figure 5',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg',\n",
       "  'img_caption': ['Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\\\scriptstyle\\\\mathrm{k=2}$ ) solves the problem correctly. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 8,\n",
       "  'id': 'Figure 6',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 6 A case study of ProsQA.'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg',\n",
       "  'img_caption': ['Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., “lempus” in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 8,\n",
       "  'id': 'Figure 7',\n",
       "  'related_ids': ['Figure 6'],\n",
       "  'img_title': 'Figure 7 An illustration of the latent search trees.'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg',\n",
       "  'img_caption': ['Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition toward more focused exploration in later stages. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 9,\n",
       "  'id': 'Figure 8',\n",
       "  'related_ids': ['picts'],\n",
       "  'img_title': 'Figure 8 Analysis of parallelism in latent tree search.'},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg',\n",
       "  'img_caption': ['Figure 9 The correlation between prediction probability of concepts and their heights. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 10,\n",
       "  'id': 'Figure 9',\n",
       "  'related_ids': [],\n",
       "  'img_title': 'Figure 9 The correlation between prediction probability of concepts and their heights.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_1 = \"\"\"![](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg)  \n",
    "Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.  \n",
    "\"\"\"\n",
    "\n",
    "line_2 = \"![](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg)  \"\n",
    "\n",
    "ptrn = r\"!\\[(.*?)\\]\\((.*?)\\s*(\\\"(.*?)\\\")?\\)\"\n",
    "match = re.match(ptrn, line_1)\n",
    "\n",
    "if match:\n",
    "    alt_text = match[0]\n",
    "    image_url = match[1]\n",
    "    title = match[3] if match[3] else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(ptrn, \"sdfasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('',\n",
       " 'images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg',\n",
       " '',\n",
       " '')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def update_image_info(line, img_lst):\n",
    "    ptrn = r\"!\\[(.*?)\\]\\((.*?)\\s*(\\\"(.*?)\\\")?\\)\"\n",
    "    matches = list(re.finditer(ptrn, line))  # 使用 finditer 获取所有匹配项\n",
    "\n",
    "    if matches:\n",
    "        new_line = line  # 初始化 new_line\n",
    "        for match in reversed(matches):  # 逆序遍历匹配项，避免替换位置错乱\n",
    "            alt_text = match.group(1).strip()\n",
    "            image_url = match.group(2)\n",
    "            title = match.group(4).strip() if match.group(4) else None\n",
    "\n",
    "            for item in img_lst:\n",
    "                if item.get('img_path') == image_url:\n",
    "                    alt_text = \"\\n\".join(item.get('img_caption', [])) + \"\\n\".join(item.get('img_footnote', [])) if alt_text is None or alt_text == \"\" else alt_text\n",
    "                    title = item.get('img_title', \"\") if title is None or title == \"\" else title\n",
    "                    title = f\"{item.get('id')}: {title}\" if item.get('id') not in title else title\n",
    "                    img_md = f\"![{alt_text.strip()}]({image_url.strip()} '{title.strip()}')\"\n",
    "\n",
    "                    # 计算替换的起始和结束位置\n",
    "                    start, end = match.span()\n",
    "                    new_line = new_line[:start] + img_md + new_line[end:]  # 精确替换\n",
    "                    break  # 找到匹配的 item 后跳出循环\n",
    "        return new_line.strip()\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_info(line, img_lst):\n",
    "    ptrn = r\"!\\[(.*?)\\]\\((.*?)\\s*(\\\"(.*?)\\\")?\\)\"\n",
    "    matches = list(re.finditer(ptrn, line))  # 使用 finditer 获取所有匹配项\n",
    "\n",
    "    if matches:\n",
    "        new_line = line  # 初始化 new_line\n",
    "        for match in reversed(matches):  # 逆序遍历匹配项，避免替换位置错乱\n",
    "            alt_text = match.group(1).strip()\n",
    "            image_url = match.group(2)\n",
    "            title = match.group(4).strip() if match.group(4) else None\n",
    "\n",
    "            for item in img_lst:\n",
    "                if item.get('img_path') == image_url:\n",
    "                    alt_text = \"\\n\".join(item.get('img_caption', [])) + \"\\n\".join(item.get('img_footnote', [])) if alt_text is None or alt_text == \"\" else alt_text\n",
    "                    title = item.get('img_title', \"\") if title is None or title == \"\" else title\n",
    "                    title = f\"{item.get('id')}: {title}\" if item.get('id') not in title else title\n",
    "                    img_md = f\"![{alt_text.strip()}]({image_url.strip()} '{title.strip()}')\"\n",
    "\n",
    "                    # 计算替换的起始和结束位置\n",
    "                    start, end = match.span()\n",
    "                    new_line = new_line[:start] + img_md + new_line[end:]  # 精确替换\n",
    "                    break  # 找到匹配的 item 后跳出循环\n",
    "        return new_line.strip()\n",
    "    return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def markdown_table_to_html(markdown_text):\n",
    "    \"\"\"\n",
    "    将 Markdown 文本中的 Markdown 表格转换为 HTML 表格。\n",
    "\n",
    "    Args:\n",
    "        markdown_text: 包含 Markdown 表格的 Markdown 文本。\n",
    "\n",
    "    Returns:\n",
    "        转换后的 Markdown 文本，表格部分已转换为 HTML 表格。\n",
    "    \"\"\"\n",
    "\n",
    "    lines = markdown_text.splitlines()\n",
    "    output_lines = []\n",
    "    in_table = False\n",
    "    table_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().startswith('|'):\n",
    "            in_table = True\n",
    "            table_lines.append(line)\n",
    "        else:\n",
    "            if in_table:\n",
    "                # 表格结束，处理之前收集的表格行\n",
    "                html_table = _convert_table_lines_to_html(table_lines)\n",
    "                output_lines.append(html_table)\n",
    "                in_table = False\n",
    "                table_lines = []\n",
    "            output_lines.append(line)\n",
    "\n",
    "    # 处理文本末尾可能存在的表格\n",
    "    if in_table:\n",
    "        html_table = _convert_table_lines_to_html(table_lines)\n",
    "        output_lines.append(html_table)\n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "def _convert_table_lines_to_html(table_lines):\n",
    "    \"\"\"\n",
    "    将 Markdown 表格行转换为 HTML 表格。\n",
    "\n",
    "    Args:\n",
    "        table_lines: Markdown 表格行的列表。\n",
    "\n",
    "    Returns:\n",
    "        HTML 表格字符串。\n",
    "    \"\"\"\n",
    "    html_lines = [\"<table>\", \"  <thead>\", \"    <tr>\"]\n",
    "    header_cells = [cell.strip() for cell in table_lines[0].strip('|').split('|')]\n",
    "    for header in header_cells:\n",
    "        html_lines.append(f\"      <th>{header}</th>\")\n",
    "    html_lines.append(\"    </tr>\")\n",
    "    html_lines.append(\"  </thead>\")\n",
    "    html_lines.append(\"  <tbody>\")\n",
    "\n",
    "    if len(table_lines) > 1 and re.match(r'^\\|[-:| ]+\\|[-:| ]*$', table_lines[1].strip()):\n",
    "        # 存在分隔行，跳过分隔行，从第三行开始是数据行\n",
    "        data_start_index = 2\n",
    "    else:\n",
    "        data_start_index = 1 # 没有分隔行，从第二行开始是数据行\n",
    "\n",
    "    for i in range(data_start_index, len(table_lines)):\n",
    "        html_lines.append(\"    <tr>\")\n",
    "        data_cells = [cell.strip() for cell in table_lines[i].strip('|').split('|')]\n",
    "        for cell in data_cells:\n",
    "            html_lines.append(f\"      <td>{cell}</td>\")\n",
    "        html_lines.append(\"    </tr>\")\n",
    "\n",
    "    html_lines.append(\"  </tbody>\")\n",
    "    html_lines.append(\"</table>\")\n",
    "    return \"\\n\".join(html_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def reorg_section_content(md_content, json_content, level):\n",
    "    title_pattern = re.compile(rf\"^#{{{level}}}\\s+(.+)$\", re.MULTILINE)\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    paragraphs = []\n",
    "    current_section = \"\"\n",
    "    current_title = \"\"\n",
    "\n",
    "    section_num = 1  # Initialize section number\n",
    "    para_id = 1  # initialize pragraph number\n",
    "\n",
    "    for line in md_content.splitlines():\n",
    "        if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"]:\n",
    "            match = title_pattern.match(line)\n",
    "            if match:\n",
    "                if current_section:  # Save the previous section\n",
    "                    sections.append({\n",
    "                        'level': level,\n",
    "                        'section_num': section_num,\n",
    "                        'title': current_title,\n",
    "                        'text': current_section.strip(),  # Remove leading/trailing whitespace\n",
    "                        'paragraphs': paragraphs\n",
    "                    })\n",
    "                    section_num += 1  # Increment for the next section\n",
    "                \n",
    "                # ready for next section\n",
    "                current_title = match.group(1).strip()\n",
    "                current_section = \"\"  # Start a new section (no title line)\n",
    "                paragraphs = []\n",
    "                para_id = 1\n",
    "            else:\n",
    "                current_section += line + \"\\n\"  # Add to the current section\n",
    "                paragraphs.append({'paragraph_id':f\"paragraph_{para_id}\", 'md_content': line})\n",
    "                para_id += 1\n",
    "\n",
    "    if current_section:  # Save the last section\n",
    "        sections.append({\n",
    "            'level': level,\n",
    "            'section_num': section_num,\n",
    "            'title': current_title,\n",
    "            'text': current_section.strip(),\n",
    "            'paragraphs': paragraphs\n",
    "        })\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = \"./tmp/2412.06769v2/full.md\"\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "sections = reorg_section_content(md_content=markdown_content, json_content=None, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level': 1,\n",
       "  'section_num': 1,\n",
       "  'title': 'Training Large Language Models to Reason in a Continuous Latent Space',\n",
       "  'text': 'Shibo Hao $^{1,2,*}$ , Sainbayar Sukhbaatar1, DiJia $\\\\mathtt{s u}^{1}$ , Xian Li1, Zhiting ${\\\\mathsf{H}}{\\\\mathsf{u}}^{2}$ , Jason Weston1, Yuandong Tian1   \\n1FAIR at Meta, $^2$ UC San Diego   \\n∗Work done at Meta  \\nLarge language models (LLMs) are restricted to reason in the “language space”, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed “continuous thought”). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can efectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-frst search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These fndings demonstrate the promise of latent reasoning and ofer valuable insights for future research.  \\nDate: December 12, 2024  \\n$\\\\infty$ Meta',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Shibo Hao $^{1,2,*}$ , Sainbayar Sukhbaatar1, DiJia $\\\\mathtt{s u}^{1}$ , Xian Li1, Zhiting ${\\\\mathsf{H}}{\\\\mathsf{u}}^{2}$ , Jason Weston1, Yuandong Tian1   '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': '1FAIR at Meta, $^2$ UC San Diego   '},\n",
       "   {'paragraph_id': 'paragraph_3', 'md_content': '∗Work done at Meta  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': 'Large language models (LLMs) are restricted to reason in the “language space”, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed “continuous thought”). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can efectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-frst search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These fndings demonstrate the promise of latent reasoning and ofer valuable insights for future research.  '},\n",
       "   {'paragraph_id': 'paragraph_5', 'md_content': 'Date: December 12, 2024  '},\n",
       "   {'paragraph_id': 'paragraph_6', 'md_content': '$\\\\infty$ Meta  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 2,\n",
       "  'title': '1 Introduction',\n",
       "  'text': 'Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an efective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach, known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to generate solutions step-by-step using natural language. However, this is in stark contrast to certain human cognition results. Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko et al., 2024).  \\nA signifcant issue arises when LLMs use language for reasoning: the amount of reasoning required for each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fuency, contributing little to the actual reasoning process. On the contrary, some critical tokens require complex planning and pose huge challenges to LLMs. While previous work has attempted to fx these problems by prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain constrained within the language space and do not solve the fundamental problems. On the contrary, it would be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their fndings into language only when necessary.  \\n![](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg)  \\nFigure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.  \\nIn this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut (Chain of Continuous Thought). It involves a simple modifcation to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token (Figure 1). This modifcation frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully diferentiable. To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024), which efectively utilizes language reasoning chains to guide the training process.  \\nInterestingly, our proposed paradigm leads to an efcient reasoning pattern. Unlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-frst search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).  \\nExperimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning (GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be benefcial to reasoning accuracy, mirroring the efects of language reasoning chains. This indicates the potential to scale and solve increasingly challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut and some of its variants even surpasses language-based CoT methods, while generating signifcantly fewer tokens during inference. We believe that these fndings underscore the potential of latent reasoning and could provide valuable insights for future research.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an efective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach, known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to generate solutions step-by-step using natural language. However, this is in stark contrast to certain human cognition results. Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko et al., 2024).  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'A signifcant issue arises when LLMs use language for reasoning: the amount of reasoning required for each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fuency, contributing little to the actual reasoning process. On the contrary, some critical tokens require complex planning and pose huge challenges to LLMs. While previous work has attempted to fx these problems by prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain constrained within the language space and do not solve the fundamental problems. On the contrary, it would be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their fndings into language only when necessary.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': '![](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': 'Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'In this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut (Chain of Continuous Thought). It involves a simple modifcation to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token (Figure 1). This modifcation frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully diferentiable. To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024), which efectively utilizes language reasoning chains to guide the training process.  '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'Interestingly, our proposed paradigm leads to an efcient reasoning pattern. Unlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-frst search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).  '},\n",
       "   {'paragraph_id': 'paragraph_7',\n",
       "    'md_content': 'Experimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning (GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be benefcial to reasoning accuracy, mirroring the efects of language reasoning chains. This indicates the potential to scale and solve increasingly challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut and some of its variants even surpasses language-based CoT methods, while generating signifcantly fewer tokens during inference. We believe that these fndings underscore the potential of latent reasoning and could provide valuable insights for future research.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 3,\n",
       "  'title': '2 Related Work',\n",
       "  'text': 'Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate an intermediate reasoning process in language before outputting the fnal answer. This includes prompting LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains, either with supervised fnetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classifed the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By employing CoT, the efective depth of the transformer increases because the generated outputs are looped back to the input (Feng et al., 2023). These analyses, combined with the established efectiveness of CoT, motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While CoT has proven efective for certain tasks, its autoregressive generation nature makes it challenging to mimic human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al., 2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi et al., 2024; Su et al., 2024). In our analysis, we fnd that after removing the constraint of a language space, a new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.  \\nLatent reasoning in LLMs. Previous works mostly defne latent reasoning in LLMs as the hidden computation in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) further proposed to intervene the latent reasoning by “back-patching” the hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a diferent latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable <pause> tokens to the training corpus. This improves LLM’s performance on a variety of tasks, especially when followed by supervised fnetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further explored the usage of fller tokens, e.g., “...”, and concluded that they work well for highly parallelizable problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in the transformer with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms. These training methods can be combined to our framework, and specifcally, we fnd that breaking down the learning of continuous thoughts into multiple stages, inspired by iCoT (Deng et al., 2024), is very benefcial for the training. Recently, looped transformers (Giannou et al., 2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating latent reasoning in comparison to language space.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate an intermediate reasoning process in language before outputting the fnal answer. This includes prompting LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains, either with supervised fnetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classifed the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By employing CoT, the efective depth of the transformer increases because the generated outputs are looped back to the input (Feng et al., 2023). These analyses, combined with the established efectiveness of CoT, motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While CoT has proven efective for certain tasks, its autoregressive generation nature makes it challenging to mimic human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al., 2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi et al., 2024; Su et al., 2024). In our analysis, we fnd that after removing the constraint of a language space, a new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Latent reasoning in LLMs. Previous works mostly defne latent reasoning in LLMs as the hidden computation in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) further proposed to intervene the latent reasoning by “back-patching” the hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a diferent latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable <pause> tokens to the training corpus. This improves LLM’s performance on a variety of tasks, especially when followed by supervised fnetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further explored the usage of fller tokens, e.g., “...”, and concluded that they work well for highly parallelizable problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in the transformer with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms. These training methods can be combined to our framework, and specifcally, we fnd that breaking down the learning of continuous thoughts into multiple stages, inspired by iCoT (Deng et al., 2024), is very benefcial for the training. Recently, looped transformers (Giannou et al., 2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating latent reasoning in comparison to language space.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 4,\n",
       "  'title': '3 Coconut: Chain of Continuous Thought',\n",
       "  'text': 'In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in an unconstrained latent space. We begin by introducing the background and notation we use for language models. For an input sequence $\\\\boldsymbol{x}=(x_{1},...,x_{T})$ , the standard large language model $\\\\mathcal{M}$ can be described as:  \\n$$\\n\\\\begin{array}{c}{H_{t}=\\\\operatorname{Transformer}(E_{t})}\\\\\\\\ {\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\le t})=\\\\operatorname{softmax}(W h_{t})}\\\\end{array}\\n$$  \\nwhere $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{t})]$ is the sequence of token embeddings up to position $t$ ; $H_{t}\\\\in\\\\mathbb{R}^{t\\\\times d}$ is the matrix of the last hidden states for all tokens up to position $t$ ; $h_{t}$ is the last hidden state of position $t$ , i.e., $h_{t}=H_{t}[t,:];\\\\,e(\\\\cdot)$ is the token embedding function; $W$ is the parameter of the language model head.  \\nMethod Overview. In the proposed Coconut method, the LLM switches between the “language mode” and “latent mode” (Figure 1). In language mode, the model operates as a standard language model, autoregressively generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding. This last hidden state represents the current reasoning state, termed as a “continuous thought”.  \\nSpecial tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought mode, respectively. As an example, we assume latent reasoning occurs between positions $i$ and $j$ , i.e., $x_{i}=$ $<b\\\\cot>$ and $x_{j}=<\\\\tt e o t>$ . When the model is in the latent mode $(i<t<j$ ), we use the last hidden state from the previous token to replace the input embedding, i.e., $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{t-1}]$  \\n![](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg)  \\nFigure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.  \\nAfter the latent mode fnishes $(t\\\\ \\\\geq\\\\ j)$ , the input reverts to using the token embedding, i.e., $E_{t}~=$ $[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{j-1},e(x_{j}),...,e(x_{t})]$ . It is worth noting that the last hidden states have been processed by the fnal normalization layer, so they are not too large in magnitude. $\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\leq t})$ is not defned when $i<t<j$ , since the latent thought is not intended to be mapped back to language space. However, softmax $\\\\left(W h_{t}\\\\right)$ can still be calculated for probing purposes (see Section 4).  \\nTraining Procedure. In this work, we focus on a problem-solving setting where the model receives a question as input and is expected to generate an answer through a reasoning process. We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al. (2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the subsequent stages, at the $k$ -th stage, the frst $k$ reasoning steps in the CoT are replaced with $k\\\\times c$ continuous thoughts1, where $c$ is a hyperparameter controlling the number of latent thoughts replacing a single language reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We insert <bot> and <eot> tokens (which are not counted towards $c$ ) to encapsulate the continuous thoughts.  \\nDuring the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions and latent thoughts. It is important to note that the objective does not encourage the continuous thought to compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore, it’s possible for the LLM to learn more efective representations of reasoning steps compared to human language.  \\nTraining Details. Our proposed continuous thoughts are fully diferentiable and allow for back-propagation. We perform $n+1$ forward passes when $n$ latent thoughts are scheduled in the current training stage, computing a new latent thought with each pass and fnally conducting an additional forward pass to obtain a loss on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the training efciency of Coconut remains an important direction for future research.  \\nInference Process. The inference process for Coconut is analogous to standard language model decoding, except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge lies in determining when to switch between latent and language modes. As we focus on the problem-solving setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two potential strategies: a) train a binary classifer on latent thoughts to enable the model to autonomously decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We found that both approaches work comparably well. Therefore, we use the second option in our experiment for simplicity, unless specifed otherwise.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in an unconstrained latent space. We begin by introducing the background and notation we use for language models. For an input sequence $\\\\boldsymbol{x}=(x_{1},...,x_{T})$ , the standard large language model $\\\\mathcal{M}$ can be described as:  '},\n",
       "   {'paragraph_id': 'paragraph_2', 'md_content': '$$'},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': '\\\\begin{array}{c}{H_{t}=\\\\operatorname{Transformer}(E_{t})}\\\\\\\\ {\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\le t})=\\\\operatorname{softmax}(W h_{t})}\\\\end{array}'},\n",
       "   {'paragraph_id': 'paragraph_4', 'md_content': '$$  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'where $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{t})]$ is the sequence of token embeddings up to position $t$ ; $H_{t}\\\\in\\\\mathbb{R}^{t\\\\times d}$ is the matrix of the last hidden states for all tokens up to position $t$ ; $h_{t}$ is the last hidden state of position $t$ , i.e., $h_{t}=H_{t}[t,:];\\\\,e(\\\\cdot)$ is the token embedding function; $W$ is the parameter of the language model head.  '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'Method Overview. In the proposed Coconut method, the LLM switches between the “language mode” and “latent mode” (Figure 1). In language mode, the model operates as a standard language model, autoregressively generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding. This last hidden state represents the current reasoning state, termed as a “continuous thought”.  '},\n",
       "   {'paragraph_id': 'paragraph_7',\n",
       "    'md_content': 'Special tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought mode, respectively. As an example, we assume latent reasoning occurs between positions $i$ and $j$ , i.e., $x_{i}=$ $<b\\\\cot>$ and $x_{j}=<\\\\tt e o t>$ . When the model is in the latent mode $(i<t<j$ ), we use the last hidden state from the previous token to replace the input embedding, i.e., $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{t-1}]$  '},\n",
       "   {'paragraph_id': 'paragraph_8',\n",
       "    'md_content': '![](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_9',\n",
       "    'md_content': 'Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.  '},\n",
       "   {'paragraph_id': 'paragraph_10',\n",
       "    'md_content': 'After the latent mode fnishes $(t\\\\ \\\\geq\\\\ j)$ , the input reverts to using the token embedding, i.e., $E_{t}~=$ $[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{j-1},e(x_{j}),...,e(x_{t})]$ . It is worth noting that the last hidden states have been processed by the fnal normalization layer, so they are not too large in magnitude. $\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\leq t})$ is not defned when $i<t<j$ , since the latent thought is not intended to be mapped back to language space. However, softmax $\\\\left(W h_{t}\\\\right)$ can still be calculated for probing purposes (see Section 4).  '},\n",
       "   {'paragraph_id': 'paragraph_11',\n",
       "    'md_content': 'Training Procedure. In this work, we focus on a problem-solving setting where the model receives a question as input and is expected to generate an answer through a reasoning process. We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al. (2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the subsequent stages, at the $k$ -th stage, the frst $k$ reasoning steps in the CoT are replaced with $k\\\\times c$ continuous thoughts1, where $c$ is a hyperparameter controlling the number of latent thoughts replacing a single language reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We insert <bot> and <eot> tokens (which are not counted towards $c$ ) to encapsulate the continuous thoughts.  '},\n",
       "   {'paragraph_id': 'paragraph_12',\n",
       "    'md_content': 'During the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions and latent thoughts. It is important to note that the objective does not encourage the continuous thought to compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore, it’s possible for the LLM to learn more efective representations of reasoning steps compared to human language.  '},\n",
       "   {'paragraph_id': 'paragraph_13',\n",
       "    'md_content': 'Training Details. Our proposed continuous thoughts are fully diferentiable and allow for back-propagation. We perform $n+1$ forward passes when $n$ latent thoughts are scheduled in the current training stage, computing a new latent thought with each pass and fnally conducting an additional forward pass to obtain a loss on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the training efciency of Coconut remains an important direction for future research.  '},\n",
       "   {'paragraph_id': 'paragraph_14',\n",
       "    'md_content': 'Inference Process. The inference process for Coconut is analogous to standard language model decoding, except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge lies in determining when to switch between latent and language modes. As we focus on the problem-solving setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two potential strategies: a) train a binary classifer on latent thoughts to enable the model to autonomously decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We found that both approaches work comparably well. Therefore, we use the second option in our experiment for simplicity, unless specifed otherwise.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 5,\n",
       "  'title': '4 Experiments',\n",
       "  'text': 'We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth. The number of newly generated tokens per question is also analyzed, as a measure of reasoning efciency. We report the clock-time comparison in Appendix B.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth. The number of newly generated tokens per question is also analyzed, as a measure of reasoning efciency. We report the clock-time comparison in Appendix B.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 6,\n",
       "  'title': '4.1 Reasoning Tasks',\n",
       "  'text': 'Math Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade school-level math problems. Compared to the other datasets in our experiments, the problems are more diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng et al. (2023).  \\nLogical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and He, 2022) questions, with fctional concept names. For each problem, a tree-structured ontology is randomly generated and described in natural language as a set of known conditions. The model is asked to judge whether a given statement is correct based on these conditions. This serves as a simplifed simulation of more advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).  \\nWe found that the generation process of ProntoQA could be more challenging, especially since the size of distracting branches in the ontology is always small, reducing the need for complex planning. To fx that, we apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions. The resulting dataset requires the model to perform substantial planning and searching over the graph to fnd the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering). A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Math Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade school-level math problems. Compared to the other datasets in our experiments, the problems are more diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng et al. (2023).  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Logical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and He, 2022) questions, with fctional concept names. For each problem, a tree-structured ontology is randomly generated and described in natural language as a set of known conditions. The model is asked to judge whether a given statement is correct based on these conditions. This serves as a simplifed simulation of more advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'We found that the generation process of ProntoQA could be more challenging, especially since the size of distracting branches in the ontology is always small, reducing the need for complex planning. To fx that, we apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions. The resulting dataset requires the model to perform substantial planning and searching over the graph to fnd the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering). A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 7,\n",
       "  'title': '4.2 Experimental Setup',\n",
       "  'text': 'We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is set to $1\\\\times10^{-4}$ while the efective batch size is 128. Following Deng et al. (2024), we also reset the optimizer when the training stages switch.  \\nMath Reasoning. By default, we use 2 latent thoughts (i.e., $c=2$ ) for each reasoning step. We analyze the correlation between performance and $c$ in Section 4.4. The model goes through 3 stages besides the initial stage. Then, we have an additional stage, where we still use $3\\\\times c$ continuous thoughts as in the penultimate stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in each remaining stage.  \\nLogical Reasoning. We use one continuous thought for every reasoning step (i.e., $c=1$ ). The model goes through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is 6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the last stage. We train the model for 5 epochs per stage.  \\nFor all datasets, after the standard schedule, the model stays in the fnal training stage, until the 50th epoch. We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the number of continuous thoughts to be consistent with their fnal training stage. We use greedy decoding for all experiments.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is set to $1\\\\times10^{-4}$ while the efective batch size is 128. Following Deng et al. (2024), we also reset the optimizer when the training stages switch.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Math Reasoning. By default, we use 2 latent thoughts (i.e., $c=2$ ) for each reasoning step. We analyze the correlation between performance and $c$ in Section 4.4. The model goes through 3 stages besides the initial stage. Then, we have an additional stage, where we still use $3\\\\times c$ continuous thoughts as in the penultimate stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in each remaining stage.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'Logical Reasoning. We use one continuous thought for every reasoning step (i.e., $c=1$ ). The model goes through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is 6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the last stage. We train the model for 5 epochs per stage.  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': 'For all datasets, after the standard schedule, the model stays in the fnal training stage, until the 50th epoch. We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the number of continuous thoughts to be consistent with their fnal training stage. We use greedy decoding for all experiments.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 8,\n",
       "  'title': '4.3 Baselines and Variants of Coconut',\n",
       "  'text': 'We consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model with supervised fnetuning, and during inference, the model generates a reasoning chain before outputting an answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain. (3) $i C o T$ (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully designed schedule that “internalizes” CoT. As the training goes on, tokens at the beginning of the reasoning chain are gradually removed until only the answer remains. During inference, the model directly predicts the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer, without a reasoning chain. However, diferent from No-CoT, special <pause> tokens are inserted between the question and answer, which are believed to provide the model with additional computational capacity to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous thoughts in Coconut.  \\nTable 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. ∗The result is from Deng et al. (2024).   \\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 ±0.2</td><td>25.0</td><td>98.8 ±0.8</td><td>92.5</td><td>77.5 ±1.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 ±0.5</td><td>2.2</td><td>93.8 ±0.7</td><td>3.0</td><td>76.7 ±1.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 ±0.3</td><td>3.0</td><td>98.2 ±0.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 ±1.8</td><td>2.2</td><td>77.7: ±21.0</td><td>3.0</td><td>75.9 ±0.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 ±1.5</td><td>8.2</td><td>99.8 ±0.2</td><td>9.0</td><td>97.0 ±0.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 ±0.8</td><td>8.2</td><td>52.4 ±0.4</td><td>9.0</td><td>76.1 ±0.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 ±0.5</td><td>2.3</td><td>99.9 ±0.1</td><td>3.0</td><td>95.5 ±1.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 ±0.7</td><td>2.2</td><td>100.0 ±0.1</td><td>3.0</td><td>96.6 ±0.8</td><td>8.2</td></tr></table></body></html>  \\nWe also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training, we directly use the data from the last stage which only includes questions and answers to train Coconut. The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage training which removes language reasoning steps gradually, but don’t use any continuous latent thoughts. While this is similar to $i C o\\\\,T$ in the high-level idea, the exact training schedule is set to be consistent with Coconut, instead of $i C o T$ . This ensures a more strict comparison. (3) Pause as thought: We use special <pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as Coconut.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'We consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model with supervised fnetuning, and during inference, the model generates a reasoning chain before outputting an answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain. (3) $i C o T$ (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully designed schedule that “internalizes” CoT. As the training goes on, tokens at the beginning of the reasoning chain are gradually removed until only the answer remains. During inference, the model directly predicts the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer, without a reasoning chain. However, diferent from No-CoT, special <pause> tokens are inserted between the question and answer, which are believed to provide the model with additional computational capacity to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous thoughts in Coconut.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. ∗The result is from Deng et al. (2024).   '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': '<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 ±0.2</td><td>25.0</td><td>98.8 ±0.8</td><td>92.5</td><td>77.5 ±1.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 ±0.5</td><td>2.2</td><td>93.8 ±0.7</td><td>3.0</td><td>76.7 ±1.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 ±0.3</td><td>3.0</td><td>98.2 ±0.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 ±1.8</td><td>2.2</td><td>77.7: ±21.0</td><td>3.0</td><td>75.9 ±0.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 ±1.5</td><td>8.2</td><td>99.8 ±0.2</td><td>9.0</td><td>97.0 ±0.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 ±0.8</td><td>8.2</td><td>52.4 ±0.4</td><td>9.0</td><td>76.1 ±0.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 ±0.5</td><td>2.3</td><td>99.9 ±0.1</td><td>3.0</td><td>95.5 ±1.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 ±0.7</td><td>2.2</td><td>100.0 ±0.1</td><td>3.0</td><td>96.6 ±0.8</td><td>8.2</td></tr></table></body></html>  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': 'We also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training, we directly use the data from the last stage which only includes questions and answers to train Coconut. The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage training which removes language reasoning steps gradually, but don’t use any continuous latent thoughts. While this is similar to $i C o\\\\,T$ in the high-level idea, the exact training schedule is set to be consistent with Coconut, instead of $i C o T$ . This ensures a more strict comparison. (3) Pause as thought: We use special <pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as Coconut.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 9,\n",
       "  'title': '4.4 Results and Discussion',\n",
       "  'text': 'We show the overall results on all datasets in Table 1. Continuous thoughts efectively enhance LLM reasoning, as shown by the consistent improvement over $n o{-}C o T$ . It even shows better performance than $\\\\mathit{C o T}$ on ProntoQA and ProsQA. We describe several key conclusions from the experiments as follows.  \\n“Chaining” continuous thoughts enhances reasoning. In conventional CoT, the output token serves as the next input, which proves to increase the efective depth of LLMs and enhance their expressiveness (Feng et al., 2023). We explore whether latent space reasoning retains this property, as it would suggest that this method could scale to solve increasingly complex problems by chaining multiple latent thoughts.  \\nIn our experiments with GSM8k, we found that Coconut outperformed other architectures trained with similar strategies, particularly surpassing the latest baseline, $i C o T$ (Deng et al., 2024). The performance is signifcantly better than Coconut (pause as thought) which also enables more computation in the LLMs. While Pfau et al. (2024) empirically shows that fller tokens, such as the special <pause> tokens, can beneft highly parallelizable problems, our results show that Coconut architecture is more efective for general problems, e.g., math word problems, where a reasoning step often heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter $c$ , which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As we increased $c$ from 0 to $^{1}$ to 2, the model’s performance steadily improved.2 These results suggest that a chaining efect similar to CoT can be observed in the latent space.  \\n![](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg)  \\nFigure 3 Accuracy on GSM8k with diferent number of continuous thoughts.  \\nIn two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and the $i C o T$ baseline also achieve impressive accuracy. This indicates that the model’s computational capacity may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering task, likely involves more complex contextual understanding and modeling, placing higher demands on computational capability.  \\nLatent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires the model to “look ahead” and evaluate the appropriateness of each step. Among our datasets, GSM8k and ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and limited branching. In contrast, ProsQA’s randomly generated DAG structure signifcantly challenges the model’s planning capabilities. As shown in Table 1, $C o T$ does not ofer notable improvement over No-CoT. However, Coconut, its variants, and $i C o T$ substantially enhance reasoning on ProsQA, indicating that latent space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of this process is provided in Section 5.  \\nThe LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most efective continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut $w/o$ curriculum). However, from the experimental results, we found the models trained this way do not perform any better than no-CoT.  \\nWith the multi-stage curriculum which decomposes the training into easier objectives, Coconut is able to achieve top performance across various tasks. The multi-stage training also integrates well with pause tokens (Coconut- pause as thought). Despite using the same architecture and similar multi-stage training objectives, we observed a small gap between the performance of $i C o T$ and Coconut (w/o thoughts). The fner-grained removal schedule (token by token) and a few other tricks in $i C o T$ may ease the training process. We leave combining $i C o T$ and Coconut as future work. While the multi-stage training used for Coconut has proven efective, further research is defnitely needed to develop better and more general strategies for learning reasoning in latent space, especially without the supervision from language reasoning chains.  \\n![](images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg)  \\nFigure 4 A case study where we decode the continuous thought into language tokens.  \\nContinuous thoughts are efficient representations of reasoning. Though the continuous thoughts are not intended to be decoded to language tokens, we can  \\nstill use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of a math word problem solved by Coconut $\\\\mathit{\\\\Pi}_{c}=1\\\\;\\\\;$ ). The frst continuous thought can be decoded into tokens like “ $\\\\mathrm{\\\\Omega}180^{\\\\circ}$ , “ $180^{\\\\circ}$ (with a space), and “9”. Note that, the reasoning trace for this problem should be $3\\\\times3\\\\times60=9\\\\times60=540$ , or $3\\\\times3\\\\times60=3\\\\times180=540$ . The interpretations of the frst thought happen to be the frst intermediate variables in the calculation. Moreover, it encodes a distribution of diferent traces into the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for planning-intense reasoning tasks.  \\n![](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg)  \\nFigure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'We show the overall results on all datasets in Table 1. Continuous thoughts efectively enhance LLM reasoning, as shown by the consistent improvement over $n o{-}C o T$ . It even shows better performance than $\\\\mathit{C o T}$ on ProntoQA and ProsQA. We describe several key conclusions from the experiments as follows.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': '“Chaining” continuous thoughts enhances reasoning. In conventional CoT, the output token serves as the next input, which proves to increase the efective depth of LLMs and enhance their expressiveness (Feng et al., 2023). We explore whether latent space reasoning retains this property, as it would suggest that this method could scale to solve increasingly complex problems by chaining multiple latent thoughts.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'In our experiments with GSM8k, we found that Coconut outperformed other architectures trained with similar strategies, particularly surpassing the latest baseline, $i C o T$ (Deng et al., 2024). The performance is signifcantly better than Coconut (pause as thought) which also enables more computation in the LLMs. While Pfau et al. (2024) empirically shows that fller tokens, such as the special <pause> tokens, can beneft highly parallelizable problems, our results show that Coconut architecture is more efective for general problems, e.g., math word problems, where a reasoning step often heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter $c$ , which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As we increased $c$ from 0 to $^{1}$ to 2, the model’s performance steadily improved.2 These results suggest that a chaining efect similar to CoT can be observed in the latent space.  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': '![](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts.  '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'In two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and the $i C o T$ baseline also achieve impressive accuracy. This indicates that the model’s computational capacity may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering task, likely involves more complex contextual understanding and modeling, placing higher demands on computational capability.  '},\n",
       "   {'paragraph_id': 'paragraph_7',\n",
       "    'md_content': 'Latent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires the model to “look ahead” and evaluate the appropriateness of each step. Among our datasets, GSM8k and ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and limited branching. In contrast, ProsQA’s randomly generated DAG structure signifcantly challenges the model’s planning capabilities. As shown in Table 1, $C o T$ does not ofer notable improvement over No-CoT. However, Coconut, its variants, and $i C o T$ substantially enhance reasoning on ProsQA, indicating that latent space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of this process is provided in Section 5.  '},\n",
       "   {'paragraph_id': 'paragraph_8',\n",
       "    'md_content': 'The LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most efective continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut $w/o$ curriculum). However, from the experimental results, we found the models trained this way do not perform any better than no-CoT.  '},\n",
       "   {'paragraph_id': 'paragraph_9',\n",
       "    'md_content': 'With the multi-stage curriculum which decomposes the training into easier objectives, Coconut is able to achieve top performance across various tasks. The multi-stage training also integrates well with pause tokens (Coconut- pause as thought). Despite using the same architecture and similar multi-stage training objectives, we observed a small gap between the performance of $i C o T$ and Coconut (w/o thoughts). The fner-grained removal schedule (token by token) and a few other tricks in $i C o T$ may ease the training process. We leave combining $i C o T$ and Coconut as future work. While the multi-stage training used for Coconut has proven efective, further research is defnitely needed to develop better and more general strategies for learning reasoning in latent space, especially without the supervision from language reasoning chains.  '},\n",
       "   {'paragraph_id': 'paragraph_10',\n",
       "    'md_content': '![](images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_11',\n",
       "    'md_content': 'Figure 4 A case study where we decode the continuous thought into language tokens.  '},\n",
       "   {'paragraph_id': 'paragraph_12',\n",
       "    'md_content': 'Continuous thoughts are efficient representations of reasoning. Though the continuous thoughts are not intended to be decoded to language tokens, we can  '},\n",
       "   {'paragraph_id': 'paragraph_13',\n",
       "    'md_content': 'still use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of a math word problem solved by Coconut $\\\\mathit{\\\\Pi}_{c}=1\\\\;\\\\;$ ). The frst continuous thought can be decoded into tokens like “ $\\\\mathrm{\\\\Omega}180^{\\\\circ}$ , “ $180^{\\\\circ}$ (with a space), and “9”. Note that, the reasoning trace for this problem should be $3\\\\times3\\\\times60=9\\\\times60=540$ , or $3\\\\times3\\\\times60=3\\\\times180=540$ . The interpretations of the frst thought happen to be the frst intermediate variables in the calculation. Moreover, it encodes a distribution of diferent traces into the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for planning-intense reasoning tasks.  '},\n",
       "   {'paragraph_id': 'paragraph_14',\n",
       "    'md_content': '![](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_15',\n",
       "    'md_content': 'Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 10,\n",
       "  'title': '5 Understanding the Latent Reasoning in Coconut',\n",
       "  'text': 'In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By leveraging its ability to switch between language and latent space reasoning, we are able to control the model to interpolate between fully latent reasoning and fully language reasoning and test their performance (Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3). Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By leveraging its ability to switch between language and latent space reasoning, we are able to control the model to interpolate between fully latent reasoning and fully language reasoning and test their performance (Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3). Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 11,\n",
       "  'title': '5.1 Experimental Setup',\n",
       "  'text': 'Methods. The design of Coconut allows us to control the number of latent thoughts by manually setting the position of the $<\\\\tt e o t>$ token during inference. When we enforce Coconut to use $k$ continuous thoughts, the model is expected to output the remaining reasoning chain in language, starting from the $k+1$ step. In our experiments, we test variants of Coconut on ProsQA with $k\\\\in\\\\{0,1,2,3,4,5,6\\\\}$ . Note that all these variants only difer in inference time while sharing the same model weights. Besides, we report the performance of $\\\\mathit{C o T}$ and no-CoT as references.  \\nTo address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum by always mixing data from other stages with a certain probability $p=0.3$ ). This updated training curriculum yields similar performance and enables efective control over the switch between latent and language reasoning.  \\nMetrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the fnal answer, regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4). To enable fne-grained analysis, we defne another metric on the reasoning process. Assuming we have a complete language reasoning chain which specifes a path in the graph, we can classify it into (1) Correct Path: The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one being asked. These four categories naturally apply to the output from Coconut ( $k=0$ ) and $C o T$ , which generate the full path. For Coconut with $k>0$ that outputs only partial paths in language (with the initial steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete it. Also, we defne Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes the path, it’s classifed as hallucination. In no-CoT and Coconut with larger $k$ , the model may only output the fnal answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six categories cover all cases without overlap.  \\n![](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg)  \\nFigure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\\\scriptstyle\\\\mathrm{k=2}$ ) solves the problem correctly.  \\n![](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg)  \\nFigure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., “lempus” in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Methods. The design of Coconut allows us to control the number of latent thoughts by manually setting the position of the $<\\\\tt e o t>$ token during inference. When we enforce Coconut to use $k$ continuous thoughts, the model is expected to output the remaining reasoning chain in language, starting from the $k+1$ step. In our experiments, we test variants of Coconut on ProsQA with $k\\\\in\\\\{0,1,2,3,4,5,6\\\\}$ . Note that all these variants only difer in inference time while sharing the same model weights. Besides, we report the performance of $\\\\mathit{C o T}$ and no-CoT as references.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'To address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum by always mixing data from other stages with a certain probability $p=0.3$ ). This updated training curriculum yields similar performance and enables efective control over the switch between latent and language reasoning.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'Metrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the fnal answer, regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4). To enable fne-grained analysis, we defne another metric on the reasoning process. Assuming we have a complete language reasoning chain which specifes a path in the graph, we can classify it into (1) Correct Path: The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one being asked. These four categories naturally apply to the output from Coconut ( $k=0$ ) and $C o T$ , which generate the full path. For Coconut with $k>0$ that outputs only partial paths in language (with the initial steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete it. Also, we defne Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes the path, it’s classifed as hallucination. In no-CoT and Coconut with larger $k$ , the model may only output the fnal answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six categories cover all cases without overlap.  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': '![](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\\\scriptstyle\\\\mathrm{k=2}$ ) solves the problem correctly.  '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': '![](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_7',\n",
       "    'md_content': 'Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., “lempus” in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 12,\n",
       "  'title': '5.2 Interpolating between Latent and Language Reasoning',\n",
       "  'text': 'Figure 5 shows a comparative analysis of diferent reasoning methods on ProsQA. As more reasoning is done with continuous thoughts (increasing $k$ ), both fnal answer accuracy (Figure 5, left) and the rate of correct reasoning processes (“Correct Label” and “Correct Path” in Figure 5, right) improve. Additionally, the rate of “Hallucination” and “Wrong Target” decrease, which typically occur when the model makes a wrong move earlier. This also indicates the better planning ability when more reasoning happens in the latent space.  \\nA case study is shown in Figure 6, where $\\\\mathit{C o T}$ hallucinates an nonexistent edge, Coconut ( $k=1$ ) leads to a wrong target, but Coconut ( $k=2$ ) successfully solves the problem. In this example, the model cannot accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in Section 5.3.  \\nThe comparison between $\\\\mathit{C o T}$ and Coconut ( $k=0$ ) reveals another interesting observation: even when  \\n![](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg)  \\nFigure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition toward more focused exploration in later stages.  \\nCoconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than $\\\\mathit{C o T}$ . The generated reasoning paths are also more accurate with less hallucination. From this, we can infer that the training method of mixing diferent stages improves the model’s ability to plan ahead. The training objective of $\\\\mathit{C o T}$ always concentrates on the generation of the immediate next step, making the model “shortsighted”. In later stages of Coconut training, the frst few steps are hidden, allowing the model to focus more on future steps. This is related to the fndings of Gloeckle et al. (2024), where they propose multi-token prediction as a new pretraining objective to improve the LLM’s ability to plan ahead.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Figure 5 shows a comparative analysis of diferent reasoning methods on ProsQA. As more reasoning is done with continuous thoughts (increasing $k$ ), both fnal answer accuracy (Figure 5, left) and the rate of correct reasoning processes (“Correct Label” and “Correct Path” in Figure 5, right) improve. Additionally, the rate of “Hallucination” and “Wrong Target” decrease, which typically occur when the model makes a wrong move earlier. This also indicates the better planning ability when more reasoning happens in the latent space.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'A case study is shown in Figure 6, where $\\\\mathit{C o T}$ hallucinates an nonexistent edge, Coconut ( $k=1$ ) leads to a wrong target, but Coconut ( $k=2$ ) successfully solves the problem. In this example, the model cannot accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in Section 5.3.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'The comparison between $\\\\mathit{C o T}$ and Coconut ( $k=0$ ) reveals another interesting observation: even when  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': '![](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition toward more focused exploration in later stages.  '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'Coconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than $\\\\mathit{C o T}$ . The generated reasoning paths are also more accurate with less hallucination. From this, we can infer that the training method of mixing diferent stages improves the model’s ability to plan ahead. The training objective of $\\\\mathit{C o T}$ always concentrates on the generation of the immediate next step, making the model “shortsighted”. In later stages of Coconut training, the frst few steps are hidden, allowing the model to focus more on future steps. This is related to the fndings of Gloeckle et al. (2024), where they propose multi-token prediction as a new pretraining objective to improve the LLM’s ability to plan ahead.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 13,\n",
       "  'title': '5.3 Interpreting the Latent Search Tree',\n",
       "  'text': 'Given the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning can be interpreted as a search tree, rather than merely a reasoning “chain”. Taking the case of Figure 6 as a concrete example, the frst step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus, grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the frontier nodes will be the grandchildren of Alex (Figure 7, right).  \\nUnlike a standard breadth-frst search (BFS), which explores all frontier nodes uniformly, the model demonstrates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model’s preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to switch back to language space after a single latent thought ( $k=1$ ), it predicts the next step in a structured format, such as “every [Concept A] is a [Concept B].” By examining the probability distribution over potential fllers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left). Similarly, when $k=2$ , the prediction probabilities for all frontier nodes—the grandchildren of Alex —are obtained (Figure 7, right).  \\nThe probability distribution can be viewed as the model’s implicit value function, estimating each node’s potential to reach the target. As shown in the fgure, “lempus”, “zhorpus”, “grimpus”, and “sterpus” have a value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the frst continuous thought, the model has mostly ruled out “sterpus” as an option but remains uncertain about the correct choice among the other three. In the second thought, however, the model has mostly ruled out other options but focused on “rorpus”.  \\nFigure 8 presents an analysis of the parallelism in the model’s latent reasoning across the frst and second thoughts. For the frst thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps between the three lines indicate that the model maintains signifcant diversity in its reasoning paths at this stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel) show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most promising paths.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Given the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning can be interpreted as a search tree, rather than merely a reasoning “chain”. Taking the case of Figure 6 as a concrete example, the frst step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus, grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the frontier nodes will be the grandchildren of Alex (Figure 7, right).  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Unlike a standard breadth-frst search (BFS), which explores all frontier nodes uniformly, the model demonstrates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model’s preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to switch back to language space after a single latent thought ( $k=1$ ), it predicts the next step in a structured format, such as “every [Concept A] is a [Concept B].” By examining the probability distribution over potential fllers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left). Similarly, when $k=2$ , the prediction probabilities for all frontier nodes—the grandchildren of Alex —are obtained (Figure 7, right).  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'The probability distribution can be viewed as the model’s implicit value function, estimating each node’s potential to reach the target. As shown in the fgure, “lempus”, “zhorpus”, “grimpus”, and “sterpus” have a value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the frst continuous thought, the model has mostly ruled out “sterpus” as an option but remains uncertain about the correct choice among the other three. In the second thought, however, the model has mostly ruled out other options but focused on “rorpus”.  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': 'Figure 8 presents an analysis of the parallelism in the model’s latent reasoning across the frst and second thoughts. For the frst thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps between the three lines indicate that the model maintains signifcant diversity in its reasoning paths at this stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel) show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most promising paths.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 14,\n",
       "  'title': '5.4 Why is a Latent Space Better for Planning?',\n",
       "  'text': 'In this section, we explore why latent reasoning is advantageous for planning, drawing on the search tree perspective and the value function defned earlier. Referring to our illustrative example, a key distinction between “sterpus” and the other three options lies in the structure of the search tree: “sterpus” is a leaf node (Figure 6). This makes it immediately identifable as an incorrect choice, as it cannot lead to the target node “bompus”. In contrast, the other nodes have more descendants to explore, making their evaluation more challenging.  \\nTo quantify a node’s exploratory potential, we measure its height in the tree, defned as the shortest distance to any leaf node. Based on this notion, we hypothesize that nodes with lower heights are easier to evaluate accurately, as their exploratory potential is limited. Consistent with this hypothesis, in our example, the model exhibits greater uncertainty between “grimpus” and “lempus”, both of which have a height of 2—higher than the other candidates.  \\nTo test this hypothesis more rigorously, we analyze the correlation between the model’s prediction probabilities and node heights during the frst and second latent reasoning steps across the test set. Figure 9 reveals a clear pattern: the model successfully assigns lower values to incorrect nodes and higher values to correct nodes when their heights are low. However, as node heights increase, this distinction becomes less pronounced, indicating greater difculty in accurate evaluation.  \\n![](images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg)  \\nFigure 9 The correlation between prediction probability of concepts and their heights.  \\nIn conclusion, these fndings highlight the benefts of leveraging latent space for planning. By delaying defnite decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search tree’s terminal states, making it easier to distinguish correct nodes from incorrect ones.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'In this section, we explore why latent reasoning is advantageous for planning, drawing on the search tree perspective and the value function defned earlier. Referring to our illustrative example, a key distinction between “sterpus” and the other three options lies in the structure of the search tree: “sterpus” is a leaf node (Figure 6). This makes it immediately identifable as an incorrect choice, as it cannot lead to the target node “bompus”. In contrast, the other nodes have more descendants to explore, making their evaluation more challenging.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'To quantify a node’s exploratory potential, we measure its height in the tree, defned as the shortest distance to any leaf node. Based on this notion, we hypothesize that nodes with lower heights are easier to evaluate accurately, as their exploratory potential is limited. Consistent with this hypothesis, in our example, the model exhibits greater uncertainty between “grimpus” and “lempus”, both of which have a height of 2—higher than the other candidates.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'To test this hypothesis more rigorously, we analyze the correlation between the model’s prediction probabilities and node heights during the frst and second latent reasoning steps across the test set. Figure 9 reveals a clear pattern: the model successfully assigns lower values to incorrect nodes and higher values to correct nodes when their heights are low. However, as node heights increase, this distinction becomes less pronounced, indicating greater difculty in accurate evaluation.  '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': '![](images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg)  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'Figure 9 The correlation between prediction probability of concepts and their heights.  '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'In conclusion, these fndings highlight the benefts of leveraging latent space for planning. By delaying defnite decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search tree’s terminal states, making it easier to distinguish correct nodes from incorrect ones.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 15,\n",
       "  'title': '6 Conclusion',\n",
       "  'text': 'In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through extensive experiments, we demonstrated that Coconut signifcantly enhances LLM reasoning capabilities. Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an efective reasoning pattern similar to BFS. Future work is needed to further refne and scale latent reasoning methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models to generalize more efectively across a wider range of reasoning scenarios. We anticipate that our fndings will inspire further research into latent reasoning methods, ultimately contributing to the development of more advanced machine reasoning systems.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through extensive experiments, we demonstrated that Coconut signifcantly enhances LLM reasoning capabilities. Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an efective reasoning pattern similar to BFS. Future work is needed to further refne and scale latent reasoning methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models to generalize more efectively across a wider range of reasoning scenarios. We anticipate that our fndings will inspire further research into latent reasoning methods, ultimately contributing to the development of more advanced machine reasoning systems.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 16,\n",
       "  'title': 'Acknowledgement',\n",
       "  'text': 'The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course of this work.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course of this work.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 17,\n",
       "  'title': 'References',\n",
       "  'text': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \\nMarie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 189:19–31, 2019.   \\nEden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.   \\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889–7901, 2023.   \\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \\nGoogle DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.   \\nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.   \\nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.   \\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \\nYing Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024.   \\nEvelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specifcity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428–16433, 2011.   \\nEvelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575–586, 2024.   \\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.   \\nKanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   \\nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398–11442. PMLR, 2023.   \\nFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   \\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.   \\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \\nShibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.   \\nAlex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.   \\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   \\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1–62, 2022.   \\nLucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   \\nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.   \\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For efective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.   \\nWilliam Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   \\nMartin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005–1016, 2007.   \\nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive inference. Proceedings of the National Academy of Sciences, 106(30):12554–12559, 2009.   \\nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of algebra and natural language. Psychological science, 23(8):914–922, 2012.   \\nJacob Pfau, William Merrill, and Samuel R Bowman. Let’s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   \\nAlec Radford, Jefrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \\nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   \\nYuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.   \\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \\nDiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.   \\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don’t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   \\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   \\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426–9439, 2024.   \\nXinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.   \\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.   \\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.   \\nSohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   \\nShunyu Yao, Dian Yu, Jefrey Zhao, Izhak Shafran, Tom Grifths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023.   \\nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efcient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.   \\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \\nPing Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024b.   \\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   \\nEric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.   \\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Marie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 189:19–31, 2019.   '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': 'Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889–7901, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'Google DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.   '},\n",
       "   {'paragraph_id': 'paragraph_7',\n",
       "    'md_content': 'Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_8',\n",
       "    'md_content': 'Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_9',\n",
       "    'md_content': 'Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_10',\n",
       "    'md_content': 'Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_11',\n",
       "    'md_content': 'Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specifcity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428–16433, 2011.   '},\n",
       "   {'paragraph_id': 'paragraph_12',\n",
       "    'md_content': 'Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575–586, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_13',\n",
       "    'md_content': 'Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_14',\n",
       "    'md_content': 'Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_15',\n",
       "    'md_content': 'Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398–11442. PMLR, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_16',\n",
       "    'md_content': 'Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_17',\n",
       "    'md_content': 'Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_18',\n",
       "    'md_content': 'Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_19',\n",
       "    'md_content': 'Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_20',\n",
       "    'md_content': 'Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_21',\n",
       "    'md_content': 'Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   '},\n",
       "   {'paragraph_id': 'paragraph_22',\n",
       "    'md_content': 'Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1–62, 2022.   '},\n",
       "   {'paragraph_id': 'paragraph_23',\n",
       "    'md_content': 'Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_24',\n",
       "    'md_content': 'Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_25',\n",
       "    'md_content': 'Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For efective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.   '},\n",
       "   {'paragraph_id': 'paragraph_26',\n",
       "    'md_content': 'William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_27',\n",
       "    'md_content': 'Martin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005–1016, 2007.   '},\n",
       "   {'paragraph_id': 'paragraph_28',\n",
       "    'md_content': 'Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive inference. Proceedings of the National Academy of Sciences, 106(30):12554–12559, 2009.   '},\n",
       "   {'paragraph_id': 'paragraph_29',\n",
       "    'md_content': 'Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of algebra and natural language. Psychological science, 23(8):914–922, 2012.   '},\n",
       "   {'paragraph_id': 'paragraph_30',\n",
       "    'md_content': 'Jacob Pfau, William Merrill, and Samuel R Bowman. Let’s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_31',\n",
       "    'md_content': 'Alec Radford, Jefrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   '},\n",
       "   {'paragraph_id': 'paragraph_32',\n",
       "    'md_content': 'Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   '},\n",
       "   {'paragraph_id': 'paragraph_33',\n",
       "    'md_content': 'Yuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_34',\n",
       "    'md_content': 'Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_35',\n",
       "    'md_content': 'DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_36',\n",
       "    'md_content': 'Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don’t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_37',\n",
       "    'md_content': 'Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   '},\n",
       "   {'paragraph_id': 'paragraph_38',\n",
       "    'md_content': 'Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426–9439, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_39',\n",
       "    'md_content': 'Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_40',\n",
       "    'md_content': 'Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.   '},\n",
       "   {'paragraph_id': 'paragraph_41',\n",
       "    'md_content': 'Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_42',\n",
       "    'md_content': 'Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_43',\n",
       "    'md_content': 'Shunyu Yao, Dian Yu, Jefrey Zhao, Izhak Shafran, Tom Grifths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_44',\n",
       "    'md_content': 'Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efcient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.   '},\n",
       "   {'paragraph_id': 'paragraph_45',\n",
       "    'md_content': 'Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_46',\n",
       "    'md_content': 'Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024b.   '},\n",
       "   {'paragraph_id': 'paragraph_47',\n",
       "    'md_content': 'Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   '},\n",
       "   {'paragraph_id': 'paragraph_48',\n",
       "    'md_content': 'Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.   '},\n",
       "   {'paragraph_id': 'paragraph_49',\n",
       "    'md_content': 'Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 18,\n",
       "  'title': 'A.1 Examples',\n",
       "  'text': 'examples of the questions and CoT solutions for the datasets used',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'examples of the questions and CoT solutions for the datasets used  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 19,\n",
       "  'title': 'GSM8k',\n",
       "  'text': 'Question $=$ \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $\\\\Phi100$ to get his grass cut. How much does he pay per year?\"  \\n$\\\\mathrm{Steps}=[\"*4–2{=}2*\"$ , $\"\\\\!\\\\ll\\\\!2/.5\\\\!\\\\!=\\\\!\\\\!4\\\\!\\\\gg\\\\!\"$ , $\"\\\\ll12/4{=}3*\"$ \", $\"\\\\!\\\\ll\\\\!100\\\\!*\\\\!3\\\\!=\\\\!300\\\\!*\\\\!\"]$ Answer $=~\"300\"$ \"',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Question $=$ \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $\\\\Phi100$ to get his grass cut. How much does he pay per year?\"  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': '$\\\\mathrm{Steps}=[\"*4–2{=}2*\"$ , $\"\\\\!\\\\ll\\\\!2/.5\\\\!\\\\!=\\\\!\\\\!4\\\\!\\\\gg\\\\!\"$ , $\"\\\\ll12/4{=}3*\"$ \", $\"\\\\!\\\\ll\\\\!100\\\\!*\\\\!3\\\\!=\\\\!300\\\\!*\\\\!\"]$ Answer $=~\"300\"$ \"  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 20,\n",
       "  'title': 'ProntoQA',\n",
       "  'text': 'Question $=$ \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus. Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are not foral. Lempuses are cold. Brimpuses are impuses. Every lorpus is foral. Every rompus is transparent. Grimpuses are mufed. Rompuses are yumpuses. Rompuses are wumpuses. Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus. Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses. Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not foral.\"  \\nSteps $=$ [\"Stella is a zumpus. Zumpuses are gorpuses.\", \"Stella is a gorpus. Gorpuses are rompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus is a lorpus.\", \"Stella is a lorpus. Every lorpus is foral.\", \"Stella is foral.\"] An $\\\\mathsf{\\\\Pi}_{\\\\mathrm{Wer}}=\\\\mathsf{\\\\Pi}^{\\\\mathsf{W}}\\\\mathbb{F}\\\\mathsf{a l s}.$ e\"',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Question $=$ \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus. Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are not foral. Lempuses are cold. Brimpuses are impuses. Every lorpus is foral. Every rompus is transparent. Grimpuses are mufed. Rompuses are yumpuses. Rompuses are wumpuses. Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus. Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses. Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not foral.\"  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Steps $=$ [\"Stella is a zumpus. Zumpuses are gorpuses.\", \"Stella is a gorpus. Gorpuses are rompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus is a lorpus.\", \"Stella is a lorpus. Every lorpus is foral.\", \"Stella is foral.\"] An $\\\\mathsf{\\\\Pi}_{\\\\mathrm{Wer}}=\\\\mathsf{\\\\Pi}^{\\\\mathsf{W}}\\\\mathbb{F}\\\\mathsf{a l s}.$ e\"  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 21,\n",
       "  'title': 'ProsQA',\n",
       "  'text': 'Question $=$ \"Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus. Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus. Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus. Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?\"   \\nSteps $=$ [\"Tom is a terpus.\", \"Every terpus is a brimpus.\", \"Every brimpus is a lempus.\"] An $\\\\mathtt{s w e r=\"10m}$ is a lempus.\"',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'Question $=$ \"Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus. Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus. Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus. Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?\"   '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Steps $=$ [\"Tom is a terpus.\", \"Every terpus is a brimpus.\", \"Every brimpus is a lempus.\"] An $\\\\mathtt{s w e r=\"10m}$ is a lempus.\"  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 22,\n",
       "  'title': 'A.2 Construction of ProsQA',\n",
       "  'text': 'To construct the dataset, we frst compile a set of typical entity names, such as “Alex” and “Jack,” along with fctional concept names like “lorpus” and “rorpus,” following the setting of ProntoQA (Saparov and He, 2022). Each problem is structured as a binary question: “Is [Entity] a [Concept A] or [Concept B]?” Assuming [Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to [Concept B].  \\nAlgorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation maintains distinct families of nodes and balances their sizes to prevent model shortcuts.  \\nTable 2 Statistics of the graph structure in ProsQA.   \\n<html><body><table><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>  \\nAfter the graph is constructed, nodes without parents are assigned entity names, while other nodes receive concept names. To formulate a question of the form “Is [Entity] a [Concept A] or [Concept B]?”, we designate node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B]. This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are randomly permuted in each question.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'To construct the dataset, we frst compile a set of typical entity names, such as “Alex” and “Jack,” along with fctional concept names like “lorpus” and “rorpus,” following the setting of ProntoQA (Saparov and He, 2022). Each problem is structured as a binary question: “Is [Entity] a [Concept A] or [Concept B]?” Assuming [Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to [Concept B].  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Algorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation maintains distinct families of nodes and balances their sizes to prevent model shortcuts.  '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': 'Table 2 Statistics of the graph structure in ProsQA.   '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': '<html><body><table><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>  '},\n",
       "   {'paragraph_id': 'paragraph_5',\n",
       "    'md_content': 'After the graph is constructed, nodes without parents are assigned entity names, while other nodes receive concept names. To formulate a question of the form “Is [Entity] a [Concept A] or [Concept B]?”, we designate node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B]. This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are randomly permuted in each question.  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 23,\n",
       "  'title': 'Algorithm 1 Graph Construction for ProsQA',\n",
       "  'text': '$e d g e s\\\\gets\\\\{\\\\}$   \\n$n o d e s\\\\gets\\\\{0,1\\\\}$   \\n$l a b e l s\\\\gets\\\\{0:1,1:2\\\\}$ $\\\\triangleright$ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).   \\n$g r o u p s\\\\gets\\\\{0:\\\\{\\\\},1:\\\\{0\\\\},2:\\\\{1\\\\},3:\\\\{\\\\}\\\\}$   \\n$i d x\\\\gets2$   \\nwhile $i d x<N$ do ▷ For each new node, randomly add edges from existing nodes $n\\\\_i n\\\\_n o d e s\\\\gets\\\\mathrm{poisson}($ (1.5) $r a n d\\\\gets\\\\mathrm{random}()$ if $r a n d\\\\leq0.35$ then candidat $z\\\\leftarrow g r o u p s[0]\\\\cup g r o u p s[1]$ ▷ Cannot be a descendant of node 1. else if $r a n d\\\\leq0.7$ then candidates $\\\\leftarrow$ groups[0] ∪ groups[2] ▷ Cannot be a descendant of node 0. else candidates ← nodes end if $n_{-}i n_{-}n o d e s\\\\gets\\\\operatorname*{min}(\\\\log(c a n d i d a t e s),n_{-}i n_{-}n o d e s)$ weights $\\\\leftarrow$ [depth_to_root(c) · 1.5 + 1 ∀c ∈ candidates] ▷ Defne sampling weights to prioritize deeper nodes. $\\\\triangleright$ This way, the solution reasoning chain is expected to be longer. in_nodes $\\\\leftarrow$ random_choice(candidates, n_in_nodes, prob = weights/sum(weights)) $c u r\\\\_l a b e l\\\\gets0$ for $i n\\\\_i d x\\\\in i n_{.}$ _nodes do cur_label ← cur_label | labels[in_idx] ▷ Update label using bitwise OR. edges.append((in_idx, idx)) end for groups[cur_label].append(idx) labels[idx] ← cur_label $n o d e s\\\\overleftarrow{}\\\\leftarrow{\\\\mathit{n o d e s}}\\\\cup\\\\{i d x\\\\}$ idx idx + 1   \\nend while',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': '$e d g e s\\\\gets\\\\{\\\\}$   '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': '$n o d e s\\\\gets\\\\{0,1\\\\}$   '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': '$l a b e l s\\\\gets\\\\{0:1,1:2\\\\}$ $\\\\triangleright$ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).   '},\n",
       "   {'paragraph_id': 'paragraph_4',\n",
       "    'md_content': '$g r o u p s\\\\gets\\\\{0:\\\\{\\\\},1:\\\\{0\\\\},2:\\\\{1\\\\},3:\\\\{\\\\}\\\\}$   '},\n",
       "   {'paragraph_id': 'paragraph_5', 'md_content': '$i d x\\\\gets2$   '},\n",
       "   {'paragraph_id': 'paragraph_6',\n",
       "    'md_content': 'while $i d x<N$ do ▷ For each new node, randomly add edges from existing nodes $n\\\\_i n\\\\_n o d e s\\\\gets\\\\mathrm{poisson}($ (1.5) $r a n d\\\\gets\\\\mathrm{random}()$ if $r a n d\\\\leq0.35$ then candidat $z\\\\leftarrow g r o u p s[0]\\\\cup g r o u p s[1]$ ▷ Cannot be a descendant of node 1. else if $r a n d\\\\leq0.7$ then candidates $\\\\leftarrow$ groups[0] ∪ groups[2] ▷ Cannot be a descendant of node 0. else candidates ← nodes end if $n_{-}i n_{-}n o d e s\\\\gets\\\\operatorname*{min}(\\\\log(c a n d i d a t e s),n_{-}i n_{-}n o d e s)$ weights $\\\\leftarrow$ [depth_to_root(c) · 1.5 + 1 ∀c ∈ candidates] ▷ Defne sampling weights to prioritize deeper nodes. $\\\\triangleright$ This way, the solution reasoning chain is expected to be longer. in_nodes $\\\\leftarrow$ random_choice(candidates, n_in_nodes, prob = weights/sum(weights)) $c u r\\\\_l a b e l\\\\gets0$ for $i n\\\\_i d x\\\\in i n_{.}$ _nodes do cur_label ← cur_label | labels[in_idx] ▷ Update label using bitwise OR. edges.append((in_idx, idx)) end for groups[cur_label].append(idx) labels[idx] ← cur_label $n o d e s\\\\overleftarrow{}\\\\leftarrow{\\\\mathit{n o d e s}}\\\\cup\\\\{i d x\\\\}$ idx idx + 1   '},\n",
       "   {'paragraph_id': 'paragraph_7', 'md_content': 'end while  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 24,\n",
       "  'title': 'A.3 Statistics',\n",
       "  'text': 'We show the size of all datasets in Table 3.  \\nTable 3 Statistics of the datasets.   \\n<html><body><table><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'We show the size of all datasets in Table 3.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Table 3 Statistics of the datasets.   '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': '<html><body><table><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 25,\n",
       "  'title': 'B Clock-Time Reasoning Efficiency Metric',\n",
       "  'text': 'We present a clock-time comparison to evaluate reasoning efciency. The reported values represent the average inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our results show that clock time is generally proportional to the number of newly generated tokens, as detailed in Table 1.  \\nTable 4 Inference time (in seconds) comparison across tasks and methods.   \\n<html><body><table><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'We present a clock-time comparison to evaluate reasoning efciency. The reported values represent the average inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our results show that clock time is generally proportional to the number of newly generated tokens, as detailed in Table 1.  '},\n",
       "   {'paragraph_id': 'paragraph_2',\n",
       "    'md_content': 'Table 4 Inference time (in seconds) comparison across tasks and methods.   '},\n",
       "   {'paragraph_id': 'paragraph_3',\n",
       "    'md_content': '<html><body><table><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>  '}]},\n",
       " {'level': 1,\n",
       "  'section_num': 26,\n",
       "  'title': 'C Using More Continuous Thoughts',\n",
       "  'text': 'In Figure 3, we present the performance of Coconut on GSM8k using $c\\\\in\\\\{0,1,2\\\\}$ . When experimenting with $c=3$ , we observe a slight performance drop accompanied by increased variance. Analysis of the training logs indicates that adding three continuous thoughts at once – particularly during the fnal stage transition – leads to a sharp spike in training loss, causing instability. Future work will explore fner-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning—e.g., generating the reasoning skeleton in language and completing the reasoning process in latent space—could provide a promising direction for improving performance and stability.',\n",
       "  'paragraphs': [{'paragraph_id': 'paragraph_1',\n",
       "    'md_content': 'In Figure 3, we present the performance of Coconut on GSM8k using $c\\\\in\\\\{0,1,2\\\\}$ . When experimenting with $c=3$ , we observe a slight performance drop accompanied by increased variance. Analysis of the training logs indicates that adding three continuous thoughts at once – particularly during the fnal stage transition – leads to a sharp spike in training loss, causing instability. Future work will explore fner-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning—e.g., generating the reasoning skeleton in language and completing the reasoning process in latent space—could provide a promising direction for improving performance and stability.  '}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "\n",
    "with open(content_json_file) as f:  \n",
    "    content_lst = json.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'equation', 'image', 'table', 'text'}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x['type'] for x in content_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract images\n",
    "img_lst = [x for x in content_lst if x['type'] == 'image']\n",
    "for img in img_lst:\n",
    "    desc = \"\\n\".join(img.get('img_caption', [])) + \"\\n\".join(img.get('img_footnote', []))\n",
    "    desc\n",
    "\n",
    "# extract tables\n",
    "tbl_lst = [x for x in content_lst if x['type'] == 'table']\n",
    "\n",
    "# extract formula\n",
    "formula_lst = [x for x in content_lst if x['type'] == 'equation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'image',\n",
       "  'img_path': 'images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg',\n",
       "  'img_caption': ['Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 1},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg',\n",
       "  'img_caption': ['Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 3},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg',\n",
       "  'img_caption': ['Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 5},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg',\n",
       "  'img_caption': ['Figure 4 A case study where we decode the continuous thought into language tokens. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 6},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg',\n",
       "  'img_caption': ['Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 7},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg',\n",
       "  'img_caption': ['Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\\\scriptstyle\\\\mathrm{k=2}$ ) solves the problem correctly. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 8},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg',\n",
       "  'img_caption': ['Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., “lempus” in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 8},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg',\n",
       "  'img_caption': ['Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition toward more focused exploration in later stages. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 9},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg',\n",
       "  'img_caption': ['Figure 9 The correlation between prediction probability of concepts and their heights. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 10}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'equation',\n",
       "  'text': '$$\\n\\\\begin{array}{c}{H_{t}=\\\\operatorname{Transformer}(E_{t})}\\\\\\\\ {\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\le t})=\\\\operatorname{softmax}(W h_{t})}\\\\end{array}\\n$$',\n",
       "  'text_format': 'latex',\n",
       "  'page_idx': 2}]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'table',\n",
       "  'img_path': 'images/c81e5cdf7aa362b6915254737e207d052d121cdf144405aa782f3632384b8feb.jpg',\n",
       "  'table_caption': ['Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. ∗The result is from Deng et al. (2024). '],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '\\n\\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 ±0.2</td><td>25.0</td><td>98.8 ±0.8</td><td>92.5</td><td>77.5 ±1.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 ±0.5</td><td>2.2</td><td>93.8 ±0.7</td><td>3.0</td><td>76.7 ±1.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 ±0.3</td><td>3.0</td><td>98.2 ±0.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 ±1.8</td><td>2.2</td><td>77.7: ±21.0</td><td>3.0</td><td>75.9 ±0.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 ±1.5</td><td>8.2</td><td>99.8 ±0.2</td><td>9.0</td><td>97.0 ±0.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 ±0.8</td><td>8.2</td><td>52.4 ±0.4</td><td>9.0</td><td>76.1 ±0.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 ±0.5</td><td>2.3</td><td>99.9 ±0.1</td><td>3.0</td><td>95.5 ±1.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 ±0.7</td><td>2.2</td><td>100.0 ±0.1</td><td>3.0</td><td>96.6 ±0.8</td><td>8.2</td></tr></table></body></html>\\n\\n',\n",
       "  'page_idx': 5},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/1b673db2e64bddfe21934298637d6cb120b86a6d220234001473a03b7899791a.jpg',\n",
       "  'table_caption': ['Table 2 Statistics of the graph structure in ProsQA. '],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '\\n\\n<html><body><table><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>\\n\\n',\n",
       "  'page_idx': 15},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/5c0ee50b58fd2b8c96024326986c44075d7064d5bea7fb7ac9033bcd9e8657b9.jpg',\n",
       "  'table_caption': ['Table 3 Statistics of the datasets. '],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '\\n\\n<html><body><table><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>\\n\\n',\n",
       "  'page_idx': 16},\n",
       " {'type': 'table',\n",
       "  'img_path': 'images/ebfe3c484ccf358de959ecc8d6ea60ad068ed53297db326bc4becbcfffc94a18.jpg',\n",
       "  'table_caption': ['Table 4 Inference time (in seconds) comparison across tasks and methods. '],\n",
       "  'table_footnote': [],\n",
       "  'table_body': '\\n\\n<html><body><table><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>\\n\\n',\n",
       "  'page_idx': 16}]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'image',\n",
       "  'img_path': 'images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg',\n",
       "  'img_caption': ['Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 1},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg',\n",
       "  'img_caption': ['Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 3},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg',\n",
       "  'img_caption': ['Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 5},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg',\n",
       "  'img_caption': ['Figure 4 A case study where we decode the continuous thought into language tokens. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 6},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg',\n",
       "  'img_caption': ['Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 7},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg',\n",
       "  'img_caption': ['Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\\\scriptstyle\\\\mathrm{k=2}$ ) solves the problem correctly. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 8},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg',\n",
       "  'img_caption': ['Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., “lempus” in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 8},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg',\n",
       "  'img_caption': ['Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition toward more focused exploration in later stages. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 9},\n",
       " {'type': 'image',\n",
       "  'img_path': 'images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg',\n",
       "  'img_caption': ['Figure 9 The correlation between prediction probability of concepts and their heights. '],\n",
       "  'img_footnote': [],\n",
       "  'page_idx': 10}]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_content = restore_md_toc(md_file, pdf_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_into_dicts(filepath, level=2):\n",
    "    \"\"\"\n",
    "    Splits a markdown file into a list of dictionaries based on a specified title level.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the markdown file.\n",
    "        level: The heading level to split by (e.g., 2 for ## headings).\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a section\n",
    "        with 'level', 'section_num', 'title', and 'text' keys.\n",
    "        Returns an empty list if the file doesn't exist.\n",
    "        Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            markdown_content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    title_pattern = re.compile(rf\"^#{{{level}}}\\s+(.+)$\", re.MULTILINE)\n",
    "\n",
    "    sections = []\n",
    "    current_section = \"\"\n",
    "    current_title = \"\"\n",
    "    section_num = 1  # Initialize section number\n",
    "\n",
    "    for line in markdown_content.splitlines():\n",
    "        match = title_pattern.match(line)\n",
    "        if match:\n",
    "            if current_section:  # Save the previous section\n",
    "                sections.append({\n",
    "                    'level': level,\n",
    "                    'section_num': section_num,\n",
    "                    'title': current_title,\n",
    "                    'text': current_section.strip()  # Remove leading/trailing whitespace\n",
    "                })\n",
    "                section_num += 1  # Increment for the next section\n",
    "            current_title = match.group(1).strip()\n",
    "            current_section = \"\"  # Start a new section (no title line)\n",
    "        else:\n",
    "            current_section += line + \"\\n\"  # Add to the current section\n",
    "\n",
    "    if current_section:  # Save the last section\n",
    "        sections.append({\n",
    "            'level': level,\n",
    "            'section_num': section_num,\n",
    "            'title': current_title,\n",
    "            'text': current_section.strip()\n",
    "        })\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_into_dicts(md_file, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec in sections:\n",
    "    sec_level = sec.get('level')\n",
    "    sec_num = sec.get('section_num')\n",
    "    sec_title = sec.get('title')\n",
    "    sec_text = sec.get('text')\n",
    "    for item in pdf_toc:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level': 1,\n",
       "  'section_num': 1,\n",
       "  'title': 'Training Large Language Models to Reason in a Continuous Latent Space',\n",
       "  'text': 'Shibo Hao $^{1,2,*}$ , Sainbayar Sukhbaatar1, DiJia $\\\\mathtt{s u}^{1}$ , Xian Li1, Zhiting ${\\\\mathsf{H}}{\\\\mathsf{u}}^{2}$ , Jason Weston1, Yuandong Tian1   \\n1FAIR at Meta, $^2$ UC San Diego   \\n∗Work done at Meta  \\n\\nLarge language models (LLMs) are restricted to reason in the “language space”, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed “continuous thought”). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can efectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-frst search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These fndings demonstrate the promise of latent reasoning and ofer valuable insights for future research.  \\n\\nDate: December 12, 2024  \\n\\n$\\\\infty$ Meta'},\n",
       " {'level': 1,\n",
       "  'section_num': 2,\n",
       "  'title': '1 Introduction',\n",
       "  'text': 'Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an efective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach, known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to generate solutions step-by-step using natural language. However, this is in stark contrast to certain human cognition results. Neuroimaging studies have consistently shown that the language network – a set of brain regions responsible for language comprehension and production – remains largely inactive during various reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko et al., 2024).  \\n\\nA signifcant issue arises when LLMs use language for reasoning: the amount of reasoning required for each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fuency, contributing little to the actual reasoning process. On the contrary, some critical tokens require complex planning and pose huge challenges to LLMs. While previous work has attempted to fx these problems by prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain constrained within the language space and do not solve the fundamental problems. On the contrary, it would be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their fndings into language only when necessary.  \\n\\n![](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg)  \\nFigure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed “continuous thought”), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.  \\n\\nIn this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut (Chain of Continuous Thought). It involves a simple modifcation to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token (Figure 1). This modifcation frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully diferentiable. To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024), which efectively utilizes language reasoning chains to guide the training process.  \\n\\nInterestingly, our proposed paradigm leads to an efcient reasoning pattern. Unlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-frst search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).  \\n\\nExperimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning (GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be benefcial to reasoning accuracy, mirroring the efects of language reasoning chains. This indicates the potential to scale and solve increasingly challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut and some of its variants even surpasses language-based CoT methods, while generating signifcantly fewer tokens during inference. We believe that these fndings underscore the potential of latent reasoning and could provide valuable insights for future research.'},\n",
       " {'level': 1,\n",
       "  'section_num': 3,\n",
       "  'title': '2 Related Work',\n",
       "  'text': 'Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate an intermediate reasoning process in language before outputting the fnal answer. This includes prompting LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains, either with supervised fnetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classifed the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By employing CoT, the efective depth of the transformer increases because the generated outputs are looped back to the input (Feng et al., 2023). These analyses, combined with the established efectiveness of CoT, motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While CoT has proven efective for certain tasks, its autoregressive generation nature makes it challenging to mimic human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al., 2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi et al., 2024; Su et al., 2024). In our analysis, we fnd that after removing the constraint of a language space, a new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.  \\n\\nLatent reasoning in LLMs. Previous works mostly defne latent reasoning in LLMs as the hidden computation in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) further proposed to intervene the latent reasoning by “back-patching” the hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a diferent latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable <pause> tokens to the training corpus. This improves LLM’s performance on a variety of tasks, especially when followed by supervised fnetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further explored the usage of fller tokens, e.g., “...”, and concluded that they work well for highly parallelizable problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently, it has also been found that one can “internalize” the CoT reasoning into latent reasoning in the transformer with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms. These training methods can be combined to our framework, and specifcally, we fnd that breaking down the learning of continuous thoughts into multiple stages, inspired by iCoT (Deng et al., 2024), is very benefcial for the training. Recently, looped transformers (Giannou et al., 2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating latent reasoning in comparison to language space.'},\n",
       " {'level': 1,\n",
       "  'section_num': 4,\n",
       "  'title': '3 Coconut: Chain of Continuous Thought',\n",
       "  'text': 'In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in an unconstrained latent space. We begin by introducing the background and notation we use for language models. For an input sequence $\\\\boldsymbol{x}=(x_{1},...,x_{T})$ , the standard large language model $\\\\mathcal{M}$ can be described as:  \\n\\n$$\\n\\\\begin{array}{c}{H_{t}=\\\\operatorname{Transformer}(E_{t})}\\\\\\\\ {\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\le t})=\\\\operatorname{softmax}(W h_{t})}\\\\end{array}\\n$$  \\n\\nwhere $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{t})]$ is the sequence of token embeddings up to position $t$ ; $H_{t}\\\\in\\\\mathbb{R}^{t\\\\times d}$ is the matrix of the last hidden states for all tokens up to position $t$ ; $h_{t}$ is the last hidden state of position $t$ , i.e., $h_{t}=H_{t}[t,:];\\\\,e(\\\\cdot)$ is the token embedding function; $W$ is the parameter of the language model head.  \\n\\nMethod Overview. In the proposed Coconut method, the LLM switches between the “language mode” and “latent mode” (Figure 1). In language mode, the model operates as a standard language model, autoregressively generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding. This last hidden state represents the current reasoning state, termed as a “continuous thought”.  \\n\\nSpecial tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought mode, respectively. As an example, we assume latent reasoning occurs between positions $i$ and $j$ , i.e., $x_{i}=$ $<b\\\\cot>$ and $x_{j}=<\\\\tt e o t>$ . When the model is in the latent mode $(i<t<j$ ), we use the last hidden state from the previous token to replace the input embedding, i.e., $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{t-1}]$  \\n\\n![](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg)  \\nFigure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.  \\n\\nAfter the latent mode fnishes $(t\\\\ \\\\geq\\\\ j)$ , the input reverts to using the token embedding, i.e., $E_{t}~=$ $[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{j-1},e(x_{j}),...,e(x_{t})]$ . It is worth noting that the last hidden states have been processed by the fnal normalization layer, so they are not too large in magnitude. $\\\\mathcal{M}(x_{t+1}\\\\mid x_{\\\\leq t})$ is not defned when $i<t<j$ , since the latent thought is not intended to be mapped back to language space. However, softmax $\\\\left(W h_{t}\\\\right)$ can still be calculated for probing purposes (see Section 4).  \\n\\nTraining Procedure. In this work, we focus on a problem-solving setting where the model receives a question as input and is expected to generate an answer through a reasoning process. We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al. (2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the subsequent stages, at the $k$ -th stage, the frst $k$ reasoning steps in the CoT are replaced with $k\\\\times c$ continuous thoughts1, where $c$ is a hyperparameter controlling the number of latent thoughts replacing a single language reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We insert <bot> and <eot> tokens (which are not counted towards $c$ ) to encapsulate the continuous thoughts.  \\n\\nDuring the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions and latent thoughts. It is important to note that the objective does not encourage the continuous thought to compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore, it’s possible for the LLM to learn more efective representations of reasoning steps compared to human language.  \\n\\nTraining Details. Our proposed continuous thoughts are fully diferentiable and allow for back-propagation. We perform $n+1$ forward passes when $n$ latent thoughts are scheduled in the current training stage, computing a new latent thought with each pass and fnally conducting an additional forward pass to obtain a loss on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the training efciency of Coconut remains an important direction for future research.  \\n\\nInference Process. The inference process for Coconut is analogous to standard language model decoding, except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge lies in determining when to switch between latent and language modes. As we focus on the problem-solving setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two potential strategies: a) train a binary classifer on latent thoughts to enable the model to autonomously decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We found that both approaches work comparably well. Therefore, we use the second option in our experiment for simplicity, unless specifed otherwise.'},\n",
       " {'level': 1,\n",
       "  'section_num': 5,\n",
       "  'title': '4 Experiments',\n",
       "  'text': 'We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth. The number of newly generated tokens per question is also analyzed, as a measure of reasoning efciency. We report the clock-time comparison in Appendix B.'},\n",
       " {'level': 1,\n",
       "  'section_num': 6,\n",
       "  'title': '4.1 Reasoning Tasks',\n",
       "  'text': 'Math Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade school-level math problems. Compared to the other datasets in our experiments, the problems are more diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng et al. (2023).  \\n\\nLogical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and He, 2022) questions, with fctional concept names. For each problem, a tree-structured ontology is randomly generated and described in natural language as a set of known conditions. The model is asked to judge whether a given statement is correct based on these conditions. This serves as a simplifed simulation of more advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).  \\n\\nWe found that the generation process of ProntoQA could be more challenging, especially since the size of distracting branches in the ontology is always small, reducing the need for complex planning. To fx that, we apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions. The resulting dataset requires the model to perform substantial planning and searching over the graph to fnd the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering). A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.'},\n",
       " {'level': 1,\n",
       "  'section_num': 7,\n",
       "  'title': '4.2 Experimental Setup',\n",
       "  'text': 'We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is set to $1\\\\times10^{-4}$ while the efective batch size is 128. Following Deng et al. (2024), we also reset the optimizer when the training stages switch.  \\n\\nMath Reasoning. By default, we use 2 latent thoughts (i.e., $c=2$ ) for each reasoning step. We analyze the correlation between performance and $c$ in Section 4.4. The model goes through 3 stages besides the initial stage. Then, we have an additional stage, where we still use $3\\\\times c$ continuous thoughts as in the penultimate stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in each remaining stage.  \\n\\nLogical Reasoning. We use one continuous thought for every reasoning step (i.e., $c=1$ ). The model goes through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is 6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the last stage. We train the model for 5 epochs per stage.  \\n\\nFor all datasets, after the standard schedule, the model stays in the fnal training stage, until the 50th epoch. We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the number of continuous thoughts to be consistent with their fnal training stage. We use greedy decoding for all experiments.'},\n",
       " {'level': 1,\n",
       "  'section_num': 8,\n",
       "  'title': '4.3 Baselines and Variants of Coconut',\n",
       "  'text': 'We consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model with supervised fnetuning, and during inference, the model generates a reasoning chain before outputting an answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain. (3) $i C o T$ (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully designed schedule that “internalizes” CoT. As the training goes on, tokens at the beginning of the reasoning chain are gradually removed until only the answer remains. During inference, the model directly predicts the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer, without a reasoning chain. However, diferent from No-CoT, special <pause> tokens are inserted between the question and answer, which are believed to provide the model with additional computational capacity to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous thoughts in Coconut.  \\n\\nTable 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. ∗The result is from Deng et al. (2024).   \\n\\n\\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 ±0.2</td><td>25.0</td><td>98.8 ±0.8</td><td>92.5</td><td>77.5 ±1.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 ±0.5</td><td>2.2</td><td>93.8 ±0.7</td><td>3.0</td><td>76.7 ±1.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 ±0.3</td><td>3.0</td><td>98.2 ±0.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 ±1.8</td><td>2.2</td><td>77.7: ±21.0</td><td>3.0</td><td>75.9 ±0.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 ±1.5</td><td>8.2</td><td>99.8 ±0.2</td><td>9.0</td><td>97.0 ±0.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 ±0.8</td><td>8.2</td><td>52.4 ±0.4</td><td>9.0</td><td>76.1 ±0.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 ±0.5</td><td>2.3</td><td>99.9 ±0.1</td><td>3.0</td><td>95.5 ±1.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 ±0.7</td><td>2.2</td><td>100.0 ±0.1</td><td>3.0</td><td>96.6 ±0.8</td><td>8.2</td></tr></table></body></html>  \\n\\nWe also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training, we directly use the data from the last stage which only includes questions and answers to train Coconut. The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage training which removes language reasoning steps gradually, but don’t use any continuous latent thoughts. While this is similar to $i C o\\\\,T$ in the high-level idea, the exact training schedule is set to be consistent with Coconut, instead of $i C o T$ . This ensures a more strict comparison. (3) Pause as thought: We use special <pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as Coconut.'},\n",
       " {'level': 1,\n",
       "  'section_num': 9,\n",
       "  'title': '4.4 Results and Discussion',\n",
       "  'text': 'We show the overall results on all datasets in Table 1. Continuous thoughts efectively enhance LLM reasoning, as shown by the consistent improvement over $n o{-}C o T$ . It even shows better performance than $\\\\mathit{C o T}$ on ProntoQA and ProsQA. We describe several key conclusions from the experiments as follows.  \\n\\n“Chaining” continuous thoughts enhances reasoning. In conventional CoT, the output token serves as the next input, which proves to increase the efective depth of LLMs and enhance their expressiveness (Feng et al., 2023). We explore whether latent space reasoning retains this property, as it would suggest that this method could scale to solve increasingly complex problems by chaining multiple latent thoughts.  \\n\\nIn our experiments with GSM8k, we found that Coconut outperformed other architectures trained with similar strategies, particularly surpassing the latest baseline, $i C o T$ (Deng et al., 2024). The performance is signifcantly better than Coconut (pause as thought) which also enables more computation in the LLMs. While Pfau et al. (2024) empirically shows that fller tokens, such as the special <pause> tokens, can beneft highly parallelizable problems, our results show that Coconut architecture is more efective for general problems, e.g., math word problems, where a reasoning step often heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter $c$ , which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As we increased $c$ from 0 to $^{1}$ to 2, the model’s performance steadily improved.2 These results suggest that a chaining efect similar to CoT can be observed in the latent space.  \\n\\n![](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg)  \\nFigure 3 Accuracy on GSM8k with diferent number of continuous thoughts.  \\n\\nIn two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and the $i C o T$ baseline also achieve impressive accuracy. This indicates that the model’s computational capacity may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering task, likely involves more complex contextual understanding and modeling, placing higher demands on computational capability.  \\n\\nLatent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires the model to “look ahead” and evaluate the appropriateness of each step. Among our datasets, GSM8k and ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and limited branching. In contrast, ProsQA’s randomly generated DAG structure signifcantly challenges the model’s planning capabilities. As shown in Table 1, $C o T$ does not ofer notable improvement over No-CoT. However, Coconut, its variants, and $i C o T$ substantially enhance reasoning on ProsQA, indicating that latent space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of this process is provided in Section 5.  \\n\\nThe LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most efective continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut $w/o$ curriculum). However, from the experimental results, we found the models trained this way do not perform any better than no-CoT.  \\n\\nWith the multi-stage curriculum which decomposes the training into easier objectives, Coconut is able to achieve top performance across various tasks. The multi-stage training also integrates well with pause tokens (Coconut- pause as thought). Despite using the same architecture and similar multi-stage training objectives, we observed a small gap between the performance of $i C o T$ and Coconut (w/o thoughts). The fner-grained removal schedule (token by token) and a few other tricks in $i C o T$ may ease the training process. We leave combining $i C o T$ and Coconut as future work. While the multi-stage training used for Coconut has proven efective, further research is defnitely needed to develop better and more general strategies for learning reasoning in latent space, especially without the supervision from language reasoning chains.  \\n\\n![](images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg)  \\nFigure 4 A case study where we decode the continuous thought into language tokens.  \\n\\nContinuous thoughts are efficient representations of reasoning. Though the continuous thoughts are not intended to be decoded to language tokens, we can  \\n\\nstill use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of a math word problem solved by Coconut $\\\\mathit{\\\\Pi}_{c}=1\\\\;\\\\;$ ). The frst continuous thought can be decoded into tokens like “ $\\\\mathrm{\\\\Omega}180^{\\\\circ}$ , “ $180^{\\\\circ}$ (with a space), and “9”. Note that, the reasoning trace for this problem should be $3\\\\times3\\\\times60=9\\\\times60=540$ , or $3\\\\times3\\\\times60=3\\\\times180=540$ . The interpretations of the frst thought happen to be the frst intermediate variables in the calculation. Moreover, it encodes a distribution of diferent traces into the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for planning-intense reasoning tasks.  \\n\\n![](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg)  \\nFigure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.'},\n",
       " {'level': 1,\n",
       "  'section_num': 10,\n",
       "  'title': '5 Understanding the Latent Reasoning in Coconut',\n",
       "  'text': 'In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By leveraging its ability to switch between language and latent space reasoning, we are able to control the model to interpolate between fully latent reasoning and fully language reasoning and test their performance (Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3). Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).'},\n",
       " {'level': 1,\n",
       "  'section_num': 11,\n",
       "  'title': '5.1 Experimental Setup',\n",
       "  'text': 'Methods. The design of Coconut allows us to control the number of latent thoughts by manually setting the position of the $<\\\\tt e o t>$ token during inference. When we enforce Coconut to use $k$ continuous thoughts, the model is expected to output the remaining reasoning chain in language, starting from the $k+1$ step. In our experiments, we test variants of Coconut on ProsQA with $k\\\\in\\\\{0,1,2,3,4,5,6\\\\}$ . Note that all these variants only difer in inference time while sharing the same model weights. Besides, we report the performance of $\\\\mathit{C o T}$ and no-CoT as references.  \\n\\nTo address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum by always mixing data from other stages with a certain probability $p=0.3$ ). This updated training curriculum yields similar performance and enables efective control over the switch between latent and language reasoning.  \\n\\nMetrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the fnal answer, regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4). To enable fne-grained analysis, we defne another metric on the reasoning process. Assuming we have a complete language reasoning chain which specifes a path in the graph, we can classify it into (1) Correct Path: The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one being asked. These four categories naturally apply to the output from Coconut ( $k=0$ ) and $C o T$ , which generate the full path. For Coconut with $k>0$ that outputs only partial paths in language (with the initial steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete it. Also, we defne Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes the path, it’s classifed as hallucination. In no-CoT and Coconut with larger $k$ , the model may only output the fnal answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six categories cover all cases without overlap.  \\n\\n![](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg)  \\nFigure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\\\scriptstyle\\\\mathrm{k=2}$ ) solves the problem correctly.  \\n\\n![](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg)  \\nFigure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., “lempus” in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.'},\n",
       " {'level': 1,\n",
       "  'section_num': 12,\n",
       "  'title': '5.2 Interpolating between Latent and Language Reasoning',\n",
       "  'text': 'Figure 5 shows a comparative analysis of diferent reasoning methods on ProsQA. As more reasoning is done with continuous thoughts (increasing $k$ ), both fnal answer accuracy (Figure 5, left) and the rate of correct reasoning processes (“Correct Label” and “Correct Path” in Figure 5, right) improve. Additionally, the rate of “Hallucination” and “Wrong Target” decrease, which typically occur when the model makes a wrong move earlier. This also indicates the better planning ability when more reasoning happens in the latent space.  \\n\\nA case study is shown in Figure 6, where $\\\\mathit{C o T}$ hallucinates an nonexistent edge, Coconut ( $k=1$ ) leads to a wrong target, but Coconut ( $k=2$ ) successfully solves the problem. In this example, the model cannot accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in Section 5.3.  \\n\\nThe comparison between $\\\\mathit{C o T}$ and Coconut ( $k=0$ ) reveals another interesting observation: even when  \\n\\n![](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg)  \\nFigure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model’s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model’s transition toward more focused exploration in later stages.  \\n\\nCoconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than $\\\\mathit{C o T}$ . The generated reasoning paths are also more accurate with less hallucination. From this, we can infer that the training method of mixing diferent stages improves the model’s ability to plan ahead. The training objective of $\\\\mathit{C o T}$ always concentrates on the generation of the immediate next step, making the model “shortsighted”. In later stages of Coconut training, the frst few steps are hidden, allowing the model to focus more on future steps. This is related to the fndings of Gloeckle et al. (2024), where they propose multi-token prediction as a new pretraining objective to improve the LLM’s ability to plan ahead.'},\n",
       " {'level': 1,\n",
       "  'section_num': 13,\n",
       "  'title': '5.3 Interpreting the Latent Search Tree',\n",
       "  'text': 'Given the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning can be interpreted as a search tree, rather than merely a reasoning “chain”. Taking the case of Figure 6 as a concrete example, the frst step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus, grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the frontier nodes will be the grandchildren of Alex (Figure 7, right).  \\n\\nUnlike a standard breadth-frst search (BFS), which explores all frontier nodes uniformly, the model demonstrates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model’s preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to switch back to language space after a single latent thought ( $k=1$ ), it predicts the next step in a structured format, such as “every [Concept A] is a [Concept B].” By examining the probability distribution over potential fllers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left). Similarly, when $k=2$ , the prediction probabilities for all frontier nodes—the grandchildren of Alex —are obtained (Figure 7, right).  \\n\\nThe probability distribution can be viewed as the model’s implicit value function, estimating each node’s potential to reach the target. As shown in the fgure, “lempus”, “zhorpus”, “grimpus”, and “sterpus” have a value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the frst continuous thought, the model has mostly ruled out “sterpus” as an option but remains uncertain about the correct choice among the other three. In the second thought, however, the model has mostly ruled out other options but focused on “rorpus”.  \\n\\nFigure 8 presents an analysis of the parallelism in the model’s latent reasoning across the frst and second thoughts. For the frst thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps between the three lines indicate that the model maintains signifcant diversity in its reasoning paths at this stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel) show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most promising paths.'},\n",
       " {'level': 1,\n",
       "  'section_num': 14,\n",
       "  'title': '5.4 Why is a Latent Space Better for Planning?',\n",
       "  'text': 'In this section, we explore why latent reasoning is advantageous for planning, drawing on the search tree perspective and the value function defned earlier. Referring to our illustrative example, a key distinction between “sterpus” and the other three options lies in the structure of the search tree: “sterpus” is a leaf node (Figure 6). This makes it immediately identifable as an incorrect choice, as it cannot lead to the target node “bompus”. In contrast, the other nodes have more descendants to explore, making their evaluation more challenging.  \\n\\nTo quantify a node’s exploratory potential, we measure its height in the tree, defned as the shortest distance to any leaf node. Based on this notion, we hypothesize that nodes with lower heights are easier to evaluate accurately, as their exploratory potential is limited. Consistent with this hypothesis, in our example, the model exhibits greater uncertainty between “grimpus” and “lempus”, both of which have a height of 2—higher than the other candidates.  \\n\\nTo test this hypothesis more rigorously, we analyze the correlation between the model’s prediction probabilities and node heights during the frst and second latent reasoning steps across the test set. Figure 9 reveals a clear pattern: the model successfully assigns lower values to incorrect nodes and higher values to correct nodes when their heights are low. However, as node heights increase, this distinction becomes less pronounced, indicating greater difculty in accurate evaluation.  \\n\\n![](images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg)  \\nFigure 9 The correlation between prediction probability of concepts and their heights.  \\n\\nIn conclusion, these fndings highlight the benefts of leveraging latent space for planning. By delaying defnite decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search tree’s terminal states, making it easier to distinguish correct nodes from incorrect ones.'},\n",
       " {'level': 1,\n",
       "  'section_num': 15,\n",
       "  'title': '6 Conclusion',\n",
       "  'text': 'In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through extensive experiments, we demonstrated that Coconut signifcantly enhances LLM reasoning capabilities. Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an efective reasoning pattern similar to BFS. Future work is needed to further refne and scale latent reasoning methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models to generalize more efectively across a wider range of reasoning scenarios. We anticipate that our fndings will inspire further research into latent reasoning methods, ultimately contributing to the development of more advanced machine reasoning systems.'},\n",
       " {'level': 1,\n",
       "  'section_num': 16,\n",
       "  'title': 'Acknowledgement',\n",
       "  'text': 'The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course of this work.'},\n",
       " {'level': 1,\n",
       "  'section_num': 17,\n",
       "  'title': 'References',\n",
       "  'text': 'Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \\nMarie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 189:19–31, 2019.   \\nEden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.   \\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889–7901, 2023.   \\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \\nGoogle DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.   \\nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.   \\nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.   \\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \\nYing Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024.   \\nEvelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specifcity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428–16433, 2011.   \\nEvelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575–586, 2024.   \\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.   \\nKanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   \\nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398–11442. PMLR, 2023.   \\nFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   \\nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.   \\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \\nShibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.   \\nAlex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.   \\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   \\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1–62, 2022.   \\nLucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   \\nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.   \\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For efective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.   \\nWilliam Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   \\nMartin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005–1016, 2007.   \\nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive inference. Proceedings of the National Academy of Sciences, 106(30):12554–12559, 2009.   \\nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of algebra and natural language. Psychological science, 23(8):914–922, 2012.   \\nJacob Pfau, William Merrill, and Samuel R Bowman. Let’s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   \\nAlec Radford, Jefrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \\nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   \\nYuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.   \\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \\nDiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.   \\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don’t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   \\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   \\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426–9439, 2024.   \\nXinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.   \\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.   \\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.   \\nSohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   \\nShunyu Yao, Dian Yu, Jefrey Zhao, Izhak Shafran, Tom Grifths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023.   \\nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efcient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.   \\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \\nPing Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024b.   \\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   \\nEric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.   \\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.'},\n",
       " {'level': 1, 'section_num': 18, 'title': 'Appendix', 'text': ''},\n",
       " {'level': 1, 'section_num': 19, 'title': 'A Datasets', 'text': ''},\n",
       " {'level': 1,\n",
       "  'section_num': 20,\n",
       "  'title': 'A.1 Examples',\n",
       "  'text': 'examples of the questions and CoT solutions for the datasets used'},\n",
       " {'level': 1,\n",
       "  'section_num': 21,\n",
       "  'title': 'GSM8k',\n",
       "  'text': 'Question $=$ \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $\\\\Phi100$ to get his grass cut. How much does he pay per year?\"  \\n\\n$\\\\mathrm{Steps}=[\"*4–2{=}2*\"$ , $\"\\\\!\\\\ll\\\\!2/.5\\\\!\\\\!=\\\\!\\\\!4\\\\!\\\\gg\\\\!\"$ , $\"\\\\ll12/4{=}3*\"$ \", $\"\\\\!\\\\ll\\\\!100\\\\!*\\\\!3\\\\!=\\\\!300\\\\!*\\\\!\"]$ Answer $=~\"300\"$ \"'},\n",
       " {'level': 1,\n",
       "  'section_num': 22,\n",
       "  'title': 'ProntoQA',\n",
       "  'text': 'Question $=$ \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus. Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are not foral. Lempuses are cold. Brimpuses are impuses. Every lorpus is foral. Every rompus is transparent. Grimpuses are mufed. Rompuses are yumpuses. Rompuses are wumpuses. Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus. Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses. Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not foral.\"  \\n\\nSteps $=$ [\"Stella is a zumpus. Zumpuses are gorpuses.\", \"Stella is a gorpus. Gorpuses are rompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus is a lorpus.\", \"Stella is a lorpus. Every lorpus is foral.\", \"Stella is foral.\"] An $\\\\mathsf{\\\\Pi}_{\\\\mathrm{Wer}}=\\\\mathsf{\\\\Pi}^{\\\\mathsf{W}}\\\\mathbb{F}\\\\mathsf{a l s}.$ e\"'},\n",
       " {'level': 1,\n",
       "  'section_num': 23,\n",
       "  'title': 'ProsQA',\n",
       "  'text': 'Question $=$ \"Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus. Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus. Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus. Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?\"   \\nSteps $=$ [\"Tom is a terpus.\", \"Every terpus is a brimpus.\", \"Every brimpus is a lempus.\"] An $\\\\mathtt{s w e r=\"10m}$ is a lempus.\"'},\n",
       " {'level': 1,\n",
       "  'section_num': 24,\n",
       "  'title': 'A.2 Construction of ProsQA',\n",
       "  'text': 'To construct the dataset, we frst compile a set of typical entity names, such as “Alex” and “Jack,” along with fctional concept names like “lorpus” and “rorpus,” following the setting of ProntoQA (Saparov and He, 2022). Each problem is structured as a binary question: “Is [Entity] a [Concept A] or [Concept B]?” Assuming [Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to [Concept B].  \\n\\nAlgorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation maintains distinct families of nodes and balances their sizes to prevent model shortcuts.  \\n\\nTable 2 Statistics of the graph structure in ProsQA.   \\n\\n\\n<html><body><table><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>  \\n\\nAfter the graph is constructed, nodes without parents are assigned entity names, while other nodes receive concept names. To formulate a question of the form “Is [Entity] a [Concept A] or [Concept B]?”, we designate node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B]. This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are randomly permuted in each question.'},\n",
       " {'level': 1,\n",
       "  'section_num': 25,\n",
       "  'title': 'Algorithm 1 Graph Construction for ProsQA',\n",
       "  'text': '$e d g e s\\\\gets\\\\{\\\\}$   \\n$n o d e s\\\\gets\\\\{0,1\\\\}$   \\n$l a b e l s\\\\gets\\\\{0:1,1:2\\\\}$ $\\\\triangleright$ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).   \\n$g r o u p s\\\\gets\\\\{0:\\\\{\\\\},1:\\\\{0\\\\},2:\\\\{1\\\\},3:\\\\{\\\\}\\\\}$   \\n$i d x\\\\gets2$   \\nwhile $i d x<N$ do ▷ For each new node, randomly add edges from existing nodes $n\\\\_i n\\\\_n o d e s\\\\gets\\\\mathrm{poisson}($ (1.5) $r a n d\\\\gets\\\\mathrm{random}()$ if $r a n d\\\\leq0.35$ then candidat $z\\\\leftarrow g r o u p s[0]\\\\cup g r o u p s[1]$ ▷ Cannot be a descendant of node 1. else if $r a n d\\\\leq0.7$ then candidates $\\\\leftarrow$ groups[0] ∪ groups[2] ▷ Cannot be a descendant of node 0. else candidates ← nodes end if $n_{-}i n_{-}n o d e s\\\\gets\\\\operatorname*{min}(\\\\log(c a n d i d a t e s),n_{-}i n_{-}n o d e s)$ weights $\\\\leftarrow$ [depth_to_root(c) · 1.5 + 1 ∀c ∈ candidates] ▷ Defne sampling weights to prioritize deeper nodes. $\\\\triangleright$ This way, the solution reasoning chain is expected to be longer. in_nodes $\\\\leftarrow$ random_choice(candidates, n_in_nodes, prob = weights/sum(weights)) $c u r\\\\_l a b e l\\\\gets0$ for $i n\\\\_i d x\\\\in i n_{.}$ _nodes do cur_label ← cur_label | labels[in_idx] ▷ Update label using bitwise OR. edges.append((in_idx, idx)) end for groups[cur_label].append(idx) labels[idx] ← cur_label $n o d e s\\\\overleftarrow{}\\\\leftarrow{\\\\mathit{n o d e s}}\\\\cup\\\\{i d x\\\\}$ idx idx + 1   \\nend while'},\n",
       " {'level': 1,\n",
       "  'section_num': 26,\n",
       "  'title': 'A.3 Statistics',\n",
       "  'text': 'We show the size of all datasets in Table 3.  \\n\\nTable 3 Statistics of the datasets.   \\n\\n\\n<html><body><table><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>'},\n",
       " {'level': 1,\n",
       "  'section_num': 27,\n",
       "  'title': 'B Clock-Time Reasoning Efficiency Metric',\n",
       "  'text': 'We present a clock-time comparison to evaluate reasoning efciency. The reported values represent the average inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our results show that clock time is generally proportional to the number of newly generated tokens, as detailed in Table 1.  \\n\\nTable 4 Inference time (in seconds) comparison across tasks and methods.   \\n\\n\\n<html><body><table><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>'},\n",
       " {'level': 1,\n",
       "  'section_num': 28,\n",
       "  'title': 'C Using More Continuous Thoughts',\n",
       "  'text': 'In Figure 3, we present the performance of Coconut on GSM8k using $c\\\\in\\\\{0,1,2\\\\}$ . When experimenting with $c=3$ , we observe a slight performance drop accompanied by increased variance. Analysis of the training logs indicates that adding three continuous thoughts at once – particularly during the fnal stage transition – leads to a sharp spike in training loss, causing instability. Future work will explore fner-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning—e.g., generating the reasoning skeleton in language and completing the reasoning process in latent space—could provide a promising direction for improving performance and stability.'}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Images and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
