[
    {
        "type": "text",
        "text": "s1: Simple test-time scaling ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Niklas Muennighof \\* 1 3 4 Zitong Yang \\* 1 Weijia $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{*}\\mathbf{\\lambda}^{2}$ Xiang Lisa $\\mathbf{L}\\mathbf{i}^{*}\\mathbf{\\Lambda}^{1}$ Li Fei-Fei 1 Hannaneh Hajishirzi 2 3 Luke Zettlemoyer 2 Percy Liang 1 Emmanuel Candès 1 Tatsunori Hashimoto 1 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI’s o1 model showed this capability but did not publicly share its methodology, leading to many replication eforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difculty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end. This can lead the model to doublecheck its answer, often fxing incorrect reasoning steps. After supervised fnetuning the Qwen2.5- 32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to $27\\%$ (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from $50\\%$ to $57\\%$ on AIME24. Our model, data, and code are open-source at https: //github.com/simplescaling/s1. ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/26a28e99d42ffab6fb20649e9cdf7ad1065c9fe00807a8eabaae5db71a29d14c.jpg",
        "img_caption": [
            "Figure 1. Test-time scaling with s1-32B. We benchmark s1-32B on reasoning-intensive tasks and vary test-time compute. "
        ],
        "img_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "of this approach is to increase the compute at test time to get better results. There has been much work exploring this idea (Snell et al., 2024; Welleck et al., 2024), and the viability of this paradigm was recently validated by OpenAI o1 (OpenAI, 2024). o1 has demonstrated strong reasoning performance with consistent gains from scaling test-time compute. OpenAI describes their approach as using largescale reinforcement learning (RL) implying the use of sizable amounts of data (OpenAI, 2024). This has led to various attempts to replicate their models relying on techniques like Monte Carlo Tree Search (Gao et al., 2024b; Zhang et al., 2024a), multi-agent approaches (Qin et al., 2024), and others (Wang et al., 2024a; Huang et al., 2024b; 2025). Among these approaches, DeepSeek R1 (DeepSeek-AI et al., 2025) has successfully replicated o1-level performance, also employing reinforcement learning via millions of samples and multiple training stages. However, despite the large number of o1 replication attempts, none have openly replicated a clear test-time scaling behavior. Thus, we ask: what is the simplest approach to achieve both test-time scaling and strong reasoning performance? ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1. Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Performance improvements of language models (LMs) over the past years have largely relied on scaling up train-time compute using large-scale self-supervised pretraining (Kaplan et al., 2020; Hofmann et al., 2022). The creation of these powerful models has set the stage for a new scaling paradigm built on top of them: test-time scaling. The aim ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "\\*Equal contribution. ZY and NM started the project. WS, NM and ZY collected the prompts, XL, ZY and NM, built the data pipeline, LZ and WS proposed using a 1K subset and ZY and NM built budget forcing. 1 Stanford University. 2 University of Washington, Seattle. 3 Allen Institute for AI. 4 Contextual AI. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We show that training on only 1,000 samples with next-token prediction and controlling thinking duration via a simple test-time technique we refer to as budget forcing leads to a strong reasoning model that scales in performance with more test-time compute. Specifcally, we construct s1K, which consists of 1,000 carefully curated questions paired with reasoning traces and answers distilled from Gemini Thinking Experimental (Google, 2024). We perform supervised fne-tuning (SFT) of an of-the-shelf pretrained model on our small dataset requiring just 26 minutes of training on 16 H100 GPUs. After training, we control the amount of test-time compute our model spends using budget forcing: (I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. Ending the thinking this way makes the model transition to generating its answer. (II) If we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “Wait” to the model’s current reasoning trace to encourage more exploration. Equipped with this simple recipe – SFT on 1,000 samples and test-time budget forcing – our model s1- 32B exhibits test-time scaling (Figure 1). Further, s1-32B is the most sample-efcient reasoning model and outperforms closed-source models like OpenAI’s o1-preview (Figure 2). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We conduct extensive ablation experiments targeting (a) our selection of 1,000 (1K) reasoning samples and (b) our testtime scaling. For (a), we fnd that jointly incorporating difculty, diversity, and quality measures into our selection algorithm is important. Random selection, selecting samples with the longest reasoning traces, or only selecting maximally diverse samples all lead to signifcantly worse performance (around $-30\\%$ on AIME24 on average). Training on our full data pool of 59K examples, a superset of s1K, does not ofer substantial gains over our 1K selection. This highlights the importance of careful data selection and echoes prior fndings for instruction tuning (Zhou et al., 2023). For ${\\bf(b)}$ , we defne desiderata for test-time scaling methods to compare diferent approaches. Budget forcing leads to the best scaling as it has perfect controllability with a clear positive slope leading to strong performance. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In summary, our contributions are: We develop simple methods for creating a sample-efcient reasoning dataset (§2) and test-time scaling (§3); Based on these we build s1-32B which is competitive with o1-preview (§4); We ablate subtleties of data (§5.1) and test-time scaling (§5.2). We end with a discussion to motivate future work on simple reasoning (§6). Our code, model, and data are open-source at https://github.com/simplescaling/s1. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2. Reasoning data curation to create s1K ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In this section, we describe our process for creating a large dataset frst in $\\S2.1$ and then fltering it down to s1K in $\\S2.2$ . ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.1. Initial collection of 59K samples ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We collect an initial 59,029 questions from 16 diverse sources following three guiding principles. Quality: Datasets should be of high quality; we always inspect samples and ignore datasets with, e.g., poor formatting; Difculty: Datasets should be challenging and require signifcant reasoning efort; Diversity: Datasets should stem from different felds to cover diferent reasoning tasks. We collect datasets of two categories: ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Curation of existing datasets Our largest source is NuminaMATH (LI et al., 2024) with 30,660 mathematical problems from online websites. We also include historical AIME problems (1983-2021). To enhance diversity, we add OlympicArena (Huang et al., 2024a) with 4,250 questions spanning Astronomy, Biology, Chemistry, Computer Science, Geography, Mathematics, and Physics from various Olympiads. OmniMath (Gao et al., 2024a) adds 4,238 competition-level mathematics problems. We also include 2,385 problems from AGIEval (Zhong et al., 2023), which features questions from standardized tests like SAT and LSAT, covering English, Law, and Logic. We refer to Table 6 in $\\S B$ for our other sources. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "New datasets in quantitative reasoning To complement these existing datasets, we create two original datasets. s1-prob consists of 182 questions from the probability section of Stanford University’s Statistics Department’s PhD Qualifying Exams (https://statistics. stanford.edu), accompanied by handwritten solutions that cover difcult proofs. The probability qualifying exam is held yearly and requires professional-level mathematical problem-solving. s1-teasers comprises 23 challenging brain-teasers commonly used in interview questions for quantitative trading positions. Each sample consists of a problem and solution taken from PuzzledQuant (https: //www.puzzledquant.com/). We only take examples with the highest difculty level (\"Hard\"). ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "For each question, we generate a reasoning trace and solution using the Google Gemini Flash Thinking API (Google, 2024) extracting its reasoning trace and response. This yields 59K triplets of a question, generated reasoning trace, and generated solution. Examples from our dataset are in $\\mathrm{\\SC}.2$ . We decontaminate all samples against our evaluation questions (MATH500, GPQA Diamond, AIME24; $\\S B.5$ ) using 8-grams and deduplicate the data. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.2. Final selection of 1K samples ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "We could directly train on our pool of 59K questions, however, our goal is to fnd the simplest approach with minimal resources. Thus, we go through three stages of fltering to arrive at a minimal set of 1,000 samples relying on our three guiding data principles: Quality, Difculty, and Diversity. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Quality We frst remove any questions where we ran into any API errors reducing our dataset to 54,116 samples. Next, we flter out low-quality examples by checking if they contain any string patterns with formatting issues, such as ASCII art diagrams, non-existent image references, or inconsistent question numbering reducing our dataset to 51,581 examples. From this pool, we identify 384 samples for our fnal 1,000 samples from datasets that we perceive as high-quality and not in need of further fltering (see $\\S B.4$ for details). ",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/fb358a87d032ad5b6372a93636e2d9fac5efa3694d8d652d28a26273eace8e3b.jpg",
        "img_caption": [
            "Figure 2. s1K and s1-32B. (left) s1K is a dataset of 1,000 high-quality, diverse, and difcult questions with reasoning traces. (right) s1-32B, a 32B parameter model fnetuned on s1K is on the sample-efciency frontier. See Table 1 for details on other models. "
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Difculty For difculty, we use two indicators: model performance and reasoning trace length. We evaluate two models on each question: Qwen2.5-7B-Instruct and Qwen2.5- 32B-Instruct (Qwen et al., 2024), with correctness assessed by Claude 3.5 Sonnet comparing each attempt against the reference solution (see $\\S B.3$ for the grading protocol). We measure the token length of each reasoning trace to indicate problem difculty using the Qwen2.5 tokenizer. This relies on the assumption that more difcult problems require more thinking tokens. Based on the grading, we remove questions that either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct can solve correctly and thus may be too easy. By using two models we reduce the likelihood of an easy sample slipping through our fltering due to a rare mistake on an easy question of one of the models. This brings our total samples down to 24,496, setting the stage for the next round of subsampling based on diversity. While fltering with these two models may be optimized for our setup as we will also use Qwen2.5-32B-Instruct as our model to fnetune, the idea of model-based fltering generalizes to other setups. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Diversity To quantify diversity we classify each question into specifc domains using Claude 3.5 Sonnet based on the Mathematics Subject Classifcation (MSC) system (e.g., geometry, dynamic systems, real analysis, etc.) from the American Mathematical Society.1 The taxonomy focuses on topics in mathematics but also includes other sciences such as biology, physics, and economics. To select our fnal examples from the pool of 24,496 questions, we frst choose one domain uniformly at random. Then, we sample one problem from this domain according to a distribution that favors longer reasoning traces (see $\\S B.4$ for details) as motivated in Difculty. We repeat this process until we have 1,000 total samples. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "This three-stage process yields a dataset spanning 50 different domains (see Table 5). In $\\S5.1$ , we will show that using our three criteria in combination is important, as only relying on quality, diversity, or difculty in isolation leads to worse datasets. Examples from our dataset are in $\\mathrm{\\SC}.2$ . ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3. Test-time scaling ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3.1. Method ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We classify test-time scaling methods into 1) Sequential, where later computations depend on earlier ones (e.g., a long reasoning trace), and 2) Parallel, where computations run independently (e.g., majority voting) (Snell et al., 2024; Brown et al., 2024). We focus on sequential scaling as intuitively we believe it should scale better, since later computations can build on intermediate results, allowing for deeper reasoning and iterative refnement. We propose new sequential scaling methods and ways to benchmark them. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Budget forcing We propose a simple decoding-time intervention by forcing a maximum and/or minimum number of thinking tokens at test time. Specifcally, we enforce a maximum token count by simply appending the end-of-thinking token delimiter and “Final Answer:” to early exit the thinking stage and make the model provide its current best answer. To enforce a minimum, we suppress the generation of the end-of-thinking token delimiter and optionally append the string “Wait” to the model’s current reasoning trace to encourage the model to refect on its current generation. Figure 3 contains an example of how this simple approach can lead the model to arrive at a better answer. ",
        "page_idx": 2
    },
    {
        "type": "table",
        "img_path": "images/d4ec4aba9df8bf28b657a5f4180578541c2dd53ef7aebb1059adf9308a007952.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>How many r in raspberry? Question</td></tr><tr><td>Let'sbreakdowntheprocessofcountingtheletter'r'inthe word \"raspberry\"... *First letter:'r'-This is an'r', count= 1. * Second letter: 'a' - Not an 'r', count remains 1 ... *Sixth letter:'e'-Not an'r',count remains 1. * Seventh letter: 'r' - This is an 'r', count = 2. * Eighth letter:'y'-Not an'r', count remains 2... Thenumber of'r's in\"raspberry\"is 2. Wait,let'sre-read thequestioncarefully.Itasks\"Howmanyr in raspberry?\"...* r - a -S-p- b - e -r - r-y ... * First 'r'.. *</td></tr><tr><td>Second 'r' ... * Third 'r' ... Count = 3 ... Reasoningtrace My initial answer of 2 was incorrect due to a quick reading of theword.FinalAnswer:The finalansweris3 Response</td></tr></table></body></html>\n\n",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Baselines We benchmark budget forcing with: (I) Conditional length-control methods, which rely on telling the model in the prompt how long it should generate for. We group them by granularity into (a) Token-conditional control: We specify an upper bound of thinking tokens in the prompt; (b) Step-conditional control: We specify an upper bound of thinking steps, where each step is around 100 tokens; (c) Class-conditional control: We write two generic prompts that tell the model to either think for a short or long amount of time (see $\\S D.1$ for details). (II) Rejection sampling, which samples until a generation fts a predetermined compute budget. This oracle captures the posterior over responses conditioned on its length. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.2. Metrics ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "We establish a set of desiderata as evaluation metrics to measure test-time scaling across methods. Importantly, we do not only care about the accuracy a method can achieve but also its controllability and test-time scaling slope. For each method we consider, we run a set of evaluations $a\\in$ $\\mathbfcal{A}$ varying test-time compute on a fxed benchmark, e.g. AIME24. This produces a piece-wise linear function $f$ with compute as the $\\mathbf{X}$ -axis measured in thinking tokens and accuracy as the y-axis (see Figure 1, where the rightmost dot for AIME24 corresponds to $f(7320)=57\\%$ ). We measure ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "three metrics: ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm{Control}=\\frac{1}{\\left|\\mathcal{A}\\right|}\\sum_{a\\in\\mathcal{A}}\\mathbb{I}(a_{\\operatorname*{min}}\\leq a\\leq a_{\\operatorname*{max}})\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "where $a_{\\mathrm{min}},a_{\\mathrm{max}}$ refer to a pre-specifed minimum and maximum amount of test-time compute; in our case thinking tokens. We usually only constrain $a_{\\mathrm{max}}$ . As tokens generated correspond to the amount of test-time compute spent, this metric measures the extent to which a method allows controllability over the use of that test-time compute. We report it as a percentage with $100\\%$ being perfect control. ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n{\\mathrm{Scaling}}={\\frac{1}{{\\binom{|A|}{2}}}}\\sum_{a,b\\in{\\mathcal{A}}\\atop b>a}{\\frac{f(b)-f(a)}{b-a}}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Scaling is the average slope of the piece-wise linear function.   \nIt must be positive for useful methods and larger is better. ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n{\\mathrm{Performance}}=\\operatorname*{max}_{a\\in{\\mathcal{A}}}f(a)\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Performance is simply the maximum performance the method achieves on the benchmark. A method with monotonically increasing scaling achieves $100\\%$ performance on any benchmark in the limit. However, the methods we investigate eventually fatten out or further scaling fails due to control or context window limitations. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4. Results ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4.1. Setup ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Training We perform supervised fnetuning on Qwen2.5- 32B-Instruct using s1K to obtain our model s1-32B using basic hyperparameters outlined in $\\S C$ . Finetuning took 26 minutes on 16 NVIDIA H100 GPUs with PyTorch FSDP. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Evaluation We select three representative reasoning benchmarks widely used in the feld: AIME24 (of America, 2024) consists of 30 problems that were used in the 2024 American Invitational Mathematics Examination (AIME) held from Wednesday, January 31 – Thursday, February 1, 2024. AIME tests mathematical problem-solving with arithmetic, algebra, counting, geometry, number theory, probability, and other secondary school math topics. High-scoring high school students in the test are invited to participate in the United States of America Mathematics Olympiad (USAMO). All AIME answers are integers ranging from 000 to 999, inclusive. Some AIME problems rely on fgures that we provide to our model using the vector graphics language Asymptote as it cannot take image inputs. MATH500 (Hendrycks et al., 2021) is a benchmark of competition math problems of varying difculty. We evaluate on the same 500 samples selected by OpenAI in prior work (Lightman et al., 2023). GPQA Diamond (Rein et al., 2023) consists of 198 ",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/ae6f1f77ab26b902e65e3b710e6b66237e96313b101eb9b3a534391169f76a82.jpg",
        "img_caption": [
            "Figure 4. Sequential and parallel test-time scaling. (a): Budget forcing shows clear scaling trends and extrapolates to some extent. For the three rightmost dots, we prevent the model from stopping its thinking 2/4/6 times, each time appending “Wait” to its current reasoning trace. $(b)$ : For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance when majority voting across 2, 4, 8, 16, 32, and 64 of these. "
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "PhD-level science questions from Biology, Chemistry and Physics. Experts with PhDs in the corresponding domains only achieved $69.7\\%$ on GPQA Diamond (OpenAI, 2024). When we write “GPQA” in the context of evaluation in this work, we always refer to the Diamond subset. We build on the “lm-evaluation-harness” framework (Gao et al., 2021; Biderman et al., 2024). ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Other models We benchmark s1-32B against: OpenAI o1 series (OpenAI, 2024), which are closed-source models that popularized the idea of test-time scaling; DeepSeek r1 series (Team, 2024a), which are open-weight reasoning models with up to o1-level performance, concurrently released to ours; Qwen’s QwQ-32B-preview (Team, 2024b), a 32B open-weight reasoning model without disclosed methodology; Sky-T1-32B-Preview (Team, 2025) and Bespoke32B (Labs, 2025), which are open models with open reasoning data distilled from QwQ-32B-preview and r1; Google Gemini 2.0 Flash Thinking Experimental (Google, 2024), the API that we distill from. As it has no ofcial evaluation scores, we use the Gemini API to benchmark it ourselves. However, the “recitation error” of the Gemini API makes evaluation challenging.2 We circumvent this, by manually inserting all 30 AIME24 questions in its web interface where the error does not appear. However, we leave out MATH500 (500 questions) and GPQA Diamond (198 questions), thus they are N.A. in Table 1. Our model, s1-32B, is fully open including weights, reasoning data, and code. ",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/238ce03b31c7cc6f1081e58756830f0271819e4a89661e621d457f415382e951.jpg",
        "table_caption": [
            "Table 1. s1-32B is an open and sample-efcient reasoning model. We evaluate s1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see $\\S4$ ). Other results are from the respective reports (Qwen et al., 2024; Team, 2024b; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs, 2025; Team, 2025). # ex. $=$ number examples used for reasoning fnetuning; BF $=$ budget forcing. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Model</td><td>#ex.</td><td>AIME 2024</td><td>MATH 500</td><td>GPQA Diamond</td></tr><tr><td colspan=\"5\">API only</td></tr><tr><td>01-preview</td><td>N.A.</td><td>44.6</td><td>85.5</td><td>73.3</td></tr><tr><td>01-mini</td><td>N.A.</td><td>70.0</td><td>90.0</td><td>60.0</td></tr><tr><td>01</td><td>N.A.</td><td>74.4</td><td>94.8</td><td>77.3</td></tr><tr><td>Gemini 2.0 Flash Think.</td><td>N.A.</td><td>60.0</td><td>N.A.</td><td>N.A.</td></tr><tr><td colspan=\"5\">Open Weights</td></tr><tr><td>Qwen2.5- 32B-Instruct</td><td>N.A.</td><td>26.7</td><td>84.0</td><td>49.0</td></tr><tr><td>QwQ-32B</td><td>N.A.</td><td>50.0</td><td>90.6</td><td>65.2</td></tr><tr><td>rl</td><td>800K</td><td>79.8</td><td>97.3</td><td>71.5</td></tr><tr><td>rl-distill</td><td>800K</td><td>72.6</td><td>94.3</td><td>62.1</td></tr><tr><td colspan=\"5\">Open Weights and Open Data</td></tr><tr><td>Sky-T1 Bespoke-32B</td><td>17K 17K</td><td>43.3 63.3</td><td>82.4 93.0</td><td>56.8 58.1</td></tr><tr><td>s1 w/o BF</td><td>1K</td><td>50.0</td><td>92.6</td><td>56.6</td></tr><tr><td>s1-32B</td><td>1K</td><td>56.7</td><td>93.0</td><td>59.6</td></tr></table></body></html>\n\n",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4.2. Performance ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Test-time scaling Figure 1 shows the performance of s1- 32B with budget forcing scales with more test-time compute. In Figure 4 (left), we expand the plot from Figure 1 (middle) showing that while we can improve AIME24 performance using our budget forcing technique (§3) and more test-time compute it does eventually fatten out at six times. Suppressing the end-of-thinking token delimiter too often can lead the model into repetitive loops instead of continued reasoning. In Figure 4 (right), we show that after training Qwen2.5- 32B-Instruct on our 1,000 samples to produce s1-32B and equipping it with the simple budget forcing technique, it operates in a diferent scaling paradigm. Scaling test-time compute on the base model via majority voting cannot catch up with the performance of s1-32B which validates our intuition from $\\S3$ that sequential scaling is more efective than parallel. We provide example generations of s1-32B in Figure 5. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Sample-efciency In Figure 2 (right) and Table 1 we compare s1-32B with other models. We fnd that s1- 32B is the most sample-efcient open data reasoning model. It performs signifcantly better than our base model (Qwen2.5-32B-Instruct) despite just training it on an additional 1,000 samples. The concurrently released r1-32B shows stronger performance than s1-32B while also only using SFT (DeepSeek-AI et al., 2025). However, it is trained on $800\\,\\times$ more reasoning samples. It is an open question whether one can achieve their performance with just 1,000 samples. Finally, our model nearly matches Gemini 2.0 Thinking on AIME24. As s1-32B is distilled from Gemini 2.0, this shows our distillation procedure was likely efective. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5. Ablations ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5.1. Data Quantity, Diversity, and Difculty ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "In $\\S2$ we outlined our three guiding principles in curating s1K: Quality, Difculty, and Diversity. Here we test the importance of combining them and the overall efcacy of our selection. Only Quality (1K-random): After obtaining our high-quality reasoning chains from Gemini, we select 1,000 samples at random; not relying on our difculty and diversity fltering at all. Table 2 shows this approach performs much worse than s1K across all benchmarks. Only Diversity (1K-diverse): For this dataset, we sample uniformly across domains to maximize diversity disregarding any notion of difculty. This approach also leads to poor performance similar to 1K-random. Only Difculty (1K-longest): Here we rely on one of our difculty indicators introduced in $\\S2$ by selecting the 1,000 samples with the longest reasoning traces. This approach signifcantly boosts GPQA performance but overall still falls short of using s1K. Maximize Quantity: Finally, we compare with just training on all of our 59K ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Table 2. s1K data ablations. We budget force (BF) a maximum of around 30,000 thinking tokens for all scores in this table. This performs slightly better than the scores without BF (Table 1) as it allows the model to fnish with a best guess when stuck in an infnite loop. We report $95\\%$ paired bootstrap confdence intervals for diferences relative to the s1K model using 10,000 bootstrap samples. E.g., the interval $[-13\\%$ , $20\\%]$ means that, with $95\\%$ confdence, the true diference between 59K-full and s1K is between $13\\%$ and $+20\\%$ . If the entire interval is negative, e.g. $[-27\\%,-3\\%]$ we can confdently say that the performance is worse than s1K. ",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/77bd3d07e7b9e15299ab136bddcac33a990392323b41f231ff4eb1b9ab659b53.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Model</td><td>AIME 2024</td><td>MATH 500</td><td>GPQA Diamond</td></tr><tr><td>1K-random</td><td>36.7 [-26.7%,-3.3%]</td><td>90.6 [-4.8%,0.0%]</td><td>52.0 [-12.6%,2.5%]</td></tr><tr><td>1K-diverse</td><td>26.7 [-40.0%,-10.0%]</td><td>91.2 [-4.0%,0.2%]</td><td>54.6 [-10.1%,5.1%]</td></tr><tr><td>1K-longest</td><td>33.3</td><td>90.4</td><td>59.6</td></tr><tr><td>59K-full</td><td>[-36.7%,0.0%] 53.3</td><td>[-5.0%,-0.2%] 92.8</td><td>[-5.1%,10.1%] 58.1</td></tr><tr><td>s1K</td><td>[-13.3%,20.0%] 50.0</td><td>[-2.6%, 2.2%] 93.0</td><td>[-6.6%, 8.6%] 57.6</td></tr></table></body></html>\n\n",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "samples, a superset of all the 1K-sample versions. This leads to a strong model but uses much more resources. To fnetune on 59K samples, we use $394\\;\\mathrm{Hl}00$ GPU hours while s1-32B only required 7 H100 GPU hours. Moreover, relying only on s1K is extremely competitive as shown in $\\S2$ Overall, combining all three criteria – Quality, Difculty, Diversity – via our methodology in $\\S2$ is key for sampleefcient reasoning training. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5.2. Test-time scaling methods ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/5fe55ebf141787ae358b96ebf4a88f06372374bf488dd122aaf75ee1314193db.jpg",
        "table_caption": [
            "Table 3. Ablations on methods to scale test-time compute on AIME24. $|{\\mathcal{A}}|$ refers to the number of evaluation runs used to estimate th e| pr|operties; thus a higher value indicates more robustness. Bold indicates our chosen method and the best values. BF $=$ budget forcing, TCC/SCC/CCC $=$ token/step/class-conditional control, $\\mathrm{RS}=$ rejection sampling. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>Control</td><td>Scaling</td><td>Performance</td><td>[AI</td></tr><tr><td>BF</td><td>100%</td><td>15</td><td>56.7</td><td>5</td></tr><tr><td>TCC</td><td>40%</td><td>-24</td><td>40.0</td><td>5</td></tr><tr><td>TCC+BF</td><td>100%</td><td>13</td><td>40.0</td><td>5</td></tr><tr><td>SCC</td><td>%09</td><td>3</td><td>36.7</td><td>5</td></tr><tr><td>SCC + BF</td><td>100%</td><td>6</td><td>36.7</td><td>5</td></tr><tr><td>CCC</td><td>50%</td><td>25</td><td>36.7</td><td>2</td></tr><tr><td>RS</td><td>100%</td><td>-35</td><td>40.0</td><td>5</td></tr></table></body></html>\n\n",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "[AIME24] ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "[MATH500] ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "[GPQA] ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Alice and Bob play the following game. A stack of $n$ tokens lies before them. The players take turns with Alice going frst. On each turn, the player removes either 1 token or 4 tokens from the stack. Whoever removes the last token wins. Find the number of positive integers $n$ less than or equal to 2024 for which there exists a strategy for Bob that guarantees that Bob will win the game regardless of Alice’s play. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The problem asks for the number of positive integers $n\\leq2024$ such that Bob has a winning strategy. This is a typical impartial game that can be analyzed using the Sprague-Grundy theorem or by directly fnding the winning and losing positions. Let $W$ be the set of winning positions (number of tokens $n$ such that the player whose turn it is can force a win) and $L$ be the set of losing positions. [...] ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\binom{2}{6}\\,{\\mathfrak{i f}}\\,a\\cdot b=8.\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The projection of vector 𝐚 onto vector 𝐛 is given by the formula: ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm{proj}_{\\mathbf{b}}\\mathbf{a}={\\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\|\\mathbf{b}\\|^{2}}}\\mathbf{b}\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "First, we need to fnd the magnitude squared of $\\mathbf{b}$ , which is $\\|\\mathbf{b}\\|^{2}$ . ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "A quantum mechanical particle of mass $m$ moves in two dimensions in the following potential, as a function of the polar coordinates $(r,\\theta)$ : ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\nV(r,\\theta)=\\frac{1}{2}k r^{2}+\\frac{3}{2}k r^{2}\\cos^{2}(\\theta)\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Find the energy spectrum. Hint: Write the potential in Cartesian coordinates. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The problem asks for the energy spectrum of a particle in a given 2D potential. The potential is given in polar coordinates $(r,\\theta)$ : ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\|\\mathbf{b}\\|^{2}=\\mathbf{b}\\cdot\\mathbf{b}=2^{2}+6^{2}+3^{2}=4+36+9=49.\\,\\!\\!\\!\\!V(r,\\theta)=\\frac{1}{2}k r^{2}+\\frac{3}{2}k r^{2}\\cos^{2}(\\theta).\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Let $P(n)$ be the statement \"the player whose turn it is when there are $n$ tokens has a winning strategy. $P(0)$ is false (the player whose turn it is and faces 0 tokens has already lost). Consider the small values of $n$ : $n=1$ : The player whose turn it is can take 1 token and win. So $P(1)$ is true. $1\\in W$ .   \n$n=2$ : The player whose turn it is can take 1 token, leaving 1 token for the opponent. Since $1\\in W$ , the opponent wins. So $P(2)$ is false. $2\\in L$ .   \n$n=3$ : The player whose turn it is can take 1 token, leaving 2 tokens for the opponent. Since $2\\in L$ , the player whose turn it is wins. So $P(3)$ is true. $3\\in$ $W$ .[...] ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Now we can plug the values into the projection formula: ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\nV(r,\\theta)=\\frac{1}{2}k r^{2}(1+3\\cos^{2}(\\theta)).\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "We can rewrite the potential as: ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm{proj_{b}a=\\frac{8}{49}b}\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Substitute the vector 𝐛: ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The Hamiltonian for a particle in 2D is given by: ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm{proj}_{\\mathfrak{b}}\\mathbf{a}=\\frac{8}{49}\\binom{2}{3}\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\nH=-\\frac{\\hbar^{2}}{2m}\\left(\\frac{\\partial^{2}}{\\partial x^{2}}+\\frac{\\partial^{2}}{\\partial y^{2}}\\right)+V(x,y).\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "In polar coordinates $(r,\\theta)$ , the Laplacian operator is: ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "We are looking for the number of positive integers $n\\leq2024$ for which Bob has a winning strategy. This means that if the game starts with $n$ tokens, and Alice goes frst, Bob can win. This happens if and only if $n$ is a losing position (because if $n\\in L$ , then Alice’s frst move must lead to a winning position for Bob). [...] ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Multiply the scalar by each component of the vector: ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n{\\mathrm{proj}}_{\\mathfrak{b}}\\mathfrak{a}={\\left(\\begin{array}{l}{{\\frac{8}{49}}\\times2}\\\\ {{\\frac{8}{49}}\\times6}\\\\ {{\\frac{8}{49}}\\times3}\\end{array}\\right)}={\\left(\\begin{array}{l}{{\\frac{16}{49}}}\\\\ {{\\frac{48}{49}}}\\\\ {{\\frac{24}{49}}}\\end{array}\\right)}\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\nabla^{2}=\\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r\\frac{\\partial}{\\partial r}\\right)+\\frac{1}{r^{2}}\\frac{\\partial^{2}}{\\partial\\theta^{2}}.\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "So the Hamiltonian in polar coordinates is: ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Let’s try to fnd a pattern in the losing positions $L=\\{0,2,5,7,10,12,15,17,20,22,...\\}$ . Diferences between consecutive terms: $2,3,2,3,2,3,2,3,2,\\ldots$ The pattern of diferences is alternating 2, 3. [...] ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The losing positions are of the form $5m$ or $5m+2$ , where $m\\geq0$ . We need to fnd the number of positive integers $n\\leq2024$ that are in the set of losing positions $L$ . [...] ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Final Answer: The fnal answer is ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\nH=-\\frac{\\hbar^{2}}{2m}\\left[\\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r\\frac{\\partial}{\\partial r}\\right)+\\frac{1}{r^{2}}\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\right]\\!+\\!V(r,\\theta).\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The time-independent Schrödinger equation is $\\emph{H}\\psi\\~=~\\emph{E}\\psi$ . We can try to separate variables by assuming a solution of the form $\\psi(r,\\theta)=R(r)\\Theta(\\theta)$ . [...] ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "This confrms the energy spectrum is: ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Case 1: $n\\:=\\:5m$ , where $m\\,\\geq\\,1$ . $1\\,\\leq\\,5m\\,\\leq\\,2024$ $1/5\\,\\leq\\,m\\,\\leq\\,2024/5\\,=\\,404.8$ Since $m$ is an integer, $1\\leq m\\leq404$ . The number of values of $m$ is 404. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Case 2: $n=5m+2$ , where $m\\geq0$ . $5m+2\\leq2024$ $5m\\,\\leq\\,2022\\ m\\,\\leq\\,2022/5\\,=\\,404.4$ Since $m$ is an integer, $0\\,\\leq\\,m\\,\\leq\\,404$ . The number of values of $m$ is $404-0+1=405$ . ",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/15a43be29a2aed4fc1f80a99419f4349db22d167c9c9bfc17698a402567228c8.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\nE=\\hbar{\\sqrt{\\frac{k}{m}}}\\left(2n_{x}+n_{y}+{\\frac{3}{2}}\\right).\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The total number of positive integers $n\\leq2024$ that are losing positions is $404+405={\\boxed{809}}.$ ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Figure 5. Example model outputs. We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model generates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of s1-32B. The gray ellipsis $[\\ldots]$ indicates that the text was trimmed to ft this page, but the generated text is actually longer. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Budget forcing In Table 3 we compare the test-time scaling methods we have introduced in $\\S3$ . Overall, we fnd that budget forcing provides perfect control, good scaling, and leads to our best AIME24 score. Thus, this is the method we use for s1-32B in Figure 1 and in $\\S4$ . In Table 4, we compare diferent strings for extrapolating performance. We fnd that “Wait” generally gives the best performance. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Class-conditional control We provide benchmark scores for this method in $\\S D.1$ and summarize three fndings here: (1) Token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so. (2) Under step-conditional control, the model generates a similar total number of tokens when given diferent step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step. Thus, the model learns to hack its way around the compute constraint making the controllability of this method mediocre. (3) Class-conditional control can work - telling a model to simply think longer can increase its test-time compute and performance, which leads good scaling in Table 3. ",
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/0876ff9d204df235f55cbeae66b2b3378f56ac493c2fd93c7e831acb61feb035.jpg",
        "table_caption": [
            "Table 4. Budget forcing extrapolation ablations. We compare ignoring the end-of-thinking delimiter twice and appending none or various strings. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Model</td><td>AIME 2024</td><td>MATH 500</td><td>GPQA Diamond</td></tr><tr><td>No extrapolation</td><td>50.0</td><td>93.0</td><td>57.6</td></tr><tr><td>2x without string</td><td>50.0</td><td>90.2</td><td>55.1</td></tr><tr><td>2x “Alternatively\"</td><td>50.0</td><td>92.2</td><td>59.6</td></tr><tr><td>2x“Hmm\"</td><td>50.0</td><td>93.0</td><td>59.6</td></tr><tr><td>2x \"Wait\"</td><td>53.3</td><td>93.0</td><td>59.6</td></tr></table></body></html>\n\n",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Rejection sampling Surprisingly, we fnd that simply sampling until the generation fts a specifc length leads to an inverse scaling trend as depicted in Figure 6. In $\\S\\mathrm{D}.2$ we inspect a question, which was answered correctly by the model when rejection sampling for $\\leq4000$ , but not for the $\\leq8000$ token setting. In the $\\leq4000$ setting the model directly jumps to the correct approach, while for the $\\leq8000$ setting it backtracks a lot. We hypothesize that there is a correlation such that shorter generations tend to be the ones where the model was on the right track from the start, whereas longer ones tend to be ones where the model made mistakes and thus backtracks or questions itself. This leads to longer samples often being wrong when rejection sampling and thus the inverse scaling trend. ",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/14f48bae42a217680f2ca946230a03e6c40037b9103320f6f9e8170fb3ce07e7.jpg",
        "img_caption": [
            "Figure 6. Rejection sampling on AIME24 with s1-32B. We sample with a temperature of 1 until all generations have less than (from left to right) 3500, 4000, 5000, 8000, and 16000 thinking tokens requiring an average of 655, 97, 8, 3, 2, and 1 tries per sample. "
        ],
        "img_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "6. Discussion and related work ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "6.1. Sample-efcient reasoning ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Models There are a number of concurrent eforts to build models that replicate the performance of o1 (OpenAI, 2024). For example, DeepSeek-r1 and $\\mathrm{k}1.5$ (DeepSeek-AI et al., 2025; Team et al., 2025) are built with reinforcement learning methods, while others rely on SFT using tens of thousands of distilled examples (Team, 2025; Xu et al., 2025; Labs, 2025). We show that SFT on only 1,000 examples sufces to build a competitive reasoning model matching o1-preview and produces a model that lies on the pareto frontier (Figure 2). Further, we introduce budget forcing which combined with our reasoning model leads to the frst reproduction of OpenAI’s test-time scaling curves (OpenAI, 2024). Why does supervised fnetuning on just 1,000 samples lead to such performance gains? We hypothesize that the model is already exposed to large amounts of reasoning data during pretraining which spans trillions of tokens. Thus, the ability to perform reasoning is already present in our model. Our sample-efcient fnetuning stage just activates it and we scale it further at test time with budget forcing. This is similar to the \"Superfcial Alignment Hypothesis\" presented in LIMA (Zhou et al., 2023), where the authors fnd that 1,000 examples can be sufcient to align a model to adhere to user preferences. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Benchmarks and methods To evaluate and push the limits of these models, increasingly challenging benchmarks have been introduced, such as Olympiad-level science competitions (He et al., 2024; Jain et al., 2024; Zhong et al., 2023) and others (Srivastava et al., 2023; Glazer et al., 2024; Su et al., 2024; Kim et al., 2024; Phan et al., 2025). To enhance models’ performance on reasoning-related tasks, researchers have pursued several strategies: Prior works have explored continuing training language models on specialized corpora related to mathematics and science (Azerbayev et al., 2023; Yang et al., 2024), sometimes even synthetically generated data (Yu et al., 2024). Others have developed training methodologies specifcally aimed at reasoning performance (Zelikman et al., 2022; 2024; Luo et al., 2025; ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Yuan et al., 2025; Wu et al., 2024a). Another signifcant line of work focuses on prompting-based methods to elicit and improve reasoning abilities, including methods like Chainof-Thought prompting (Wei et al., 2023; Yao et al., 2023a;b; Bi et al., 2023; Fu et al., 2023; Zhang et al., 2024b; Xiang et al., 2025; Hu et al., 2024). These combined eforts aim to advance the reasoning ability of language models, enabling them to handle more complex and abstract tasks efectively. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "6.2. Test-time scaling ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Methods As we introduce in $\\S3$ , we diferentiate two methods to scale test-time compute: parallel and sequential. The former relies on multiple solution attempts generated in parallel and selecting the best outcome via specifc criteria. These criteria include choosing the most frequent response for majority voting or the best response based on an external reward for Best-of-N (Brown et al., 2024; Irvine et al., 2023; Snell et al., 2024). Unlike repeated sampling, previous sequential scaling methods let the model generate solution attempts sequentially based on previous attempts, allowing it to refne each attempt based on previous outcomes (Snell et al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based search methods (Gandhi et al., 2024; Wu et al., 2024b) ofer a hybrid approach between sequential and parallel scaling, such as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024; Zhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and guided beam search (Xie et al., 2023). REBASE (Wu et al., 2024b) employs a process reward model to balance exploitation and pruning during tree search. Empirically, REBASE has been shown to outperform sampling-based methods and MCTS (Wu et al., 2024b). Reward models (Lightman et al., 2023; Wang et al., 2024b;c) play a key role in these methods. They come in two variants: outcome reward models and process reward models. Outcome reward models (Xin et al., 2024; Ankner et al., 2024) assign a score to complete solutions and are particularly useful in Best-of-N selection, while process reward models (Lightman et al., 2023; Wang et al., 2024b; Wu et al., 2024b) assess individual reasoning steps and are efective in guiding tree-based search methods. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Limits to further test-time scaling We have shown that budget forcing allows extrapolating test-time compute in $\\S4$ , e.g., improving AIME24 performance from $50\\%$ to $57\\%$ . However, it has two key limitations when scaling further: it eventually fattens out (Figure 4), and the context window of the underlying language model constrains it. Despite these constraints, our work shows test-time scaling across a wide range of accuracies (Figure 1), partly because scaling down test-time compute behaves predictably and does not sufer from these constraints. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Continuing test-time scaling will require approaches that can further extrapolate test-time compute. How can we get such extrapolation? There may be improvements to budget forcing such as rotating through diferent strings, not only “Wait”, or combining it with frequency penalties or higher temperature to avoid repetitive loops. An exciting direction for future work is also researching whether applying budget forcing to a reasoning model trained with reinforcement learning yields better extrapolation; or if RL allows for new ways of test-time scaling beyond budget forcing. Our work defnes the right metrics (§3.2) – Control, Scaling, and Performance – to enable future research and progress on extrapolating test-time compute. ",
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/4151a7ce00d5baf47a0d564fefd287e4ba2264268786a6f6d12730dc8f891daf.jpg",
        "img_caption": [
            "Figure 7. Scaling further with parallel scaling methods. All metrics averaged over the 30 questions in AIME24. Average thinking tokens for REBASE do not account for the additional compute from the reward model. For sequential scaling, we prompt the model to use up to (from left to right) 32, 64, 256, and 512 steps. For REBASE and majority voting we generate 16 parallel trajectories to aggregate across. "
        ],
        "img_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Parallel scaling as a solution Parallel scaling ofers one solution to the limits of sequential scaling, thus we augment our sequentially scaled model with two methods: (I) Majority voting: After generating $k$ solutions, the fnal solution is the most frequent one across generations; $(\\mathbf{II})$ Tree search via REBASE: We use the REBASE process reward model, which is initialized from LLaMA-34B and further fnetuned on a synthetic process reward modeling dataset (Wu et al., 2024b). We then aggregate the solutions generated by REBASE via majority voting. As shown in Figure 7, augmenting our model with REBASE scales better than majority voting, and even sequential scaling in this scenario. However, REBASE requires an additional forward pass at each step for the reward model adding some computation overhead. For sequential scaling, when prompted to use up to 512 steps, for 12 out of the 30 evaluation questions the model generates a response that exceeds the context window leading to a large performance drop. Overall, we fnd that these parallel scaling methods complement sequential scaling thus they ofer an avenue for scaling test-time compute even further; beyond fxed context windows. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Impact Statement ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Language models with strong reasoning capabilities have the potential to greatly enhance human productivity, from assisting in complex decision-making to driving scientifc breakthroughs. However, recent advances in reasoning, such as OpenAI’s o1 and DeepSeek’s r1, lack transparency, limiting broader research progress. Our work aims to push the frontier of reasoning in a fully open manner, fostering innovation and collaboration to accelerate advancements that ultimately beneft society. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Acknowledgements ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "This work was partly conducted using the Stanford Marlowe GPU cluster (Kapfer et al., 2025) made possible by fnancial support from Stanford University. We thank Alexander M. Rush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting Zhou, John Yang, Ludwig Schmidt, Samy Jelassi, Tengyu Ma, Xuechen Li, Yu Sun and Yue Zhang for very constructive discussions. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Ankner, Z., Paul, M., Cui, B., Chang, J. D., and Ammanabrolu, P. Critique-out-loud reward models, 2024. URL https://arxiv.org/abs/2408.11791.   \nArora, D., Singh, H. G., and Mausam. Have llms advanced enough? a challenging problem solving benchmark for large language models, 2023. URL https://arxiv. org/abs/2305.15074.   \nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics, 2023.   \nBi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and Chen, H. When do program-of-thoughts work for reasoning?, 2023. URL https://arxiv.org/abs/2308. 15452.   \nBiderman, S., Schoelkopf, H., Sutawika, L., Gao, L., Tow, J., Abbasi, B., Aji, A. F., Ammanamanchi, P. S., Black, S., Clive, J., DiPof, A., Etxaniz, J., Fattori, B., Forde, J. Z., Foster, C., Hsu, J., Jaiswal, M., Lee, W. Y., Li, H., Lovering, C., Muennighof, N., Pavlick, E., Phang, J., Skowron, A., Tan, S., Tang, X., Wang, K. A., Winata, G. I., Yvon, F., and Zou, A. Lessons from the trenches on reproducible evaluation of language models, 2024.   \nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787.   \nCesista, F. L. Multimodal structured generation: Cvpr’s 2nd mmfm challenge technical report, 2024. URL https: //arxiv.org/abs/2406.11403.   \nChen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: A theoremdriven question answering dataset, 2023. URL https: //arxiv.org/abs/2305.12524.   \nChoi, S., Fang, T., Wang, Z., and Song, Y. Kcts: Knowledgeconstrained tree search decoding with token-level hallucination detection, 2023. URL https://arxiv.org/ abs/2310.09044.   \nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948.   \nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., et al. The llama 3 herd ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "of models, 2024. URL https://arxiv.org/abs/ 2407.21783.   \nFu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T. Complexity-based prompting for multi-step reasoning, 2023. URL https://arxiv.org/abs/2210. 00720.   \nGandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (sos): Learning to search in language, 2024. URL https://arxiv. org/abs/2404.03683.   \nGao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li, L., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D., Quan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T., and Chang, B. Omni-math: A universal olympiad level mathematic benchmark for large language models, 2024a. URL https://arxiv.org/abs/2410.07985.   \nGao, L., Tow, J., Biderman, S., Black, S., DiPof, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighof, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, September 2021. URL https:// doi.org/10.5281/zenodo.5371628.   \nGao, Z., Niu, B., He, X., Xu, H., Liu, H., Liu, A., Hu, X., and Wen, L. Interpretable contrastive monte carlo tree search reasoning, 2024b. URL https://arxiv.org/abs/ 2410.01707.   \nGlazer, E., Erdil, E., Besiroglu, T., Chicharro, D., Chen, E., Gunning, A., Olsson, C. F., Denain, J.-S., Ho, A., de Oliveira Santos, E., Järviniemi, O., Barnett, M., Sandler, R., Vrzala, M., Sevilla, J., Ren, Q., Pratt, E., Levine, L., Barkley, G., Stewart, N., Grechuk, B., Grechuk, T., Enugandla, S. V., and Wildon, M. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai, 2024. URL https://arxiv.org/abs/2411. 04872.   \nGoogle. Gemini 2.0 fash thinking mode (gemini-2.0- fash-thinking-exp-1219), December 2024. URL https://cloud.google.com/vertex-ai/ generative-ai/docs/thinking-mode.   \nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighof, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models, 2024.   \nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientifc problems, 2024. URL https://arxiv.org/abs/2402.14008.   \nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874.   \nHofmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203. 15556.   \nHou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J., Tang, J., and Dong, Y. Advancing language model reasoning through reinforcement learning and inference scaling, 2025. URL https://arxiv.org/abs/2501. 11651.   \nHu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models, 2024. URL https://arxiv.org/ abs/2406.09403.   \nHuang, Z., Wang, Z., Xia, S., Li, X., Zou, H., Xu, R., Fan, R.-Z., Ye, L., Chern, E., Ye, Y., Zhang, Y., Yang, Y., Wu, T., Wang, B., Sun, S., Xiao, Y., Li, Y., Zhou, F., Chern, S., Qin, Y., Ma, Y., Su, J., Liu, Y., Zheng, Y., Zhang, S., Lin, D., Qiao, Y., and Liu, P. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai, 2024a. URL https://arxiv.org/abs/2406. 12753.   \nHuang, Z., Zou, H., Li, X., Liu, Y., Zheng, Y., Chern, E., Xia, S., Qin, Y., Yuan, W., and Liu, P. O1 replication journey – part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?, 2024b. URL https://arxiv.org/abs/2411.16489.   \nHuang, Z., Geng, G., Hua, S., Huang, Z., Zou, H., Zhang, S., Liu, P., and Zhang, X. O1 replication journey – part 3: Inference-time scaling for medical reasoning, 2025. URL https://arxiv.org/abs/2501.06458.   \nIrvine, R., Boubert, D., Raina, V., Liusie, A., Zhu, Z., Mudupalli, V., Korshuk, A., Liu, Z., Cremer, F., Assassi, V., Beauchamp, C.-C., Lu, X., Rialan, T., and Beauchamp, W. Rewarding chatbots for real-world engagement with millions of users, 2023. URL https: //arxiv.org/abs/2303.06135.   \nJain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974.   \nKapfer, C., Stine, K., Narasimhan, B., Mentzel, C., and Candes, E. Marlowe: Stanford’s gpu-based computational instrument, January 2025. URL https://doi.org/ 10.5281/zenodo.14751899.   \nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361.   \nKim, E., Suk, J., Kim, S., Muennighof, N., Kim, D., and Oh, A. Llm-as-an-interviewer: Beyond static testing through dynamic llm evaluation, 2024. URL https://arxiv. org/abs/2412.10424.   \nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efcient memory management for large language model serving with pagedattention, 2023. URL https://arxiv. org/abs/2309.06180.   \nLabs, B. Bespoke-stratos: The unreasonable efectiveness of reasoning distillation, 2025. URL https://hf. co/bespokelabs/Bespoke-Stratos-32B. Accessed: 2025-01-22.   \nLee, K.-H., Fischer, I., Wu, Y.-H., Marwood, D., Baluja, S., Schuurmans, D., and Chen, X. Evolving deeper llm thinking, 2025. URL https://arxiv.org/abs/ 2501.09891.   \nLI, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numinamath, 2024. URL https://github.com/project-numina/ aimo-progress-prize/blob/main/report/ numina_dataset.pdf.   \nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let’s verify step by step, 2023. URL https: //arxiv.org/abs/2305.20050.   \nLing, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation $:$ Learning to solve and explain algebraic word problems, 2017. URL https: //arxiv.org/abs/1705.04146.   \nLiu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning, 2020. URL https: //arxiv.org/abs/2007.08124.   \nLiu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi, H., and Celikyilmaz, A. Don’t throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2024. URL https: //arxiv.org/abs/2309.15028.   \nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019.   \nLuo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., Tang, Y., and Zhang, D. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, 2025. URL https: //arxiv.org/abs/2308.09583.   \nMuennighof, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, P., Tafjord, O., Lambert, N., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden, D., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi, A., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H. Olmoe: Open mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2409.02060.   \nof America, M. A. Aime, February 2024. URL https: //artofproblemsolving.com/wiki/index. php/AIME_Problems_and_Solutions/.   \nOpenAI. Learning to reason with llms, September 2024. URL https://openai.com/index/ learning-to-reason-with-llms/.   \nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Shi, S., Choi, M., Agrawal, A., Chopra, A., et al. Humanity’s last exam, 2025. URL https://arxiv.org/abs/ 2501.14249.   \nQin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 replication journey: A strategic progress report – part 1, 2024. URL https://arxiv.org/abs/2410.18982.   \nQwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2024. URL https://arxiv.org/ abs/2412.15115.   \nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: A graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022.   \nShi, Q., Tang, M., Narasimhan, K., and Yao, S. Can language models solve olympiad programming?, 2024. URL https://arxiv.org/abs/2404.10952.   \nSnell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more efective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314.   \nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.   \nSu, H., Yen, H., Xia, M., Shi, W., Muennighof, N., yu Wang, H., Liu, H., Shi, Q., Siegel, Z. S., Tang, M., Sun, R., Yoon, J., Arik, S. O., Chen, D., and Yu, T. Bright: A realistic and challenging benchmark for reasoning-intensive retrieval, 2024. URL https://arxiv.org/abs/ 2407.12883.   \nSun, L., Han, Y., Zhao, Z., Ma, D., Shen, Z., Chen, B., Chen, L., and Yu, K. Scieval: A multi-level large language model evaluation benchmark for scientifc research, 2024. URL https://arxiv.org/abs/2308.13149.   \nTeam, D. Deepseek r1, November 2024a. URL https://x.com/deepseek_ai/status/ 1859200141355536422.   \nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L., Dong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen, Y., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599.   \nTeam, N. Sky-t1: Fully open-source reasoning model with o1-preview performance in $\\mathbb{S}450$ budget, 2025. URL https://novasky-ai.github.io/ posts/sky-t1. Accessed: 2025-01-09.   \nTeam, Q. Qwq: Refect deeply on the boundaries of the unknown, November 2024b. URL https://qwenlm. github.io/blog/qwq-32b-preview/.   \nWang, J., Meng, F., Liang, Y., and Zhou, J. Drt-o1: Optimized deep reasoning translation via long chain-ofthought, 2024a. URL https://arxiv.org/abs/ 2412.17498.   \nWang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024b. URL https://arxiv.org/abs/2312.08935.   \nWang, S., Liu, Z., Zhong, W., Zhou, M., Wei, Z., Chen, Z., and Duan, N. From lsat: The progress and challenges of complex reasoning, 2021. URL https://arxiv. org/abs/2108.00648.   \nWang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J. J., Sreedhar, M. N., and Kuchaiev, O. Helpsteer2: Open-source dataset for training top-performing reward models, 2024c. URL https://arxiv.org/ abs/2406.08673.   \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903.   \nWelleck, S., Bertsch, A., Finlayson, M., Schoelkopf, H., Xie, A., Neubig, G., Kulikov, I., and Harchaoui, Z. From decoding to meta-generation: Inference-time algorithms for large language models, 2024. URL https://arxiv. org/abs/2406.16838.   \nWu, T., Lan, J., Yuan, W., Jiao, J., Weston, J., and Sukhbaatar, S. Thinking llms: General instruction following with thought generation, 2024a. URL https: //arxiv.org/abs/2410.10630.   \nWu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of computeoptimal inference for problem-solving with language models, 2024b. URL https://arxiv.org/abs/2408. 00724.   \nXiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A., Blagden, C., Phung, D., Rafailov, R., Lile, N., Mahan, D., Castricato, L., Franken, J.-P., Haber, N., and Finn, C. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought, 2025. URL https: //arxiv.org/abs/2501.04682.   \nXie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He, J., and Xie, Q. Self-evaluation guided beam search for reasoning, 2023. URL https://arxiv.org/abs/ 2305.00633.   \nXin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan, C., Li, W., and Liang, X. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data, 2024. URL https://arxiv.org/abs/2405. 14333.   \nXu, H., Wu, X., Wang, W., Li, Z., Zheng, D., Chen, B., Hu, Y., Kang, S., Ji, J., Zhang, Y., Guo, Z., Yang, Y., Zhang, M., and Zhang, D. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025. URL https://arxiv.org/abs/2501.11284.   \nYang, Z., Band, N., Li, S., Candès, E., and Hashimoto, T. Synthetic continued pretraining, 2024. URL https: //arxiv.org/abs/2409.07431.   \nYao, S., Yu, D., Zhao, J., Shafran, I., Grifths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023a. URL https://arxiv.org/abs/2305.10601.   \nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023b. URL https://arxiv. org/abs/2210.03629.   \nYu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models, 2024. URL https://arxiv.org/abs/2309. 12284.   \nYuan, S., Chen, Z., Xi, Z., Ye, J., Du, Z., and Chen, J. Agent-r: Training language model agents to refect via iterative self-training, 2025. URL https://arxiv. org/abs/2501.11425.   \nZelikman, E., Wu, Y., Mu, J., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.   \nZelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. D. Quiet-star: Language models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/2403.09629.   \nZhang, H. and Chen, C. Test-time compute scaling laws, 2024. URL https://github.com/hughbzhang/ o1_inference_scaling_laws.   \nZhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J. B., and Gan, C. Planning with large language models for code generation, 2023. URL https://arxiv.org/ abs/2303.05510.   \nZhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and Sang, J. o1-coder: an o1 replication for coding, 2024a. URL https://arxiv.org/abs/2412.00154.   \nZhang, Y., Yang, J., Yuan, Y., and Yao, A. C.-C. Cumulative reasoning with large language models, 2024b. URL https://arxiv.org/abs/2308.04371.   \nZhong, H., Xiao, C., Tu, C., Zhang, T., Liu, Z., and Sun, M. Jec-qa: A legal-domain question answering dataset, 2019. URL https://arxiv.org/abs/1911.12011.   \nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: A human-centric benchmark for evaluating foundation models, 2023. URL https://arxiv.org/abs/2304. 06364.   \nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifes reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/2310.04406.   \nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more for alignment, 2023. URL https://arxiv.org/abs/ 2305.11206. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Contents ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "1 Introduction 1 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "2.1 Initial collection of 59K samples 2   \n2.2 Final selection of 1K samples . 2 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "3 Test-time scaling 3 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "3.1 Method 3   \n3.2 Metrics 4 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4 Results 4 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "4.1 Setup 4   \n4.2 Performance . 6 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "5 Ablations 6 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "5.1 Data Quantity, Diversity, and Difculty 6   \n5.2 Test-time scaling methods 6   \n6.1 Sample-efcient reasoning 8   \n6.2 Test-time scaling 9 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "A Evaluation determinism 16 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "B s1K details 16 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "B.1 s1K summary 16   \nB.2 Dataset composition for full 59K questions 16   \nB.3 s1K grading prompt . 18   \nB.4 s1K diversity selection 18   \nB.5 Decontamination 18 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "C Training details 20 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "C.1 Training Ablations: Sequence length 20   \nC.2 Training Samples 20 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "D Test-time scaling details 34 ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "D.1 Sequential scaling ablations 34   \nD.2 Examples for rejection sampling ablation . 36 ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "A. Evaluation determinism ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We run our evaluations using vLLM (Kwon et al., 2023) as it is faster than the alternatives we tried. However, we fnd that even when using the same random seeds and greedy sampling, evaluation scores can change signifcantly across runs: ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "• Diferent batch sizes causing diferent results see https://github.com/vllm-project/vllm/issues/ 5898   \n• Continuing generations causing diferent results see https://github.com/vllm-project/vllm/issues/ 11783   \n• Changes in tensor parallelism causing diferent results ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "As our model generates long reasoning traces prior to its answer, small numeric changes can snowball into large diferences. We encounter many generations that are exactly the same for thousands of tokens and then suddenly difer in one token eventually ending up with an entirely diferent answer. To partly counter this issue we generally run our fnal evaluations using full precision unless otherwise indicated. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "B. s1K details ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "B.1. s1K summary ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "table",
        "img_path": "images/217e1d099a45abc2e5b6a67d77b331760f85f7ec309cc42ec4a5a5ed58975259.jpg",
        "table_caption": [
            "Table 5. Summary of our dataset s1K. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given several questions from the domain. ",
            "B.2. Dataset composition for full 59K questions "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Domain</td><td>#questions</td><td>Total token count</td><td>Keywords</td></tr><tr><td>Geometry</td><td>109</td><td>560.2K</td><td>Area, Triangle, Distance</td></tr><tr><td>Number theory</td><td>98</td><td>522.5K</td><td>Sequences, Divisibility</td></tr><tr><td>Combinatorics</td><td>75</td><td>384.7K</td><td>Permutations, Counting</td></tr><tr><td>Real functions</td><td>43</td><td>234.8K</td><td>Trigonometry, Calculus</td></tr><tr><td>Biology</td><td>41</td><td>120.9K</td><td>Organic reactions</td></tr><tr><td>Complex functions</td><td>32</td><td>170.2K</td><td>Complex roots</td></tr><tr><td>Quantum theory</td><td>32</td><td>127.9K</td><td>Particles, Wave functions</td></tr><tr><td>Field theory</td><td>28</td><td>150.1K</td><td>Polynomials, Roots</td></tr><tr><td>Calculus of variations</td><td>28</td><td>155.5K</td><td>Optimization, Control</td></tr><tr><td>Difference equations</td><td>24</td><td>132.5K</td><td>Recurrence, Recursion</td></tr><tr><td>Electromagnetic theory</td><td>23</td><td>95.8K</td><td>Optics, Waves, Diffraction</td></tr><tr><td>Group theory</td><td>22</td><td>100.0K</td><td>Groups, Automorphisms</td></tr><tr><td>Linear algebra</td><td>22</td><td>128.3K</td><td>Matrices, Determinants</td></tr><tr><td>Probability theory</td><td>20</td><td>114.6K</td><td>Random walk, Expectation</td></tr><tr><td>Algebraic systems</td><td>19</td><td>109.9K</td><td>Functional equations</td></tr><tr><td>Mechanics</td><td>19</td><td>103.6K</td><td>Forces, Motion, Energy</td></tr><tr><td>Thermodynamics</td><td>19</td><td>74.2K</td><td>Heat engines, Entropy</td></tr><tr><td>Differential equations</td><td>18</td><td>89.6K</td><td>Substitution,Existence</td></tr><tr><td>Computer science</td><td>18</td><td>34.2K</td><td>Complexity theory, Algorithms</td></tr><tr><td>Numerical analysis</td><td>18</td><td>76.5K</td><td>Error analysis, Stability</td></tr><tr><td>Calculus</td><td>17</td><td>96.3K</td><td>Convergence, Summation</td></tr><tr><td>Algebraic structures</td><td>17</td><td>90.4K</td><td>Inequalities, Sets</td></tr><tr><td>Astronomy</td><td>16</td><td>37.7K</td><td>Stellar populations, Orbits</td></tr><tr><td>Remaining 27 domains</td><td>242</td><td>982.2K</td><td>Domains with ≤ 16 questions</td></tr><tr><td>All domains (51)</td><td>1000</td><td>4.7M</td><td>s1K</td></tr></table></body></html>\n\n",
        "page_idx": 15
    },
    {
        "type": "table",
        "img_path": "images/902158089823534f43efd67639292f6ab0a13c61e9f49a6df17e7b81abe84930.jpg",
        "table_caption": [
            "Table 6. Composition of full 59K questions. Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct tokenizer (Qwen et al., 2024). In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from 2022-2023 as we use these 90 questions during our development stage of s1-32B. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Source</td><td>Description</td><td>#Samples</td><td>Avg. thinking</td></tr><tr><td>NuminaMATH (LI et al., 2024)</td><td>Math problems from online websites</td><td>30660</td><td>length 4.1K</td></tr><tr><td>MATH (Hendrycks et al., 2021)</td><td>Math problems from competitions</td><td>11999</td><td>2.9K</td></tr><tr><td>OlympicArena (Huang et al., 2024a)</td><td>Astronomy, Biology, Chemistry, Com- puter Science, Geography, Math, and</td><td>4250</td><td>3.2K</td></tr><tr><td>OmniMath (Gao et al.,2024a) AGIEval (Zhong et al., 2023;</td><td>Physics olympiad questions Math problems from competitions</td><td>4238</td><td>4.4K</td></tr><tr><td>Ling et al., 2017; Hendrycks et al., 2021; Liu et al., 2020; Zhong</td><td>English, Law, Logic and Math prob- lems from the SAT, LSAT and other exams</td><td>2385</td><td>1.2K</td></tr><tr><td>et al., 2019; Wang et al., 2021) xword</td><td>Crossword puzzles</td><td>999</td><td>0.7K</td></tr><tr><td>OlympiadBench (He et al., 2024)</td><td>Math and Physics olympiad questions</td><td>896</td><td>3.9K</td></tr><tr><td>AIME (1983-2021)</td><td>American Invitational Mathematics Ex- amination</td><td>890</td><td>4.7K</td></tr><tr><td>TheoremQA (Chen et al., 2023)</td><td>Computer Science, Finance, Math, and Physics university-level questions relat-</td><td>747</td><td>2.1K</td></tr><tr><td>USACO (Shi et al., 2024)</td><td>ing to theorems Code problems from the USA Comput- 519 ing Olympiad</td><td></td><td>3.6K</td></tr><tr><td>JEEBench (Arora et al.,2023)</td><td>Chemistry, Math, and Physics problems used in the university entrance exami- nation of the Indian Institute of Tech-</td><td>515</td><td>2.9K</td></tr><tr><td>GPQA (Rein et al., 2023)</td><td>nology PhD-Level Science Questions</td><td>348</td><td>2.9K</td></tr><tr><td>SciEval (Sun et al., 2024)</td><td>Biology, Chemistry, and Physics prob- lems from various sources</td><td>227</td><td>0.7K</td></tr><tr><td>s1-prob</td><td>Stanford statistics qualifying exams</td><td>182</td><td>4.0K</td></tr><tr><td>LiveCodeBench (Jain et al., 2024)</td><td>Code problems from coding websites</td><td>151</td><td>3.5K</td></tr><tr><td>s1-teasers</td><td>(LeetCode, AtCoder, and CodeForces) Math brain-teasers crawled from the In-</td><td>23</td><td>4.1K</td></tr><tr><td>All 59K questions</td><td>ternet Composite of the above datasets with reasoning traces and solutions</td><td>59029</td><td>3.6K</td></tr></table></body></html>\n\n",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "B.3. s1K grading prompt ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "To grade whether an example is correct for our dataset selection in $\\S2$ , we use the prompt in Figure 8. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "You are an AI assistant for grading a science problem. The user will provide you with the question itself, an attempt made by a student and the correct answer to the problem. Your job is to judge whether the attempt is correct by comparing it with the correct answer. If the expected solution concludes with a number or choice, there should be no ambiguity. If the expected solution involves going through the entire reasoning process, you should judge the attempt based on whether the reasoning process is correct with correct answer if helpful. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "The user will provide the attempt and the correct answer in the following format: ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "# Problem {problem} ## Correct answer {solution} ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Explain your reasoning, and end your response on a new line with only \"Yes\" or \"No\" (without quotes). ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Figure 8. Grading prompt. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "B.4. s1K diversity selection ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Algorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in $\\S2$ , we also include samples from some specifc benchmarks we perceive as high-quality. None of the samples overlap with our fnal evaluation. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "B.5. Decontamination ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We flter all samples by checking for an 8-gram overlap between the selected examples and the evaluation benchmarks: MATH500, GPTQA Diamond, and AIME24. We exclude questions with more than an 8-gram overlap. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Algorithm 1 Two-stage sampling for s1K ",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "1: Input: $\\scriptstyle Q\\;:=\\;\\mathbf{S}\\mathrm{et}$ of 24,496 questions with features   \n2: Output: $S:=\\mathrm{Set}$ of 1,000 selected questions   \n3: $S\\gets\\emptyset$ Initialize the output set (only tracks unique elements)   \n4: for $q\\in\\mathcal{Q}$ do   \n5: if IsGeminiCorrect $(q)$ and (IsAIME $(q)$ or IsGPQA(𝑞)) then   \n6: $S\\leftarrow S\\cup\\{q\\}$   \n7: Select all correct AIME/GPQA solutions   \n8: else if IsGeminiCorrect $(q)$ and IsMATH(𝑞) and ThinkingLength $(q)>5600$ then   \n9: $S\\leftarrow S\\cup\\{q\\}$   \n10: Select correct MATH500 solutions with long chains   \n11: end if   \n12: end for   \n13: $D\\gets\\mathrm{All}$ available domains   \n14: Initialize domain pool   \n15: while $\\vert\\boldsymbol{S}\\vert<1000$ do   \n16: $d\\leftarrow$ R|andomChoice $(D)$   \n17: Randomly select a domain   \n18: $Q_{d}\\gets$ Questions in domain 𝑑   \n19: Get questions from this domain   \n20: ranks $\\leftarrow$ RankByThinkingLength $(Q_{d})$   \n21: Rank by thinking length   \n22: weights ← 2−ranks   \n23: Apply power-law weighting   \n24: $q\\leftarrow$ WeightedSample $\\boldsymbol{Q}_{d}$ , weights)   \n25: Sample favoring longer chains   \n26: $S\\leftarrow S\\cup\\{q\\}$   \n27: Add selected question   \n28: 𝑄 ← 𝑄 ⧵ {𝑞}   \n29: if 𝑄𝑑 = ∅ then   \n30: ← ⧵ {𝑑}   \n31: Remove exhausted domains   \n32: end if   \n33: end while ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "C. Training details ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "We take a model that has already been pretrained and instruction tuned and further fnetune it for reasoning. Specifcally, we use Qwen2.5-32B-Instruct (Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5- 72B-Instruct (Qwen et al., 2024) or other open models (Dubey et al., 2024; Groeneveld et al., 2024; Muennighof et al., 2024). We use token delimiters to separate the thinking stage from the answering stage. We enclose the thinking stage with $<|$ im_start $|>$ think and $<|$ im_start|>answer; both preceded and followed by a newline. Samples from our dataset are in $\\mathrm{\\SC}.2$ . We use basic fne-tuning hyperparameters: we train for 5 epochs with a batch size of 16 for a total of 315 gradient steps. We train in bfoat16 precision with a learning rate of $1e-5$ warmed up linearly for $5\\%$ (16 steps) and then decayed to 0 over the rest of training (299 steps) following a cosine schedule. We use the AdamW optimizer (Loshchilov & Hutter, 2019) with $\\beta_{1}=0.9,\\beta_{2}=0.95$ and weight decay of $1e-4$ . We do not compute loss on questions, only on reasoning traces and solutions. We ensure the sequence length is large enough to avoid cutting of any samples; a setting we ablate in $\\mathrm{\\SC.1}$ . The training takes just 26 minutes on 16 NVIDIA H100 GPUs. ",
        "page_idx": 19
    },
    {
        "type": "image",
        "img_path": "images/2f8acff808d5c545b08f09b3ddcd0b006a2884b6dc42e1ba3eaf20b2653b6b56.jpg",
        "img_caption": [
            "Figure 9. Training dynamics of s1-32B on s1K. "
        ],
        "img_footnote": [],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "C.1. Training Ablations: Sequence length ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "table",
        "img_path": "images/a32d830321193a2909c2fcc4550c74115e67c07158c455dc144efc3f8a5bbe17.jpg",
        "table_caption": [
            "Table 7. Training sequence length ablation. We report “accuracy / average thinking tokens per sample”; the higher the accuracy and the fewer the thinking tokens (inference cost) the better. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td></td><td>Model A</td><td>Model B</td></tr><tr><td>Training sequence length</td><td>4096</td><td>32768</td></tr><tr><td>% training samples cutoff</td><td>74%</td><td>0%</td></tr><tr><td>AIME24</td><td>30.0%/20721</td><td>50.0%/6984</td></tr><tr><td>MATH500</td><td>90.0%/5324</td><td>91.0%/3268</td></tr><tr><td>GPQA</td><td>52.5%/6841</td><td>53.0%/3568</td></tr></table></body></html>\n\n",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Besides our scaling ablations in $\\S5.2$ , the main training hyperparameter we ablate is the sequence length used during training. We fnd that a shorter training sequence length leads to longer reasoning traces at test time. This is because when training with a shorter sequence length the answer section of the training sample is more commonly cut of. Inversely, when the training sequence length is longer, more samples appear in their entirety with the section where the model answers. Thus the model receives more gradient updates where it learns to generate an answer following its chain. This in turn leads to a higher log probability of the answer section at any point during the generation and thus shorter reasoning traces at test time. Performance-wise, we also fnd that the model trained with a longer sequence length performs better. Thus we opt for the longest training sequence length as it leads to better performance and makes inference more efcient by leading to shorter reasoning traces. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "C.2. Training Samples ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Table 8, Table 9, Table 10 contain training samples from s1K. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Table 8. s1K sample stemming from s1-prob. The black text is the question and the yellow text is the solution. Based solely on the question we generate the light blue reasoning trace and blue response with Gemini, which we use for training. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "You are given integrable random variables $X,Y_{0}$ and $Z_{0}$ on the same probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ , and two $\\sigma$ -algebras $A\\subset F,B\\subset F$ . For $k=1,2,\\dots,$ , let ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\nY_{k}:=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},\\ldots,Z_{k-1})],\\quad Z_{k}:=\\mathbb{E}[X|\\sigma(\\mathcal{B},Y_{0},\\ldots,Y_{k-1})].\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "(a) Show that there exist integrable random variables $Y_{\\infty}$ and $Z_{\\infty}$ such that as $n~\\rightarrow~\\infty$ both $\\mathbb{E}[|Y_{n}-Y_{\\infty}|]\\,\\to\\,0$ and $\\mathbb{E}[|Z_{n}-Z_{\\infty}|]\\to0$ . (b) Prove that almost surely $Y_{\\infty}=Z_{\\infty}$ . ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "1. Introduce the notations, $\\mathcal{F}_{n}:=\\sigma(\\mathcal{A},Z_{0},\\ldots,Z_{n-1})$ and $\\mathcal{G}_{n}:=\\sigma(B,Y_{0},\\ldots,Y_{n-1})$ , for all $n\\geq1$ and $\\mathcal{F}_{0}:=\\mathcal{A},\\mathcal{G}_{0}:=\\mathcal{B}.$ Clearly, both are fltrations and by defnition, $Y_{n}=\\mathbb{E}\\left[X|\\mathcal{F}_{n}\\right]$ and $Z_{n}=\\mathbb{E}\\left[X|\\mathcal{G}_{n}\\right]$ , for all $n\\geq0$ . Thus $\\left\\{Y_{n}\\right\\}_{n\\ge0}$ (or $\\left\\{Z_{n}\\right\\}_{n\\ge0}\\,,$ ) is a Doob’s Martingale with respect to fltration $\\left\\{\\mathcal{F}_{n}\\right\\}_{n\\ge0}$ (or $\\left\\{\\mathcal{G}_{n}\\right\\}_{n\\ge0})$ and hence is U.I., $X$ being integrable. Now apply the fact that a Martingale is U.I. if and only if it converges in $L^{1}$ and conclude. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "2. We provide two diferent proofs. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "1. Observe the following identity which follows from Tower Property. ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[Y_{n}|\\mathcal{G}_{n}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[X|\\mathcal{F}_{n}\\right]|\\mathcal{G}_{n}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[X|\\mathcal{G}_{n}\\right]|\\mathcal{F}_{n}\\right]=\\mathbb{E}\\left[Z_{n}|\\mathcal{F}_{n}\\right].}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Note that 𝔼 𝔼 $\\left|Y_{n}|\\mathcal{G}_{n}\\right|-\\mathbb{E}\\left|Y_{\\infty}|\\mathcal{G}_{n}\\right||\\leq\\mathbb{E}|Y_{n}-Y_{\\infty}|=o(1)$ and by Levy’s Upward Theorem, $\\mathbb{E}\\left[Y_{\\infty}|\\mathcal{G}_{n}\\right]\\overset{L^{1}}{\\longrightarrow}\\mathbb{E}\\left[Y_{\\infty}|\\mathcal{G}_{\\infty}\\right]$ , where ${\\mathcal{G}}_{\\infty}=\\sigma\\left(\\cup_{n\\geq0}{\\mathcal{G}}_{n}\\right)=\\sigma(B,Y_{0},Y_{1}\\ldots)$ . It is |evident that $Y_{\\infty}\\in m\\mathcal{G}_{\\infty}$ (since it is almost sur|e limit of $Y_{n}\\in m\\boldsymbol{\\mathcal{G}}_{n},$ ) and hence $\\mathbb{E}\\left[Y_{\\infty}|\\mathcal{G}_{\\infty}\\right|=Y_{\\infty}$ almost surely. Combining the above observations we conclude that $\\mathbb{E}\\left[Y_{n}|\\mathcal{G}_{n}\\right]\\xrightarrow{L^{1}}Y_{\\infty}$ . Similarly, $\\mathbb{E}\\left[Z_{n}|\\mathcal{F}_{n}\\right]\\stackrel{L^{1}}{\\longrightarrow}Z_{\\infty}.$ . Now invoke Equation (4), to conclude that $Y_{\\infty}=Z_{\\infty}$ almost surely. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "2. Since $X$ is integrable, we have by Levy’s Upward Theorem, $\\begin{array}{r}{Y_{\\infty}\\,=\\,\\operatorname*{lim}_{n\\to\\infty}Y_{n}\\,=\\,\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}\\left|X|\\mathcal{F}_{n}\\right|\\,=\\,\\mathbb{E}\\left|X|\\mathcal{F}_{\\infty}\\right|.}\\end{array}$ Similarly, $Z_{\\infty}\\,=\\,\\mathbb{E}\\left[X|\\mathcal{G}_{\\infty}\\right].$ . On the otherhand, in frst proof we have observed that $Y_{\\infty}\\in\\,m\\mathcal{G}_{\\infty}$ and hence $Y_{\\infty}\\in$ $m\\mathcal{G}_{\\infty}\\cap m\\mathcal{F}_{\\infty}=m\\left(\\mathcal{G}_{\\infty}\\cap\\mathcal{F}_{\\infty}\\right)$ . This yields the following. ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\nY_{\\infty}=\\mathbb{E}\\left[Y_{\\infty}|\\mathcal{G}_{\\infty}\\cap\\mathcal{F}_{\\infty}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[X|\\mathcal{F}_{\\infty}\\right]|\\mathcal{G}_{\\infty}\\cap\\mathcal{F}_{\\infty}\\right]=\\mathbb{E}\\left[X|\\mathcal{G}_{\\infty}\\cap\\mathcal{F}_{\\infty}\\right].\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Similarly, $Z_{\\infty}=\\mathbb{E}\\left[X|\\mathcal{G}_{\\infty}\\cap\\mathcal{F}_{\\infty}\\right].$ This proves that $Y_{\\infty}=Z_{\\infty}$ almost surely. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Let’s analyze the problem. We are given two sequences of random variables $Y_{k}$ and $Z_{k}$ defned by conditional expectations.   \nWe need to show that these sequences converge in $L^{1}$ and that their limits are equal almost surely.   \nPart (a): Convergence in $L^{1}$ . ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "$\\mathcal{F}_{k}=\\sigma(\\mathcal{A},Z_{0},\\ldots,Z_{k-1})$ $\\mathcal{G}_{k}=\\sigma(B,Y_{0},\\ldots,Y_{k-1})$ For the $Y_{k}$ sequence: 𝑌1 = 𝔼[𝑋 𝜎( )] 𝑌2 = 𝔼[𝑋|𝜎( , 𝑍0)] 𝑌3 = 𝔼[𝑋|𝜎( , 𝑍0, 𝑍1)] ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "The sequence of sigma-algebras $\\sigma(\\mathcal{A},\\mathcal{L}_{0},\\ldots,\\mathcal{L}_{k-1})$ is increasing. By the Martingale Convergence Theorem (for conditional expectations), since $X$ is integrable, the sequence $Y_{k}$ converges almost surely and in $L^{1}$ to $\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ . Let $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)].$ . ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Similarly, for the $Z_{k}$ sequence: $Z_{1}=\\mathbb{E}[X|\\sigma(B)]$ $Z_{2}=\\mathbb{E}[X|\\sigma(B,Y_{0})]$ $Z_{3}=\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1})]$ The sequence of sigma-algebras $\\sigma(B,Y_{0},\\ldots,Y_{k-1})$ is increasing. By the Martingale Convergence Theorem, the sequence $Z_{k}$ converges almost surely and in $L^{1}$ to $\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1},...)]$ . Let $Z_{\\infty}=\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1},...)]$ .   \nPart (b): Prove that almost surely $Y_{\\infty}=Z_{\\infty}$ We have $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ and $Z_{\\infty}=\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1},\\ldots)]$ .   \nLet $\\mathcal{V}_{\\infty}=\\sigma(\\mathcal{A},Z_{0},Z_{1},\\ldots)$ and $\\mathcal{Z}_{\\infty}=\\sigma(B,Y_{0},Y_{1},...)$ .| Then $Y_{\\infty}=\\mathbb{E}[X|\\mathcal{D}_{\\infty}]$ and $Z_{\\infty}=\\mathbb{E}[X|\\mathcal{Z}_{\\infty}]$ .   \nConsider $Y_{1}=\\mathbb{E}[X|\\mathcal{A}]$ . $Z_{1}=\\mathbb{E}[X\\vert B]$ .   \n$Y_{2}=\\mathbb{E}[X|\\sigma(A,Z_{0})]$ . Since $Z_{0}=\\mathbb{E}[X|B]$ , $\\sigma(\\mathcal{A},Z_{0})=\\sigma(\\mathcal{A},\\mathbb{E}[X|\\mathcal{B}])$ .   \n$Z_{2}=\\mathbb{E}[X|\\sigma(B,Y_{0})]$ . Since $Y_{0}$ is not de|fned, let’s assume $Y_{0}$ and $Z_{0}$ are given.   \nLet’s consi|der the sigma-algebra generated by all the $Y_{i}$ and $Z_{i}$ .   \nLet ${\\mathcal{G}}=\\sigma(Y_{0},Z_{0},Y_{1},Z_{1},...)$ .   \nBoth $Y_{k}$ and $Z_{k}$ are measurable with respect to $\\boldsymbol{\\varsigma}$ . Thus, $Y_{\\infty}$ and $Z_{\\infty}$ are measurable with respect to $\\boldsymbol{\\varsigma}$ .   \nConsider $\\begin{array}{r}{\\mathbb{E}[Y_{n+1}|\\mathcal{A},Z_{0},\\ldots,Z_{n-1}]=\\mathbb{E}[\\mathbb{E}[X\\,|\\sigma(\\mathcal{A},Z_{0},\\ldots,Z_{n})]|\\mathcal{A},Z_{0},\\ldots,Z_{n-1}]=Y_{n}}\\end{array}$ .   \nConside $\\begin{array}{r}{\\ensuremath{\\mathbb{F}}[Z_{n+1}|\\mathcal{B},Y_{0},\\ldots,Y_{n-1}]=\\ensuremath{\\mathbb{E}}[\\ensuremath{\\mathbb{E}}[X|\\sigma(\\mathcal{B},Y_{0},\\ldots,Y_{n})]|\\mathcal{B},Y_{0},\\ldots,Y_{n-1}]=Z_{n}.}\\end{array}$ Let $\\mathcal{H}=\\sigma(\\mathcal{A}\\cup\\mathcal{B}\\cup\\sigma(Y_{0},Z_{0},Y_{1},Z_{1},...))$ .   \nConsider $\\mathbb{E}[X|\\mathcal{A},Z_{0},\\ldots,Z_{k-1}]=Y_{k}$ .   \nConsider $\\mathbb{E}[X|B,Y_{0},\\ldots,Y_{k-1}]=Z_{k}$ .   \nLet’s consider |the limit sigma-algebras $\\mathcal{D}_{\\infty}=\\sigma(\\mathcal{A},Z_{0},Z_{1},\\ldots)$ and $\\mathcal{Z}_{\\infty}=\\sigma(B,Y_{0},Y_{1},...).$ .   \nWe know that $Y_{\\infty}$ is $\\mathcal{P}_{\\infty}$ -measurable and $Z_{\\infty}$ is $\\mathcal{Z}_{\\infty}$ -measurable.   \nLet ${\\mathcal{C}}=\\sigma({\\mathcal{A}}\\cup{\\mathcal{B}})$ .   \n$Y_{1}=\\mathbb{E}[X\\vert A]$ . $Z_{1}=\\mathbb{E}[X\\vert B]$ .   \n$Y_{2}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0})]=\\mathbb{E}[X|\\sigma(\\mathcal{A},\\mathbb{E}[X|\\mathcal{B}])].$ $Z_{2}=\\operatorname{\\mathbb{E}}[X|\\sigma(B,Y_{0})]=\\operatorname{\\mathbb{E}}[X|\\sigma(B,\\operatorname{\\mathbb{E}}[X|\\sigma])]$ .   \nConsider th|e case w ${\\mathfrak{h e n}}\\,A$ a|nd $\\beta$ are in|dependent. Then $Y_{1}=\\mathbb{E}[X\\vert\\mathcal{A}]$ and $Z_{1}=\\mathbb{E}[X|B]$ .   \n$Y_{2}=\\mathbb{E}[X|\\sigma(\\mathcal{A},\\mathbb{E}[X|B])]$ .   \nLet’s cons|ider the sig|ma-algebra generated by all the information. Let $\\mathcal{F}_{\\infty}=\\sigma(\\mathcal{A},\\mathcal{B},Y_{0},Z_{0},Y_{1},Z_{1},...).$ Then both $Y_{\\infty}$ and $Z_{\\infty}$ are $\\boldsymbol{\\mathscr{F}}_{\\infty}$ -measurable.   \nWe have $Y_{\\infty}=\\operatorname*{lim}_{n\\to\\infty}Y_{n}$ and $Z_{\\infty}=\\dim_{n\\to\\infty}Z_{n}$ a.s. and in $L^{1}$ .   \nConsider $\\mathbb{E}[X|\\mathcal{A},Z_{0},Z_{1},\\ldots]=Y_{\\infty}$ and $\\mathbb{E}[X|B,Y_{0},Y_{1},...]=Z_{\\infty}$ .   \nLet $\\mathcal{H}_{\\infty}=\\sigma(\\mathcal{A}\\cup\\mathcal{B})$ .   \nConsider $\\mathbb{E}[X|\\mathcal{H}_{\\infty}]$ .   \nLet’s try to sho|w that both $Y_{\\infty}$ and $Z_{\\infty}$ are equal to $\\mathbb{E}[X|\\sigma(\\mathcal{A}\\vee B)]$ . This seems too simple.   \nLet’s consider the defnition of conditional expectation. |For any set $A\\in\\mathcal{V}_{\\infty}$ , $\\mathbb{E}[Y_{\\infty}1_{A}]=\\mathbb{E}[X1_{A}].$ .   \nFor any set $B\\in\\mathcal{Z}_{\\infty}$ , $\\mathbb{E}[Z_{\\infty}\\boldsymbol{1}_{B}]=\\mathbb{E}[\\boldsymbol{X}\\boldsymbol{1}_{B}]$ .   \nConsider $\\mathbb{E}[Y_{k+1}|\\mathcal{F}_{k}]=Y_{k}$ where $\\mathcal{F}_{k}=\\sigma(\\mathcal{A},\\mathcal{L}_{0},\\ldots,\\mathcal{L}_{k-1})$ .   \nConsider 𝔼 $:\\![Z_{k+1}|{\\mathcal G}_{k}]=Z_{k}$ where $\\mathcal{G}_{k}=\\sigma(B,Y_{0},\\ldots,Y_{k-1})$ .   \nConsider the sigm|a-algebra $\\sigma({\\mathcal{A}}\\cup{\\mathcal{B}})$ .   \nLet $W=\\mathbb{E}[X|\\sigma(A\\cup B)]$ .   \nWe have $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ .   \nSince $Z_{k}$ depends o|n $Y_{k-1}$ which depends on $Z_{k-2}$ etc., it is complex.   \nLet’s consider the case where $Y_{0}=\\mathbb{E}[X\\vert\\mathcal{A}]$ and $Z_{0}=\\mathbb{E}[X\\vert B]$ .   \nThen $Y_{1}=\\mathbb{E}[X\\vert A]=Y_{0}.$ .   \n$Z_{1}=\\mathbb{E}[X|B]=Z_{0}$ .   \nIf this were| the case, then $Y_{k}=Y_{0}$ and $Z_{k}=Z_{0}$ , so $Y_{\\infty}=Y_{0}$ and $Z_{\\infty}=Z_{0}$ .   \nWe need $Y_{0}=Z_{0}$ a.s., i.e., $\\mathbb{E}[X|\\mathcal{A}]=\\mathbb{E}[X|B]$ a.s. This is not generally true.   \nConsider the sigma-algebra gen|erated by al|l $Y_{i}$ and $Z_{i}$ , denoted by $\\mathcal{G}_{\\infty}=\\sigma(Y_{0},Z_{0},Y_{1},Z_{1},...)$ .   \nThen $\\mathcal{V}_{\\infty}=\\sigma(\\mathcal{A},Z_{0},Z_{1},...)\\subseteq\\sigma(\\mathcal{A},\\mathcal{G}_{\\infty}).$ .   \n$\\mathcal{Z}_{\\infty}=\\sigma(\\beta,Y_{0},Y_{1},...)\\subseteq\\sigma(\\beta,\\mathcal{G}_{\\infty})$ . Consider $\\mathbb{E}[X|\\mathcal{A},Z_{0},Z_{1},\\ldots]=Y_{\\infty}$ . This means for any $A\\in\\sigma(\\mathcal{A},Z_{0},Z_{1},...),$ $\\mathbb{E}[X1_{A}]=\\mathbb{E}[Y_{\\infty}1_{A}].$ Consider $\\mathbb{E}[X|B,Y_{0},Y_{1},...]=Z_{\\infty}.$ This means for any $B\\in\\sigma(B,Y_{0},Y_{1},...)$ , $\\mathbb{E}[X1_{B}]=\\mathbb{E}[Z_{\\infty}1_{B}]$ .   \nLet $\\mathcal{H}=\\sigma(\\mathcal{A},\\mathcal{B})$ .   \nConsider $\\mathbb{E}[X|\\mathcal{H}]$ .   \nConsider 𝔼[𝑌∞| , $Y_{0},Y_{1},...]=\\mathbb{E}[\\mathbb{E}[X|\\mathcal{A},Z_{0},Z_{1},...]|\\mathcal{B},Y_{0},Y_{1},...].$ .   \nLet’s use the fac|t that $Y_{\\infty}$ is $\\sigma(\\mathcal{A},Z_{0},Z_{1},...)$ -measurab|le and $Z_{\\infty}$ is $\\sigma(B,Y_{0},Y_{1},...)$ -measurable.   \nConsider the smallest sigma-algebra containing both $\\sigma(\\mathcal{A},Z_{0},Z_{1},\\ldots)$ and $\\sigma(B,Y_{0},Y_{1},...)$ .   \nThis is $\\sigma(\\mathcal{A}\\cup B\\cup\\{Y_{i}\\}_{i\\ge0}\\cup\\{Z_{i}\\}_{i\\ge0})$ .   \nLet’s try to show that $\\mathbb{E}[(Y_{\\infty}-Z_{\\infty})^{2}]=0$ .   \n$\\mathbb{E}[(Y_{\\infty}-Z_{\\infty})^{2}]=\\mathbb{E}[Y_{\\infty}^{2}]+\\mathbb{E}[Z_{\\infty}^{2}]-2\\mathbb{E}[Y_{\\infty}Z_{\\infty}].$ .   \nConsider the defnition of $Y_{k}$ and $Z_{k}$ .   \n$\\mathbb{E}[Y_{k+1}|\\mathcal{A},Z_{0},\\ldots,Z_{k-1}]=Y_{k}.$ .   \n$\\mathbb{E}[Z_{k+1}|B,Y_{0},\\ldots,Y_{k-1}]=Z_{k}$ .   \nConside|r $\\mathbb{E}[Y_{\\infty}|B,Y_{0},Y_{1},...]$ .   \nSince $Y_{\\infty}$ is me|asurable with respect to $\\sigma(\\mathcal{A},Z_{0},Z_{1},\\ldots).$ , this does not simplify easily.   \nLet’s assume $Y_{\\infty}\\neq Z_{\\infty}$ on a set of positive measure.   \nConsider the case where $\\mathcal{A}$ and $\\beta$ are trivial.   \nThen $Y_{k}=\\mathbb{E}[X|\\sigma(Z_{0},\\dots,Z_{k-1})]$ and $Z_{k}=\\mathbb{E}[X|\\sigma(Y_{0},\\ldots,Y_{k-1})]$ .   \nLet $\\mathcal{H}_{\\infty}=\\sigma(\\mathcal{A}\\cup\\mathcal{B}\\cup\\{Y_{i},Z_{i}\\}_{i\\ge0})$ .   \nConsider $\\mathbb{E}[X|\\mathcal{H}_{\\infty}]$ .   \nWe have $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ .   \nBy the properties of| conditional expectation, $\\|Y_{\\infty}|\\mathcal{A}\\|=\\mathbb{E}[\\mathbb{E}[X|\\mathcal{A},Z_{0},Z_{1},...]|\\mathcal{A}].$ Consider the sigma-algebra $D$ generated by all the |random variab|les involved.   \n$\\begin{array}{r}{D=\\sigma(\\mathcal{A}\\cup\\mathcal{B}\\cup\\{Y_{i}\\}_{i\\geq0}\\cup\\{Z_{i}\\}_{i\\geq0})}\\end{array}$ .   \nLet $M_{\\infty}=\\mathbb{E}[X|\\mathcal{A}\\vee B]$ .   \nConsider the lim|its. As $n\\rightarrow\\infty$ , $\\sigma({\\mathcal{A}},Z_{0},\\ldots,Z_{n-1})\\uparrow\\sigma({\\mathcal{A}},Z_{0},Z_{1},\\ldots).$ As $n\\to\\infty$ , $\\sigma(B,Y_{0},\\ldots,Y_{n-1})$ ↑ $\\sigma(B,Y_{0},Y_{1},...)$ .   \nLet $\\mathcal{V}_{\\infty}=\\sigma(\\mathcal{A},Z_{0},Z_{1},\\ldots)$ and $\\mathcal{Z}_{\\infty}=\\sigma(B,Y_{0},Y_{1},...)$ .   \nWe have $Y_{\\infty}=\\mathbb{E}[X|\\mathcal{D}_{\\infty}]$ and $Z_{\\infty}=\\mathbb{E}[X|\\mathcal{Z}_{\\infty}]$ .   \nConsider $\\mathbb{E}[Y_{\\infty}-Z_{\\infty}]^{2}=\\mathbb{E}[(Y_{\\infty}-X)+(X-Z_{\\infty})]^{2}$ $=\\mathbb{E}[(Y_{\\infty}-X)^{\\angle}]+\\mathbb{E}[(X-Z_{\\infty})^{\\angle}]+2\\mathbb{E}[(Y_{\\infty}-X)(X-Z_{\\infty})]$ = $:\\mathbb{E}[(X-\\mathbb{E}[X|\\mathcal{V}_{\\infty}])^{2}]+\\mathbb{E}[(X-\\mathbb{E}[X|\\mathcal{Z}_{\\infty}])^{2}]+2\\mathbb{E}$ [𝑌∞𝑋 − 𝑌∞𝑍∞ − 𝑋2 + 𝑋𝑍∞] = $:\\mathbb{E}[X^{2}]-\\mathbb{E}[Y_{\\infty}^{2}]+\\mathbb{E}[X^{2}]-\\mathbb{E}[Z_{\\infty}^{2}]+2\\mathbb{E}[Y_{\\infty}X]-$ 2𝔼[𝑌∞𝑍∞] − 2𝔼[𝑋2] + 2𝔼[𝑋𝑍∞] $=-\\mathbb{E}[Y_{\\infty}^{2}]-\\mathbb{E}[Z_{\\infty}^{2}]+2\\mathbb{E}[Y_{\\infty}^{2}]-2\\mathbb{E}[Y_{\\infty}Z_{\\infty}]+2\\mathbb{E}$ [𝑍2∞] $=\\mathbb{E}[Y_{\\infty}^{2}]+\\mathbb{E}[Z_{\\infty}^{2}]-2\\mathbb{E}[Y_{\\infty}Z_{\\infty}]$ Consider the sigma-algebra ${\\mathcal{C}}=\\sigma(A\\cup B)$ .   \nLet $W=\\mathbb{E}[X\\vert\\boldsymbol{C}]$ .   \nConsider the li|mit sigma-algebra generated by all the information.   \nLet $\\mathcal{F}_{\\infty}=\\sigma(\\mathcal{A},B,Y_{0},Z_{0},Y_{1},Z_{1},...)$ .   \nThen ${\\mathcal{V}}_{\\infty}\\subseteq{\\mathcal{F}}_{\\infty}$ and $\\mathcal{Z}_{\\infty}\\subseteq\\mathcal{F}_{\\infty}$ We have $\\mathbb{E}[X|\\mathcal{D}_{\\infty}]=Y_{\\infty}$ and $\\mathbb{E}[X|\\mathcal{Z}_{\\infty}]=Z_{\\infty}$ .   \nConsider $\\mathbb{E}[Y_{\\infty}|\\mathcal{Z}_{\\infty}]$ and $\\mathbb{E}[Z_{\\infty}|\\mathcal{D}_{\\infty}]$ .   \nLet $\\mathcal{H}=\\sigma(\\mathcal{A}\\cup\\mathcal{B})$ .   \nConsider $\\mathbb{E}[X|\\mathcal{H}]$ .   \nConsider the s|equence of pairs $(Y_{k},Z_{k})$ .   \nIf the limits are the same, then for any bounded measurable function $f,\\mathbb{E}[f(Y_{\\infty})]=\\mathbb{E}[f(Z_{\\infty})]$ .   \nConsider the sigma-algebra $D=\\sigma(A\\cup B\\cup\\sigma(Y_{0},Z_{0},\\ldots))$ .   \nThen both $Y_{\\infty}$ and $Z_{\\infty}$ are measurable with respect to $D$ .   \nConsider $\\mathbb{E}[(Y_{n}-Z_{n})^{2}]$ .   \nLet $\\mathcal{G}=\\sigma(\\mathcal{A}\\cup\\mathcal{B})$ .   \nConsider $W=\\mathbb{E}[X\\vert\\mathcal{G}]$ .   \nThen $\\mathbb{E}[W|\\mathcal{A}]=\\mathbb{E}[X|\\mathcal{A}]$ and $\\mathbb{E}[W|B]=\\mathbb{E}[X|B]$ . Consider the limit sigma-algebra ${\\mathcal{L}}=\\sigma(A\\cup B\\cup\\sigma(Y_{0},Z_{0},...))$ .   \nWe have $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ and $Z_{\\infty}=\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1},\\ldots)]$ .   \nLet $\\mathcal{M}=\\sigma(\\mathcal{A},Z_{0},Z_{1},\\ldots)\\vee\\sigma(\\mathcal{B},Y_{0},Y_{1},\\ldots).$ .   \nConsider $\\mathbb{E}[X|\\mathcal{M}]$ .   \nConsider the r|andom variable $X$ . Then $Y_{\\infty}=\\operatorname*{lim}_{n}\\mathbb{E}[X|\\mathcal{F}_{n}]$ and $Z_{\\infty}=\\operatorname*{lim}_{n}\\mathbb{E}[X|\\mathcal{G}_{n}].$ .   \nLet $\\mathcal{H}_{\\infty}=\\sigma(\\mathcal{A}\\cup\\mathcal{B}\\cup\\sigma(Y_{0},Z_{0},\\ldots))$ .   \nConsider $E=\\{\\omega:Y_{\\infty}(\\omega)\\neq Z_{\\infty}(\\omega)\\}$ . Suppose $P(E)>0$ .   \nConsider the conditional expectation with respect to the join of the sigma-algebras.   \nLet $\\mathcal{C}_{\\infty}=\\sigma(\\mathcal{A},B,Y_{0},Z_{0},\\ldots)$ .   \nConsider $\\mathbb{E}[X|C_{\\infty}]$ .   \nThen $\\mathbb{E}[\\mathbb{E}[X|\\mathcal{C}_{\\infty}]|\\mathcal{A},\\mathcal{Z}_{0},...]=\\mathbb{E}[X|\\mathcal{A},\\mathcal{Z}_{0},...]=Y_{n+1}$ .   \nAnd 𝔼[𝔼 $X|C_{\\infty}||B,Y_{0},...]=\\mathbb{E}[X|B,Y_{0},...]=Z_{n+1}$ .   \nSo $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ an|d $Z_{\\infty}=\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1},\\ldots)].$ .   \nLet $\\mathcal{F}_{\\infty}=\\sigma(\\mathcal{A},Z_{0},Z_{1},...)$ and $\\mathcal{G}_{\\infty}=\\sigma(B,Y_{0},Y_{1},...)$ .   \nConsider $\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]$ .   \nThen 𝔼[ $\\bar{=}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]|\\mathcal{F}_{\\infty}]=\\mathbb{E}[X|\\mathcal{F}_{\\infty}]=Y_{\\infty}.$ .   \nAnd $\\begin{array}{r}{\\mathbb{E}[\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]|\\mathcal{G}_{\\infty}]=\\mathbb{E}[X|\\mathcal{G}_{\\infty}]=Z_{\\infty}}\\end{array}$   \nThis means $Y_{\\infty}=\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]$ a|.s. and $Z_{\\infty}=\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]$ a.s.   \nTherefore, $Y_{\\infty}=Z_{\\infty}$ a.|s.   \nSolution:   \n(a) For the sequence $Y_{k}$ , the $\\sigma$ -algebras $\\mathcal{F}_{k}=\\sigma(\\mathcal{A},Z_{0},\\ldots,Z_{k-1})$ are increasing as $k$ increases. Since $X$ is integrable, by the Martingale Convergence Theorem for conditional expectations, the sequence $Y_{k}=\\mathbb{E}[X|\\mathcal{F}_{k}]$ converges almost surely and in $L^{1}$ to $Y_{\\infty}=\\mathbb{E}[X|\\sigma(\\mathcal{A},Z_{0},Z_{1},...)]$ . Similarly, for the sequence $Z_{k}$ , the $\\sigma$ -algebras $\\mathcal{G}_{k}=\\sigma(B,Y_{0},\\ldots,Y_{k-1})$ are increasing. Since $X$ is integr|able, by the Martingale Convergence Theorem, the sequence $Z_{k}=\\mathbb{E}[X|\\mathcal{G}_{k}]$ converges almost surely and in $L^{1}$ to $Z_{\\infty}=\\mathbb{E}[X|\\sigma(B,Y_{0},Y_{1},...)]$ .   \n(b) Let $\\mathcal{F}_{\\infty}=\\sigma(\\mathcal{A},Z_{0},Z_{1},...)$ and $\\mathcal{G}_{\\infty}=\\sigma(B,Y_{0},Y_{1},...)$ . We have $Y_{\\infty}=\\mathbb{E}[X|\\mathcal{F}_{\\infty}]$ and $Z_{\\infty}=\\mathbb{E}[X|\\mathcal{G}_{\\infty}]$ . Consider the $\\sigma$ -algebra ${\\mathcal{H}}={\\mathcal{F}}_{\\infty}\\vee{\\mathcal{G}}_{\\infty}=\\sigma({\\mathcal{A}},B,Y_{0},Z_{0},Y_{1},Z_{1},...)$ . Consider $W=\\mathbb{E}[X|\\mathcal{H}]$ .|   \nWe have $r_{\\infty}\\subseteq H$ , so $\\mathbb{E}[W|\\mathcal{F}_{\\infty}]=\\mathbb{E}[\\mathbb{E}[X|\\mathcal{H}]|\\mathcal{F}_{\\infty}]=\\mathbb{E}[X|\\mathcal{F}_{\\infty}]=Y_{\\infty}$ .   \nSince $W$ is $_{\\mathcal{H}}$ -measurable, $\\mathbb{E}[W|\\mathcal{F}_{\\infty}]=Y_{\\infty}$ im|plies that $Y_{\\infty}$ is the conditional expectation of $W$ given $\\mathbf{\\boldsymbol{\\mathscr{F}}_{\\infty}}$ . But since $Y_{\\infty}$ is already $\\boldsymbol{r}_{\\infty}$ -measurable, we have| $\\mathbb{E}[W|\\mathcal{F}_{\\infty}]=W$ if $W$ is $\\boldsymbol{r}_{\\infty}$ -measurable, which is not necessarily true.   \nHowever, we know that $\\mathbb{E}[Y_{\\infty}|\\mathcal{F}_{\\infty}]=Y_{\\infty}$ and $\\mathbb{E}[Z_{\\infty}|\\mathcal{G}_{\\infty}]=Z_{\\infty}$ .   \nConsider $\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]$ .   \nThen $\\begin{array}{r}{\\mathbb{E}[\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]|\\mathcal{F}_{\\infty}]=\\mathbb{E}[X|\\mathcal{F}_{\\infty}]=Y_{\\infty}.}\\end{array}$   \nThis implies |that $\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]=Y_{\\infty}$ almost surely.   \nSimilarly, $\\begin{array}{r}{\\mathbb{E}[\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]|\\mathcal{G}_{\\infty}]=\\mathbb{E}[X|\\mathcal{G}_{\\infty}]=Z_{\\infty}}\\end{array}$ .   \nThis implies that $\\mathbb{E}[X|\\mathcal{F}_{\\infty}\\vee\\mathcal{G}_{\\infty}]=Z_{\\infty}$ a|lmost surely.   \nTherefore, $Y_{\\infty}=Z_{\\infty}$ a|lmost surely.   \nFinal Answer: The fnal answer is $Y_{\\infty}=Z_{\\infty}$ ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Table 9. s1K sample stemming from s1-teasers. The black text is the question and the yellow text is the solution. Based solely on the question we generate the light blue reasoning trace and blue response with Gemini, which we use for training. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Alice and Bob each have a coin and fip it until they get a heads. If Bob fipped his coin more times than Alice, what is the expected number of times Alice fipped her coin? ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "4/3 ",
        "text_level": 1,
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "The problem asks for the expected number of times Alice fipped her coin, given that Bob fipped his coin more times than Alice.   \nLet $X$ be the number of fips Alice makes until she gets a head.   \nLet $Y$ be the number of fips Bob makes until he gets a head.   \n$X$ and $Y$ are independent random variables following a geometric distribution with parameter $p=0.5$ (probability of getting a head).   \nThe probability mass function for a geometric distribution is $P(K=k)=(1-p)^{k-1}p$ , for $k=1,2,\\dots.$   \nIn this case, $p=0.5$ , so $\\begin{array}{r}{P(K=k)=(0.5)^{k-1}(0.5)=(0.5)^{k}=\\frac{1}{2^{k}}}\\end{array}$ .   \nWe are given the condition that Bob fipped his coin more times than Alice, which means $Y>X$ .   \nWe want to fnd the expected value of $X$ given that $Y>X,$ i.e., $E[X|Y>X]$ .   \nThe formula for conditional expectation is $\\begin{array}{r}{E[X|A]=\\sum_{x}x P(X=x|A)}\\end{array}$ .   \nAlso, 𝑃 (𝑋 = 𝑥 𝐴) = 𝑃(𝑋=𝑥∩𝐴) .   \nIn our case, $A$ i|s the event $Y>X$ .   \nWe need to fnd $P(Y>X)$ .   \n$\\begin{array}{r}{P(Y>X)=\\sum_{x=1}^{\\infty}\\sum_{y=x+1}^{\\infty}P(X=x,Y=y)}\\end{array}$   \nSince $X$ and $Y$ are independent, $\\begin{array}{r}{P(X=x,Y=y)=P(X=x)P(Y=y)=\\frac{1}{2^{x}}\\frac{1}{2^{y}}=\\frac{1}{2^{x+y}}.}\\end{array}$   \n$\\begin{array}{r}{P(Y>X)=\\sum_{x=1}^{\\infty}\\sum_{y=x+1}^{\\infty}\\frac{1}{2^{x+y}}=\\sum_{x=1}^{\\infty}\\frac{1}{2^{x}}\\sum_{y=x+1}^{\\infty}\\frac{1}{2^{y}}}\\end{array}$   \nThe inner sum is a geometric series: $\\begin{array}{r}{\\sum_{y=x+1}^{\\infty}(\\frac{1}{2})^{y}=(\\frac{1}{2})^{x+1}+(\\frac{1}{2})^{x+2}+\\dots}\\end{array}$   \nThis is a geometric series with frst term $a=(\\frac{1}{2})^{x+1}$ and common ratio $\\begin{array}{r}{r=\\frac{1}{2}}\\end{array}$   \nThe sum is $\\begin{array}{r}{\\frac{a}{1-r}=\\frac{(\\frac{1}{2})^{x+1}}{1-\\frac{1}{2}}=\\frac{(\\frac{1}{2})^{x+1}}{\\frac{1}{2}}=(\\frac{1}{2})^{x}.}\\end{array}$   \nSo, $\\begin{array}{r}{P(Y>X)=\\sum_{x=1}^{\\infty}\\frac{1}{2^{x}}(\\frac{1}{2})^{x}=\\sum_{x=1}^{\\infty}(\\frac{1}{4})^{x}.}\\end{array}$ .   \nThis is another geometric series with frst term $\\begin{array}{r}{a=\\frac{1}{4}}\\end{array}$ and common ratio $\\begin{array}{r}{r=\\frac{1}{4}}\\end{array}$   \n$\\begin{array}{r}{\\frac{a}{1-r}=\\frac{\\frac{1}{4}}{1-\\frac{1}{4}}=\\frac{\\frac{1}{4}}{\\frac{3}{4}}=\\frac{1}{3}.}\\end{array}$   \nNow, we need to fnd $P(X=x\\cap Y>X)=P(X=x,Y>x).$   \n$P(X=x,Y>x)=P(X=x)P(Y>x)$ , due to independence.   \n$\\begin{array}{r}{P(Y>x)=\\sum_{y=x+1}^{\\infty}P(Y=y)=\\sum_{y=x+1}^{\\infty}\\frac{1}{2^{y}}=(\\frac{1}{2})^{x+1}+(\\frac{1}{2})^{x+2}+\\cdots=(\\frac{1}{2})^{x}.}\\end{array}$   \nSo, $\\begin{array}{r}{P(X=x,Y>x)=\\frac{1}{2^{x}}\\times(\\frac{1}{2})^{x}=(\\frac{1}{4})^{x},}\\end{array}$ .   \nNow, we can fnd $\\begin{array}{r}{P(X=x|Y>X)=\\frac{P(X=x\\cap Y>X)}{P(Y>X)}=\\frac{(\\frac{1}{4})^{x}}{\\frac{1}{3}}=3(\\frac{1}{4})^{x}.}\\end{array}$   \nFinally, we can calculate th|e conditional expectation:   \n$\\begin{array}{r}{E[X|Y>X]=\\sum_{x=1}^{\\infty}x P(X=x|Y>X)=\\sum_{x=1}^{\\infty}x\\cdot3(\\frac{1}{4})^{x}=3\\sum_{x=1}^{\\infty}x(\\frac{1}{4})^{x}.}\\end{array}$   \nCons|ider the ser ies $\\textstyle S=\\sum_{x=1}^{\\infty}x r^{x}$ , where $|r|<1$ .   \n$S=r+2r^{2}+3r^{3}+4r^{4}+\\dots$   \n$\\begin{array}{r}{\\frac{S}{r}=1+2r+3r^{2}+4r^{3}+\\dots}\\end{array}$   \nConsider the geometric series $\\begin{array}{r}{G=1+r+r^{2}+r^{3}+\\cdots=\\frac{1}{1-r}.}\\end{array}$   \nDiferentiating with respect to $r$ :   \n𝑑𝑑𝐺𝑟 = 1 + 2𝑟 + 3𝑟2 + 4𝑟3 + ⋯ = 𝑑𝑑𝑟 ( 11−𝑟 ) = (1−1𝑟)2 .   \nSo, $\\begin{array}{r}{\\frac S r=\\frac{1}{(1-r)^{2}}}\\end{array}$ , w1hich means $\\begin{array}{r}{S=\\frac{r}{(1-r)^{2}}}\\end{array}$ .   \nIn our case, 𝑟 = 4 . $\\begin{array}{r}{\\sum_{x=1}^{\\infty}x(\\frac{1}{4})^{x}=\\frac{\\frac{1}{4}}{(1-\\frac{1}{4})^{2}}=\\frac{\\frac{1}{4}}{(\\frac{3}{4})^{2}}=\\frac{\\frac{1}{4}}{\\frac{9}{16}}=\\frac{1}{4}\\times\\frac{16}{9}=\\frac{4}{9}.}\\end{array}$ Therefore, $\\begin{array}{r}{E[X|Y>X]=3\\times\\frac{4}{9}=\\frac{12}{9}=\\frac{4}{3}}\\end{array}$ .   \nAlternative appr|oach: Symmetry.   \nConsider the events $Y>X$ , $X>Y$ , and $X=Y$ .   \n$P(Y>X)=P(X>Y)$ due to symmetry.   \n𝑃 (𝑋 = 𝑌 ) $\\begin{array}{r}{=\\sum_{k=1}^{\\infty}P(X=k,Y=k)=\\sum_{k=1}^{\\infty}P(X=k)P(Y=k)=\\sum_{k=1}^{\\infty}(\\frac{1}{2^{k}})^{2}=\\sum_{k=1}^{\\infty}(\\frac{1}{4})^{k}=\\frac{\\frac{1}{4}}{1-\\frac{1}{4}}=\\frac{\\frac{1}{4}}{\\frac{3}{4}}=\\frac{1}{3}.}\\end{array}$ We have $P(Y>X)+P(X>Y)+P(X=Y)=1.$ $\\begin{array}{r}{P(Y>X)+P(Y>X)+\\frac{1}{3}=1.}\\end{array}$ .   \n$\\begin{array}{r}{2P(Y>X)=1-\\frac{1}{3}=\\frac{2}{3}.}\\end{array}$ .   \n$\\begin{array}{r}{P(Y>X)=\\frac{1}{3}}\\end{array}$ . This confrms our previous calculation.   \nLet $E[X|Y>X]=\\mu$ .   \nWe are lo|oking for the expected number of fips Alice made, given Bob fipped more times.   \nConsider the case where the frst fip for both is tails. Then the process restarts, and the condition remains.   \nLet $A_{1}$ be Alice’s frst fip, $B_{1}$ be Bob’s frst fip.   \n$P(A_{1}=H)=1/2$ , $P(A_{1}=T)=1/2$ .   \n$P(B_{1}=H)=1/2,P(B_{1}=T)=1/2$ .   \nCase 1: $A_{1}=H,B_{1}=H$ . $X=1,Y=1$ , so $Y>X$ is false.   \nCase 2: $A_{1}=H,B_{1}=T.$ Alice stops at 1, Bob continues. $Y>1$ .   \nCase 3: $A_{1}=T,B_{1}=H$ . Bob stops at 1, Alice continues.   \nCase 4: $A_{1}=T,B_{1}=T$ . Both continue.   \nConsider the conditional probabilities given $Y>X$ .   \nLet $E=E[X|Y>X]$ .   \nIf the frst fip|s are HH, the condition $Y>X$ is not met.   \nIf the frst fips are HT, Alice’s number of fips is 1.   \nIf the frst fips are TH, Bob’s number of fips is 1, Alice’s is at least 2, so $Y<X$ .   \nIf the frst fips are TT, the problem restarts, and the expected number of additional fips for Alice is still $E$ .   \nLet $N_{A}$ be the number of fips Alice makes, $N_{B}$ be the number of fips Bob makes.   \nWe are given $N_{B}>N_{A}$ .   \nConsider the frst fips: HH (prob 1/4): Condition not met.   \nHT (prob 1/4): Alice fips 1 time.   \nTH (prob 1/4): Bob fips 1 time, Alice $>1$ , so $N_{A}>N_{B}$ , condition not met.   \nTT (prob 1/4): Both continue.   \nLet $E$ be the expected number of Alice’s fips given Bob fipped more.   \nIf the frst fips are HT, Alice fipped 1 time, and the condition is met.   \nIf the frst fips are TT, the situation is the same as the beginning, so the expected number of additional fips for Alice is $E$ .   \nConsider the event $Y>X$ .   \nConditioning on the frst fips: 𝐸[𝑋 𝑌 > 𝑋] = 𝐸[𝑋 $|Y>X,H H|P(H H|Y>X)+E[X|Y>X,H T]P(H T|Y>X)+E[X|Y>X,T H]P(T H|Y>X)|<|Y|<X,$ 𝑋) +| 𝐸[𝑋 𝑌 > 𝑋, 𝑇 |𝑇 ]𝑃 (𝑇 𝑇 𝑌 > 𝑋) $P(Y>X|H H)=0.$ .   \n$P(Y>X|H T)=1.$ .   \n$P(Y>X|T H)=0$ .   \n$P(Y>X|T T)$ depends on the future fips.   \nLet’s use t|he defnition of conditional expectation directly.   \n$\\begin{array}{r}{E[X|Y>X]=\\sum_{x=1}^{\\infty}x P(X=x|Y>X)=\\sum_{x=1}^{\\infty}x\\frac{P(X=x,Y>x)}{P(Y>X)}}\\end{array}$ Cons|ider the pro babilities of the f|rst fips giv en $Y>X$ .   \n$\\begin{array}{r}{P(H H|Y>X)=\\frac{P(Y>X|H H)P(H H)}{P(Y>X)}=\\frac{0\\cdot1/4}{1/3}=0}\\\\ {P(H T|Y>X)=\\frac{P(Y>X|H T)P(H T)}{P(Y>X)}=\\frac{1\\cdot1/4}{1/3}=\\frac{3}{4}.}\\end{array}$ $\\begin{array}{r l}&{P(T H|Y>X)=\\frac{P(Y>X|T H)P(T H)}{P(Y>X)}=\\frac{0\\cdot1/4}{1/3}=0.}\\\\ &{P(T T|Y>X)=\\frac{P(Y>X|T T)P(T T)}{P(Y>X)}}\\end{array}$   \nIf the f| rst fips are TT, the future behavior is the same, and the condition $Y>X$ holds if Bob takes more fips than Alice in the subsequent trials.   \n$P(Y>X|T T)=P(\\mathrm{Bo}|$ b needs more fips than Alice starting from scratch) $=P(Y>X)=1/3.$   \n$\\begin{array}{r}{P(T T|Y>X)=\\frac{(1/3)\\cdot(1/4)}{1/3}=\\frac{1}{4}.}\\end{array}$   \nSum o|f conditional probabilities: $0+3/4+0+1/4=1.$ .   \nNow consider the conditional expectations of $X$ :   \n$E[X|Y>X,H H]$ : Not relevant since $P(H H|Y>X)=0$ .   \n$E[X|Y>X,H T]$ : Given HT and $Y>X$ , Alic|e fipped 1 time. $E=1$ .   \n$E[X|Y>X,T H]$ : Not relevant.   \n$E[X|Y>X,T T]$ : Given TT and $Y>X$ , Alice has already fipped once, and the expected number of additional fips is $E$ . So, th|e total number of fips is $1+E$ .   \n$E=0\\cdot0+1\\cdot(3/4)+$ undefned ⋅ $0+(1+E)\\cdot(1/4)$   \n$E=0+3/4+0+1/4+E/4$   \n$E=1+E/4$   \n$E-E/4=1$   \n$3E/4=1$   \n$E=4/3$ .   \nThis confrms the previous result.   \nLet’s think about the intuition. If we have the condition $Y>X,$ , it means Alice was \"lucky\" and got a head early, or Bob was \"unlucky\".   \nConsider the random variable $Z=Y-X$ . We are given $Z>0$ .   \nWe want $E[X|Z>0]$ ].   \nLet $p=P(\\mathrm{head})=0.5$ .   \nThe number of tails before the frst head follows a geometric distribution with parameter $p$ .   \n$X-1\\sim G e o m(p)$ , $Y-1\\sim G e o m(p).$ .   \n$P(X-1=k)=(1-p)^{k}p.$ .   \n$P(X=x)=(1-p)^{x-1}p.$ .   \nConsider the joint distribution of $X$ and $Y$ .   \n$P(X=x,Y=y)=p^{2}(1-p)^{x-1}(1-p)^{y-1}$   \nConsider the event $Y=X+k$ , where $k\\geq1$ .   \n$\\begin{array}{r l}&{P(Y=X+k)=\\sum_{x=1}^{\\infty}P(X=x,Y=x+k)=\\sum_{x=1}^{\\infty}p^{2}(1-p)^{x-1}(1-p)^{x+k-1}}\\\\ &{=p^{2}(1-p)^{k-1}\\sum_{x=1}^{\\infty}((1-p)^{2})^{x-1}=p^{2}(1-p)^{k-1}\\frac{1}{1-(1-p)^{2}}}\\\\ &{=p^{2}(1-p)^{k-1}\\frac{1}{1-(1-2p+p^{2})}=p^{2}(1-p)^{k-1}\\frac{1}{2p-p^{2}}=p^{2}(1-p)^{k-1}\\frac{1}{p(2-p)}}\\\\ &{=\\frac{p(1-p)^{k-1}}{2-p}.}\\end{array}$   \n$p=0.5$   \n$\\begin{array}{r l}&{^{11}\\emph{p}\\!=\\!\\cup\\!..,\\!\\mathrm{uncn~1}-\\rho=\\!\\cup\\!...,z-\\rho=1...).}\\\\ &{\\ P(Y=X+k)=\\frac{0.5(0.5)^{k-1}}{1.5}=\\frac{(0.5)^{k}}{1.5}=\\frac{(1/2)^{k}}{3/2}=\\frac{1}{2^{k}}\\cdot\\frac{2}{3}=\\frac{1}{3\\cdot2^{k-1}}.}\\\\ &{\\mathrm{Check~if~}\\sum_{k=1}^{\\infty}P(Y=X+k)=P(Y>X)=1/3.}\\\\ &{\\sum_{k=1}^{\\infty}\\frac{1}{3\\cdot2^{k-1}}=\\frac{1}{3}\\sum_{k=1}^{\\infty}(\\frac{1}{2})^{k-1}=\\frac{1}{3}(1+1/2+1/4+...)=\\frac{1}{3}\\frac{1}{1-1/2}=\\frac{1}{3}\\cdot2=\\frac{2}{3}.}\\end{array}$   \nSomething is wrong here.   \nLet’s recalculate $P(Y>X)$ .   \n$\\begin{array}{r}{P(Y>X)=\\sum_{x=1}^{\\infty}\\sum_{y=x+1}^{\\infty}(1-p)^{x-1}p(1-p)^{y-1}p}\\end{array}$   \n$\\begin{array}{r}{=p^{2}\\sum_{x=1}^{\\infty}(1-p)^{x-1}\\sum_{y=x+1}^{\\infty}(1-p)^{y-1}}\\end{array}$   \nInner sum: (1 − 𝑝)𝑥 + (1 − 𝑝)𝑥+1 + ⋯ = (1−𝑝)𝑥 = (1−𝑝)𝑥 .   \n$\\begin{array}{r}{P(Y>X)=p^{2}\\sum_{x=1}^{\\infty}(1-p)^{x-1}\\frac{(1-p)^{x}}{p}=p(1-p)\\sum_{x=1}^{\\infty}(1-p)^{2(x-1)}}\\end{array}$   \n= 𝑝(1 − 𝑝) ∑𝑥∞=1((1 − 𝑝)2)𝑥−1 = 𝑝(1 − 𝑝) 1−(11−𝑝)2   \n$\\begin{array}{r}{=p(1-p)\\frac{1}{1-(1-2p+p^{2})}=p(1-p)\\frac{1}{2p-p^{2}}=p(1-p)\\frac{1}{p(2-p)}=\\frac{1-p}{2-p}.}\\end{array}$   \nIf $p=0.5$ , $\\begin{array}{r}{P(Y>X)=\\frac{0.5}{1.5}=\\frac{1}{3}.}\\end{array}$ . Correct. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 26
    },
    {
        "type": "equation",
        "text": "$P(X=x)=(1-p)^{x-1}p.$ $$\n\\begin{array}{r l}&{P(X=x)=\\frac{1}{2}\\times10^{-9}\\times10^{-11}}\\\\ &{P(X=x)=\\sum_{k=1}^{\\infty}\\alpha_{k}\\left(1-p\\right)^{k}\\left(1-p\\right)^{k}\\left(1-(1-p)^{k+1}+(1-p)^{k+1}+\\cdots\\right)=p_{1}^{k+1.9}\\alpha_{1}^{\\prime}=1-p^{k}\\left(1-p\\right)^{k}}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=1-p^{k}\\left(1-p\\right)^{k}\\alpha_{1}^{\\prime}=p_{2}-p^{k}\\left(1-p\\right)^{k+1}\\;,}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq X=\\sum_{k=1}^{\\infty}\\alpha_{1}^{\\prime}=1-p^{k}\\left(1-p\\right)^{k}\\alpha_{1}^{\\prime}=p^{k}\\left(1-p\\right)^{k+1}}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=1-p^{k}\\left(1-p\\right)^{k}\\alpha_{1}^{\\prime}=10^{-9}\\dots}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=10^{-9}\\dots\\quad\\frac{1}{2}\\times10^{-11}}\\\\ &{\\qquad\\quad\\times117\\times10^{-8}p\\leq\\alpha_{1}^{\\prime}=10^{-8}p\\leq\\alpha_{1}^{\\prime}x\\geq p(1-p)}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p\\leq\\alpha_{1}^{\\prime}=10^{-9}}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p\\leq\\alpha_{1}^{\\prime}=00^{-9}\\dots\\quad}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p}\\\\ &{\\qquad\\quad\\times10^{-1}p\\leq\\alpha_{1}^{\\prime}=10^{-9}p\\leq\\alpha_{1}^ \n$$",
        "text_format": "latex",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Solution: ",
        "text_level": 1,
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Let $X$ be the number of fips Alice makes until she gets a head, and $Y$ be the number of fips Bob makes until he gets a head. $X$ and $Y$ are independent geometric random variables with parameter $p=0.5$ . The probability mass function is $P(K=k)=(1-p)^{k-1}p$ . We are interested in the expected number of times Alice fipped her coin, given that Bob fipped his coin more times than Alice, i.e., $E[X|Y>X]$ . Using the defni|tion of conditional expectation, $\\begin{array}{r}{E[X|Y>X]=\\sum_{x=1}^{\\infty}x P(X=x|Y>X).}\\end{array}$ $\\begin{array}{r}{P(X=x|Y>X)=\\frac{P(X=x,Y>x)}{P(Y>X)}.}\\end{array}$ First, calc|ulate $P(Y>X)$ . $\\begin{array}{r}{P(Y>X)=\\sum_{x=1}^{\\infty}\\sum_{y=x+1}^{\\infty}P(X=x,Y=y)=\\sum_{x=1}^{\\infty}P(X=x)P(Y>x).}\\end{array}$ $\\begin{array}{r}{P(X=x)=(0.5)^{x-1}(0.5)=(0.5)^{x}=\\frac{1}{\\gamma x}.}\\end{array}$ $\\begin{array}{r}{P(Y>x)=\\sum_{y=x+1}^{\\infty}(0.5)^{y-1}(0.5)=(0.5)^{x}.}\\end{array}$ $\\begin{array}{r}{P(Y>X)=\\sum_{x=1}^{\\infty}\\frac{1}{2^{x}}(\\frac12)^{x}=\\sum_{x=1}^{\\infty}(\\frac14)^{x}=\\frac{1/4}{1-1/4}=\\frac{1/4}{3/4}=\\frac{1}{3}.}\\end{array}$ Next, ca $\\begin{array}{r}{\\mathrm{1culate~}P(X=x,Y>x)=P(X=x)P(Y>x)=\\frac{1}{2^{x}}(\\frac{1}{2})^{x}=(\\frac{1}{4})^{x}.}\\end{array}$ Now, fnd $\\begin{array}{r}{P(X=x|Y>X)=\\frac{P(X=x,Y>x)}{P(Y>X)}=\\frac{(1/4)^{x}}{1/3}=3(\\frac{1}{4})^{x}}\\end{array}$ . Finally, calculate th|e conditional expectation: $\\begin{array}{r}{E[X|Y>X]=\\sum_{x=1}^{\\infty}x P(X=x|Y>X)=\\sum_{x=1}^{\\infty}x\\cdot3(\\frac{1}{4})^{x}=3\\sum_{x=1}^{\\infty}x(\\frac{1}{4})^{x}.}\\end{array}$ $\\begin{array}{r}{\\sum_{x=1}^{\\infty}x r^{x}={\\frac{r}{(1-r)^{2}}}}\\end{array}$ $r=1/4$ $\\begin{array}{r}{\\sum_{x=1}^{\\infty}x(\\frac{1}{4})^{x}=\\frac{1/4}{(1-1/4)^{2}}=\\frac{1/4}{(3/4)^{2}}=\\frac{1/4}{9/16}=\\frac{1}{4}\\times\\frac{16}{9}=\\frac{4}{9}.}\\end{array}$ $\\begin{array}{r}{E[X|Y>X]=3\\times\\frac{4}{9}=\\frac{12}{9}=\\frac{4}{3}}\\end{array}$ . Final| Answer: The fnal answer is 4∕3 ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Table 10. s1K sample stemming from the astronomy subset of OlympicArena. The black text is the question and the blue text is the solution. Based solely on the question we generate the light blue reasoning trace and blue response with Gemini, which we use for training. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "An often-repeated fun fact is that humans produce more power per unit volume than stars. If the sun were the same size, but it produced the same amount of power per unit volume as a human, what would its surface temperature be? Assume the \"average human\" produces 100 watts of power and has a volume of 66400 cubic centimeters. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "A: 3500 K B: 10000 K C: 25000 K D: 40000 K E: 50000 K ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Using the numbers from the problem, the average human produces ",
        "page_idx": 28
    },
    {
        "type": "equation",
        "text": "$$\nu={\\frac{100}{66400\\times10^{-6}}}=1506{\\mathrm{~W/m}}^{3}\n$$",
        "text_format": "latex",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "The volume of the sun is $\\begin{array}{r}{\\frac{4}{3}\\pi R_{\\odot}^{3}}\\end{array}$ , so its new power output would be $\\begin{array}{r}{P=\\frac{4}{3}\\pi R_{\\odot}^{3}u}\\end{array}$ . To fnd the equilibrium temperature, we use ",
        "page_idx": 28
    },
    {
        "type": "equation",
        "text": "$$\nP=\\sigma A T^{4}=4\\pi\\sigma R_{\\odot}^{2}T^{4}\n$$",
        "text_format": "latex",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Solving for $T$ , we get ",
        "text_level": 1,
        "page_idx": 28
    },
    {
        "type": "equation",
        "text": "$$\nT=\\sqrt[4]{\\frac{R_{\\odot}u}{3\\sigma}}=\\sqrt[4]{\\frac{\\left(6.96\\times10^{8}\\right)\\left(1506\\right)}{3\\left(5.67\\times10^{-8}\\right)}}=49823\\approx50000\\,\\mathrm{K}.\n$$",
        "text_format": "latex",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "The problem asks for the surface temperature of the sun if it had the same size but produced power per unit volume as a human.   \nFirst, let’s fnd the power density of a human.   \nPower produced by a human, $P_{h u m a n}=100$ W   \nVolume of a human, $V_{h u m a n}=66400\\;\\mathrm{cm}^{3}$   \nConvert the volume to cubic meters:   \n$1\\ \\mathrm{m}=100\\ \\mathrm{cm}$   \n$\\begin{array}{r l}&{\\mathrm{~1~m^{3}=(100\\,c m)^{3}=10^{6}~c m^{3}}}\\\\ &{V_{h u m a n}=66400\\,c\\mathrm{m^{3}}=66400\\times10^{-6}\\mathrm{\\,m^{3}}=0.0664\\mathrm{\\,m^{3}}}\\\\ &{\\mathrm{Power\\,density\\,\\,of\\,a\\,human},\\,\\rho_{p o w e r,h u m a n}=\\frac{P_{h u m a n}}{V_{h u m a n}}=\\frac{100\\mathrm{\\,W}}{0.0664\\mathrm{\\,m^{3}}}}\\\\ &{\\rho_{p o w e r,h u m a n}=1506.024\\mathrm{\\,W/m^{3}}}\\end{array}$   \nNow, consider the sun with the same size as it currently is, but with a power density equal to that of a human.   \nWe need the radius of the sun to calculate its volume. The average radius of the sun is $R_{s u n}=6.95\\times10^{8}\\ \\mathrm{m}$ .   \nVolume of the sun, $V_{s u n}=\\textstyle{\\frac{4}{3}}\\pi R_{s u n}^{3}=\\textstyle{\\frac{4}{3}}\\pi(6.95\\times10^{8}\\mathrm{~m})^{3}$   \n$V_{s u n}={\\textstyle{\\frac{4}{3}}}\\pi(337.159375\\times10^{24})\\;\\mathrm{m}^{3}$   \n𝑉𝑠𝑢𝑛 = 1410.24 × 1024 × 𝜋 m3   \n$V_{s u n}=4.4307\\times10^{27}~\\mathrm{m}^{3}$   \nIf the sun had the same power density as a human, the total power produced by this modifed sun would be:   \n$P_{m o d i f i e d\\_s u n}=\\rho_{p o w e r,h u m a n}\\times V_{s u n}$   \n𝑃 $\\phantom{}_{m o d i f i e d\\_s u n}=1506.024\\,\\mathrm{W}/\\mathrm{m}^{3}\\times4.4307\\times10^{27}\\,\\mathrm{m}$ 3   \n$P_{m o d i f i e d\\_s u n}=6.6726\\times10^{30}$ W   \nThe surface temperature of a star is related to its luminosity (total power output) through the Stefan-Boltzmann law:   \n$L=4\\pi R^{2}\\sigma T^{4}$   \nwhere $L$ is the luminosity (total power output), $R$ is the radius of the star, $\\sigma$ is the Stefan-Boltzmann constant, and $T$ is the   \nsurface temperature.   \nIn this case, the luminosity of the modifed sun is $P_{m o d i f i e d\\_s u n}$ . The radius of the modifed sun is the same as the actual sun, 𝑅𝑠𝑢𝑛.   \nThe Stefan-Boltzmann constant, $\\sigma=5.67\\times10^{-8}\\ \\mathrm{Wm^{-2}K^{-4}}$ . $P_{m o d i f i e d\\_s u n}=4\\pi R_{s u n}^{2}\\sigma T^{4}$   \n$6.6726\\times10^{30}\\,\\,\\mathrm{W}=4\\pi(6.95\\times10^{8}\\,\\,\\mathrm{m})^{2}\\times5.67\\times10^{-8}\\,\\,\\mathrm{W}\\mathrm{m}^{-2}\\mathrm{K}^{-4}\\times T^{4}$   \nCalculate the term $4\\pi R_{s u n}^{2}\\sigma$ : $R_{s u n}^{2}=(6.95\\times10^{8})^{2}=48.3025\\times10^{16}\\;\\mathrm{m}^{2}$   \n$4\\pi R_{s u n}^{2}=4\\pi\\times48.3025\\times10^{16}=607.08\\times10^{16}\\,\\,\\mathrm{m}^{2}$   \n$4\\pi R_{\\mathrm{e}\\imath n}^{2}\\sigma=607.08\\times10^{16}\\,\\mathrm{m}^{2}\\times5.67\\times10^{-8}\\,\\mathrm{Wm}^{-2}\\mathrm{K}^{-4}$ $4\\pi R_{s u n}^{2}\\sigma=3441.93\\times10^{8}\\;\\mathrm{WK}^{-4}=3.44193\\times10^{11}\\;\\mathrm{WK}^{-4}$   \nNow, substitute this back into the equation:   \n$6.6726\\times10^{30}=3.44193\\times10^{11}\\times T^{4}$   \nSolve for 𝑇 4:   \n$\\begin{array}{r}{T^{4}=\\frac{6.6726\\times10^{30}}{3.44\\underline{{19}}3\\times10^{11}}=1.9386\\times10^{19}}\\end{array}$   \nSolve for $T$ :   \n$T=(1.9386\\times10^{19})^{1/4}=(193.86\\times10^{17})^{1/4}$   \n$T=(1.9386)^{1/4}\\times(10^{19})^{1/4}=(1.9386)^{0.25}\\times10^{4.75}$   \nCalculate (1.9386)0.25:   \nLet $y=1.9386^{0.25}$ , then $\\ln y=0.25\\ln(1.9386)$ $\\ln(1.9386)\\approx0.6619$ $\\ln y=0.25\\times0.6619=0.165475$   \n$y=e^{0.165475}\\approx1.1799$   \nSo, $T=1.1799\\times10^{4.75}=1.1799\\times10^{4}\\times10^{0.75}$ $10^{0.75}=10^{3/4}=(10^{3})^{1/4}=(1000)^{1/4}$   \nWe know $5^{4}=625$ and $6^{4}=1296.$ , so $(1000)^{1/4}$ is between 5 and 6.   \nAlternatively, $10^{0.75}=10^{0.5}\\times10^{0.25}=\\sqrt{10}\\times10^{0.25}\\approx3.16\\times1.778=5.62$   \n$T=1.1799\\times10^{4}\\times5.62=6.63\\times10^{4}\\;\\mathrm{K}=66300\\;\\mathrm{K}$   \nLet’s double-check the calculations.   \nPower density of human: $\\rho_{p o w e r,h u m a n}=1506.02\\;\\mathrm{W}/\\mathrm{m}^{3}$   \nVolume of sun: $V_{s u n}=4.43\\times10^{27}~\\mathrm{m}^{3}$   \nPower of modifed sun: $P_{m o d i\\,f i e d\\ s u n}=1506.02\\times4.43\\times10^{27}=6671.67\\times10^{27}=6.67\\times10^{30}\\ \\mathrm{W}$   \nStefan-Boltzmann law: $P=4\\pi R^{2}\\sigma T^{4}$   \n𝑇 4 = 4𝜋𝑃𝑅2𝜎   \n$4\\pi R_{s u n}^{2}\\sigma=3.44\\times10^{11}$ = 63..6474××110011 = 1.939 × 1019   \n$T=(1.939\\times10^{19})^{1/4}=(193.9\\times10^{17})^{1/4}$ $T=(193.9)^{0.25}\\times(10^{17})^{0.25}=(193.9)^{0.25}\\times10^{4.25}$   \nCalculate (193.9)0.25:   \nLet $y=193.9^{0.25}$ , then $\\ln y=0.25\\ln(193.9)$   \n$\\ln(193.9)\\approx5.267$ $\\ln y=0.25\\times5.267=1.31675$   \n$y=e^{1.31675}\\approx3.73$ $T=3.73\\times10^{4.25}=3.73\\times10^{4}\\times10^{0.25}=37300\\times1.778=66329\\;\\mathrm{K}$   \nThe result is around 66000 K. This is not among the options. Let’s review the assumptions and calculations.   \nAssumption: The sun radiates as a black body, which is reasonable for estimating surface temperature. Consider potential errors in calculations.   \nLet’s try a simpler approach by setting up ratios.   \nThe power per unit volume of the modifed sun is equal to that of a human. $\\begin{array}{r}{\\frac{P_{m o d i f i e d\\_s u n}}{V_{s u n}}=\\frac{P_{h u m a n}}{V_{h u m a n}}}\\end{array}$   \nWe also know $P_{m o d i f i e d\\_s u n}=4\\pi R_{s u n}^{2}\\sigma T_{m o d i f i e d\\_s u n}^{4}.$   \n$\\begin{array}{r l}&{\\frac{4\\pi R_{s u n}^{2}\\sigma T_{m o d i f i e d\\_s u n}^{4}}{\\frac{4}{3}\\pi R_{s u n}^{3}}=\\frac{P_{h u m a n}}{V_{h u m a n}}}\\\\ &{\\frac{3\\sigma T_{m o d i f i e d\\_s u n}^{4}}{R_{s u n}}=\\rho_{p o w e r\\_h u m a n}}\\\\ &{T_{m o d i f i e d\\_s u n}^{4}=\\frac{\\rho_{p o w e r\\_h u m a n}R_{s u n}}{3\\sigma}}\\end{array}$   \nSubstitute the values:   \n𝜌𝑝𝑜𝑤𝑒𝑟,ℎ𝑢𝑚𝑎𝑛 = 1506.02 W∕m3   \n𝑅𝑠𝑢𝑛 = 6.95 × 108 m   \n𝜎 = 5.67 × 10−8 Wm−2K−4   \n𝑇𝑚4𝑜𝑑𝑖𝑓𝑖𝑒𝑑_𝑠𝑢𝑛 $\\begin{array}{r}{T_{m o d i f i e d\\_s u n}^{4}=\\frac{1506.02\\times6.95\\times10^{8}}{3\\times5.67\\times10^{-8}}}\\end{array}$   \n$\\begin{array}{r}{T_{m o d i f i e d\\_s u n}^{4}=\\frac{10466.839\\times10^{8}}{17.01\\times10^{-8}}=\\frac{1.0466\\times10^{12}}{1.701\\times10^{-7}}=0.6153\\times10^{19}=6.153\\times10^{18}}\\end{array}$ $T_{m o d i\\,f i e d\\ s u n}=(6.153\\times10^{18})^{1/4}=(6153\\times10^{15})^{1/4}$ 𝑇𝑚𝑜𝑑𝑖𝑓𝑖𝑒𝑑_𝑠𝑢𝑛 = (6153)0.25 × (1015)0.25 = (6153)0.25 × 103.75   \nCalculate $(6153)^{0.25}$ :   \nLet $y=6153^{0.25}$ , then $\\ln y=0.25\\ln(6153)$   \n$\\ln(6153)\\approx8.724$   \n$\\ln y=0.25\\times8.724=2.181$   \n$y=e^{2.181}\\approx8.85$   \n$T_{m o d i f i e d\\_s u n}=8.85\\times10^{3.75}=8.85\\times10^{3}\\times10^{0.75}=8850\\times5.62=49737\\,\\mathrm{K}$ This is close to option E: 50000 K.   \nLet’s re-check the units and calculations carefully.   \nPower density of human: $\\textstyle{\\frac{100\\mathrm{~W~}}{0.0664\\mathrm{~m}^{3}}}=1506.02\\mathrm{~W}/\\mathrm{m}^{3}$   \nThe formula derived was 𝑇 4 = 𝜌𝑝𝑜𝑤𝑒𝑟,ℎ𝑢𝑚𝑎𝑛𝑅𝑠𝑢𝑛 .   \nUnits check:   \n$[\\rho_{p o w e r,h u m a n}]=\\mathrm{W}/\\mathrm{m}^{3}$   \n$[R_{s u n}]={\\mathrm{m}}$   \n[𝜎] = Wm−2K−4   \n$\\begin{array}{r}{\\frac{(\\mathrm{W}/\\mathrm{m}^{3})\\times\\mathrm{m}}{\\mathrm{W}\\mathrm{m}^{-2}\\mathrm{K}^{-4}}=\\frac{\\mathrm{W}/\\mathrm{m}^{2}}{\\mathrm{W}\\mathrm{m}^{-2}\\mathrm{K}^{-4}}=\\frac{\\mathrm{W}}{\\mathrm{m}^{2}}\\times\\frac{\\mathrm{m}^{2}\\mathrm{K}^{4}}{\\mathrm{W}}=\\mathrm{K}^{4}}\\end{array}$   \nThe units match for $T^{4}$ .   \nNumerical calculation check:   \nNumerator: $1506.02\\times6.95\\times10^{8}=10466.839\\times10^{8}=1.0466\\times10^{12}$   \nDenominator: $3.\\times5.67\\times10^{-8}=17.01\\times10^{-8}$   \nRatio: 11.70.40616××1100−182 = 0.061528 × 1020 = 6.1528 × 1018   \n$T^{4}=6.1528\\times10^{18}$   \n$T=(6.1528\\times10^{18})^{1/4}=(6152.8\\times10^{15})^{1/4}$   \n𝑇 = (6152.8)0.25 × 103.75   \nLet’s use logarithms for precision.   \n$\\log_{10}(6152.8)\\approx3.789$   \n$0.25\\times3.789=0.94725$   \n$10^{0.94725}\\approx8.856$   \n$T=8.856\\times10^{3.75}=8.856\\times10^{3}\\times10^{0.75}=8856\\times5.623=49806\\;\\mathrm{K}$   \nThis is very close to 50000 K.   \nPossible sources of minor discrepancies: rounding errors, precision of constants. Let’s verify the formula derivation.   \nPower per unit volume $\\begin{array}{r}{={\\frac{\\mathrm{Total}\\,\\mathrm{Power}}{\\mathrm{Volume}}}}\\end{array}$ $\\begin{array}{r l}&{\\rho_{p o w e r}=\\frac{L}{V}}\\\\ &{L=4\\pi R^{2}\\sigma T^{4}}\\\\ &{V=\\frac{4}{3}\\pi R^{3}}\\\\ &{\\rho_{p o w e r}=\\frac{4\\pi R^{2}\\sigma T^{4}}{\\frac{4}{3}\\pi R^{3}}=\\frac{3\\sigma T^{4}}{R}}\\end{array}$   \nIf the modifed sun has the power density of a human:   \n$\\begin{array}{r l}&{\\rho_{p o w e r,h u m a n}=\\frac{3\\sigma T_{m o d i f i e d\\_s u n}^{4}}{R_{s u n}}}\\\\ &{T_{m o d i f i e d\\_s u n}^{4}=\\frac{\\rho_{p o w e r,h u m a n}R_{s u n}}{3\\sigma}}\\end{array}$   \nThe formula derivation seems correct.   \nLet’s use approximate values to check the order of magnitude.   \n$\\begin{array}{r l}&{\\rho_{p o w e r,h u m a n}\\sim10^{3}\\,\\,\\mathrm{W/m^{3}}}\\\\ &{R_{s u n}\\approx7\\times10^{8}\\,\\,\\mathrm{m}}\\\\ &{\\sigma\\approx6\\times10^{-8}\\,\\,\\mathrm{Wm^{-2}K^{-4}}}\\\\ &{T^{4}\\approx\\frac{10^{3}\\times7\\times10^{8}}{3\\times6\\times10^{-8}}=\\frac{7\\times10^{11}}{18\\times10^{-8}}=0.38\\times10^{19}=3.8\\times10^{18}}\\\\ &{T\\approx(3.8\\times10^{18})^{1/4}=(3800\\times10^{15})^{1/4}}\\\\ &{T\\approx(3800)^{0.25}\\times10^{3.75}}\\end{array}$   \n(3800)0.25: $7^{4}=2401$ , $8^{4}=4096$ . So, between 7 and 8.   \n$\\approx7.8$   \n$T\\approx7.8\\times10^{3.75}=7800\\times5.6=43680\\;\\mathrm{K}$   \nThe order of magnitude seems consistent with the detailed calculation.   \nThe problem statement is straightforward and doesn’t seem to have any hidden complexities. The assumptions made are standard for such problems.   \nConsider the precision of the given values.   \nPower of human: 100 W (exact)   \nVolume of human: $66400~\\mathrm{cm}^{3}$ (exact)   \nThe result obtained is very close to option E. Given the nature of multiple-choice questions, the closest option is likely the intended answer.   \nIf there were any misinterpretations of the question, it would likely be in the setup. However, the phrase \"If the sun were the same size, but it produced the same amount of power per unit volume as a human\" seems clear.   \nThe surface temperature is calculated using the Stefan-Boltzmann law, assuming black-body radiation.   \nDouble-check the provided options:   \nA: 3500 K   \nB: 10000 K   \nC: 25000 K   \nD: 40000 K   \nE: 50000 K   \nOur calculated value is approximately 49800 K, which is closest to 50000 K.   \nIf we used more precise values for the constants, the result might be slightly diferent, but likely not enough to shift to another option.   \nThe fun fact mentioned in the question implies that the power density of a human is higher than that of the sun. Let’s verify this.   \nPower output of the sun: $3.846\\times10^{26}$ W   \nVolume of the sun: $4.43\\times10^{27}~\\mathrm{m}_{\\sim}^{3}$ $\\frac{\\mathrm{:}10^{27}\\mathrm{~m}^{3}}{4.846\\times10^{26}}=0.868\\mathrm{~W}/\\mathrm{m}^{3}$   \nPower density of the sun:   \nPower density of human: $1506\\;\\mathrm{W}/\\mathrm{m}^{3}$   \nIndeed, the power density of a human is signifcantly higher than that of the sun.   \nFinal check of the calculation steps and logic. The approach of equating the power density of the modifed sun to that of a human and then using the Stefan-Boltzmann law seems correct. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Solution: ",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "The power produced by an average human is $P_{h u m a n}=100\\;\\mathrm{W}$ , and the volume of an average human is $V_{h u m a n}=66400\\;\\mathrm{cm}^{3}=$ $0.0664\\;\\mathrm{m}^{3}$ .   \nThe power density of a human is 𝜌𝑝𝑜𝑤𝑒𝑟,ℎ𝑢𝑚𝑎𝑛 = 𝑃 $\\begin{array}{r}{\\rho_{p o w e r,h u m a n}=\\frac{P_{h u m a n}}{V_{h u m a n}}=\\frac{100\\;\\mathrm{W}}{0.0664\\;\\mathrm{m}^{3}}\\approx1506.02\\;\\mathrm{W}/\\mathrm{m}^{3}.}\\end{array}$   \nIf the sun were the same size but produced the same amount of power per unit volume as a human, its power density would be 𝜌𝑝𝑜𝑤𝑒𝑟,𝑚𝑜𝑑𝑖𝑓𝑖𝑒𝑑_𝑠𝑢𝑛 = 𝜌𝑝𝑜𝑤𝑒𝑟,ℎ𝑢𝑚𝑎𝑛.   \nThe radius of the sun is $R_{s u n}=6.95\\times10^{8}\\ \\mathrm{m}$ .   \nThe volume of the sun is $\\begin{array}{r}{V_{s u n}=\\frac{4}{3}\\pi R_{s u n}^{3}}\\end{array}$ .   \nThe power density of the modifed sun is given by 𝜌𝑝𝑜𝑤𝑒𝑟,𝑚𝑜𝑑𝑖𝑓𝑖𝑒𝑑_𝑠𝑢𝑛 = $\\begin{array}{r}{\\rho_{p o w e r,m o d i f i e d\\_s u n}=\\frac{P_{m o d i f i e d\\_s u n}}{V_{s u n}}.}\\end{array}$   \nWe also know that $P_{m o d i f i e d\\_s u n}=4\\pi R_{s u n}^{2}\\sigma T^{4}$ , where $T$ is the surface temperature of the modifed sun.   \nEquating the power density:   \n$\\begin{array}{r}{\\rho_{p o w e r,h u m a n}=\\frac{4\\pi R_{s u n}^{2}\\sigma T^{4}}{\\frac{4}{3}\\pi R_{s u n}^{3}}=\\frac{3\\sigma T^{4}}{R_{s u n}}}\\end{array}$   \nSolving for $T^{4}$ :   \n$\\begin{array}{r}{T^{4}=\\frac{\\rho_{p o w e r,h u m a n}R_{s u n}}{3\\sigma}}\\end{array}$   \nSubstitute the values:   \n$\\begin{array}{r}{T^{4}=\\frac{1506.02\\mathrm{~W}/\\mathrm{m}^{3}\\times6.95\\times10^{8}\\mathrm{~m}}{3\\times5.67\\times10^{-8}\\mathrm{~Wm}^{-2}\\mathrm{K}^{-4}}}\\end{array}$   \n$\\begin{array}{r}{T^{4}=\\frac{1.0466\\times10^{12}}{1.701\\times10^{-7}}=6.1528\\times10^{18}\\;\\mathrm{K}^{4}}\\end{array}$   \nSolving for 𝑇 :   \n$T=(6.1528\\times10^{18})^{1/4}=(6152.8)^{0.25}\\times10^{18/4}=(6152.8)^{0.25}\\times10^{4.5}$   \nUsing logarithms:   \n$\\log_{10}(6152.8)\\approx3.789$   \n$0.25\\times3.789=0.94725$   \n$(6152.8)^{0.25}=10^{0.94725}\\approx8.856$   \n$T=8.856\\times10^{4.5}=8.856\\times10^{4}\\times10^{0.5}=88560\\times\\sqrt{10}\\approx88560\\times3.16=280000\\;\\mathrm{K}$   \nLet’s redo the exponent calculation: $18/4=4.5$ .   \n$T=8.856\\times10^{4.5}=8.856\\times10^{4}\\times10^{0.5}$ - error in previous step.   \n$T=(6.1528\\times10^{18})^{0.25}=49806\\mathrm{\\;K}$   \nThe surface temperature would be approximately 49806 K, which is closest to 50000 K.   \nFinal Answer: The fnal answer is $\\lceil{\\cal E}\\rceil$ ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "D. Test-time scaling details ",
        "text_level": 1,
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "D.1. Sequential scaling ablations ",
        "text_level": 1,
        "page_idx": 33
    },
    {
        "type": "table",
        "img_path": "images/68ffccc321b186e539b4d15669ee9510575a0c33197a7d719f48e98ad0a46db4.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td><lim_startl>user</td><td><lim_startl>user</td></tr><tr><td>What is the answer to Life, the Universe and Everything?</td><td>What is the answer to Life, the Universe and Everything?</td></tr><tr><td>Thinkfor up to2048tokens.</td><td>Think for up to 64 steps.</td></tr><tr><td><lim_startl>assistant</td><td><lim_startl>assistant</td></tr><tr><td><lim_startl>think</td><td><lim_startl>64 steps left</td></tr><tr><td>Let me break down this question into the three parts it</td><td>Let me break down this question into the three parts it is asking for:</td></tr><tr><td>is asking for: 1) Life 2) Universe 3) Everything</td><td>1) Life 2) Universe 3) Everything</td></tr><tr><td rowspan=\"2\">Let me start with life...</td><td><lim_startl>63 steps left</td></tr><tr><td>Let me start with life...</td></tr><tr><td><lim_startl>answer</td><td><lim_startl>answer</td></tr><tr><td>The answer is...</td><td>The answer is...</td></tr></table></body></html>\n\n",
        "page_idx": 33
    },
    {
        "type": "table",
        "img_path": "images/c523a1e8cad1f5bdf3976936c216cc7d2d9ce6f450692d6ef9f982997d224ed9.jpg",
        "table_caption": [
            "Figure 10. Token and step instruction data formats for controlling test-time compute. We only train our model on the reasoning trace and the answer. ",
            "Table 11. Scaling thinking time via tokens-conditional control. All metrics are averaged over the 30 questions in AIME24. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td colspan=\"5\">Tokens instructed (→) 1024 2048 4096 8192 16384</td></tr><tr><td colspan=\"5\">Nointerventionattest-time</td></tr><tr><td>Thinking tokens</td><td>7939 7158</td><td>8263</td><td>7108</td><td>7500</td></tr><tr><td>Answertokens</td><td>689 669</td><td>659</td><td>722</td><td>724</td></tr><tr><td>AIME24</td><td>26.7 30.0</td><td>33.3</td><td>33.3</td><td>40.0</td></tr><tr><td colspan=\"5\">Forcingendofthinkingwhentokenbudgetisreached</td></tr><tr><td>Thinking tokens</td><td>1024 2048</td><td>4031</td><td>5664</td><td>6330</td></tr><tr><td>Answertokens</td><td>15 15</td><td>142</td><td>722</td><td>691</td></tr><tr><td>AIME24</td><td>3.3 30.0</td><td>33.3</td><td>33.3</td><td>40.0</td></tr></table></body></html>\n\n",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Token-conditional control One general approach is to simply tell a model in the prompt precisely how many tokens it should generate. Ideally, the model can keep track of its token count and adjust its generation to fnish within the desired limits. We experiment with this approach by training a model with token instructions using the format in Figure 10 (left). We bucket the lengths of the reasoning traces from our 1,000 training examples into powers of two (rounded upwards) and add a corresponding instruction to the user prompt. For example, if the instruction says “Think for up to 2048 tokens”, then the reasoning trace has anywhere between 1024 and 2048 tokens. In Table 11, we show that after training the model hardly follows the token instruction. It does sometimes generate more tokens when given a higher limit but often overshoots the limit. This may not be unique to our model as prior work suggests that OpenAI o1-mini can also not follow token instructions (Zhang & Chen, 2024). To prevent exceeding the limit, we test budget forcing the thinking to end once the limit is reached. This leads to perfect control (Table 11 (lower)). With budget forcing, the scaling trend is also clearer as the model can no longer overshoot the limit when given a small thinking budget. This leads to better test-time scaling values for Token Prompting $^+$ budget forcing in Table 3. To compute Control reported in Table 3 for token-conditional control variants we divide the number of times the thinking tokens in Table 11 are less than the upper limit by the total evaluations (2/5 for without intervention; 5/5 for with intervention). ",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Step-conditional control Token instructions fail as current models cannot count tokens. To accommodate this lack of capability, we experiment with making the counting more coarse-grained. We partition the reasoning traces into steps and ask the model to think for a specifc number of steps rather than tokens. We split our reasoning traces on double newlines into steps, which we fnd act as intuitive separators based on manual inspection of samples. We bucket our training samples into powers of 2 depending on their number of steps and add a corresponding step instruction following the format in Figure 10 (right). This format is based on early experiments, where we found the model to be more likely to adhere to the step limit when counting down (“3 steps left...2 steps left”) rather than counting up (“Step2...Step3...”). This is likely because if counting down, the fnal step is always 1, which will act as a strong prior to the model to fnish its generation. If counting up, the fnal step before the answer varies, thus if the model does not remember the original step instruction, it may fail to stop. We conclude the following from our results in Table 12: (1) The model still struggles to adhere to the step limit. The model sometimes simply continues counting into negative steps, e.g. “-1 steps left”. To solve this issue, we automatically stop the thinking process once 0 steps are reached and then force the model to transition to answering mode by appending the answer token delimiter (§3). This leads to perfect step adherence (lower half of Table 12), yet problems remain. (2) The model compensates for fewer steps by making each step longer. For example, when forced to use up to 16 steps vs 256 steps, the model generates an average of 96 tokens per step vs 56. Despite this issue, more steps still clearly correlate with more total thinking tokens in Table 12 and better performance leading to a positive slope (3) Step instructions are more costly than other methods. The step delimiters require around 6 tokens each which for e.g. 64 steps adds up to a total of around 380 tokens. When ignoring the step delimiters in token counts as in Table 12, the model still requires 7551 thinking tokens on average to achieve only $33.3\\%$ on AIME24. To compute Control reported in Table 3 for step-conditional control variants, we frst decide that 100 tokens are an upper limit per step and then multiply this number by the steps instructed to arrive at a proxy total token limit, e.g. 1600 for 16 steps instructed. We then check whether the thinking tokens in Table 12 ft within the respective limit for each evaluation run (3/5 for without intervention; 5/5 for with intervention). For the model in Figure 7, we use a model with step-conditional control trained on an earlier version of our data and using an earlier version of our evaluation codebase. ",
        "page_idx": 33
    },
    {
        "type": "table",
        "img_path": "images/b73ad33d2f45c9d727accabefe2820a666ee5b355e6487c0882d21de22a0b3bc.jpg",
        "table_caption": [
            "Table 12. Scaling thinking time via step-conditional control. All metrics are averaged over the 30 samples in AIME24. Token counts ignore the thinking and step delimiters. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Steps instructed (→)</td><td>16</td><td>32</td><td>64</td><td>128</td><td>256</td></tr><tr><td colspan=\"6\">Nointerventionattest-time</td></tr><tr><td>Steps used</td><td>123</td><td>90</td><td>80</td><td>82</td><td>136</td></tr><tr><td>Tokens per step</td><td>60</td><td>70</td><td>69</td><td>66</td><td>56</td></tr><tr><td>Thinking tokens</td><td>7252</td><td>6277</td><td>5396</td><td>5552</td><td>7551</td></tr><tr><td>Answer tokens</td><td>665</td><td>653</td><td>735</td><td>777</td><td>754</td></tr><tr><td>AIME24</td><td>33.3</td><td>23.3</td><td>33.3</td><td>36.7</td><td>33.3</td></tr><tr><td colspan=\"6\">Forcing end of thinkingwhen O steps are reached</td></tr><tr><td>Steps used</td><td>16</td><td>32</td><td>59</td><td>78</td><td>136</td></tr><tr><td>Tokens per step</td><td>96</td><td>94</td><td>80</td><td>70</td><td>56</td></tr><tr><td>Thinking tokens</td><td>1517</td><td>2963</td><td>4636</td><td>5409</td><td>7551</td></tr><tr><td>Answer tokens</td><td>1111</td><td>788</td><td>799</td><td>794</td><td>754</td></tr><tr><td>AIME24</td><td>23.3</td><td>23.3</td><td>33.3</td><td>36.7</td><td>33.3</td></tr></table></body></html>\n\n",
        "page_idx": 34
    },
    {
        "type": "table",
        "img_path": "images/fc54e55399390895bc77cafe49aa1b5e904c4dee7d6bd1f66481e25713885122.jpg",
        "table_caption": [
            "Table 13. Scaling thinking time via class-conditional control. We report “accuracy / average thinking tokens per sample”; the higher the accuracy and the fewer the thinking tokens (inference cost) the better. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Prompt appended to the question after two newlines</td><td>AIME24</td><td>MATH500GPQA</td><td></td></tr><tr><td>Answerafterashortamountofthinking.Donotspend excessivetimedouble-checkingyourwork.</td><td>30.0% 8033</td><td>90.4% 2537</td><td>/56.6% 4177</td></tr><tr><td>Answerafteralongamountofthinking.Ifyoufeellikeyou arefinishedearly,spendtheextratimetryingtodouble- checkyourworkuntilyouareabsolutelysurethatyouhave thecorrectanswer.</td><td>9651</td><td>3875</td><td>36.7%/91.4%/51.0% 4827</td></tr><tr><td>Without generic prompt appending</td><td>50.0%/ 6109</td><td>93.0%/57.6% 3298</td><td>3510</td></tr></table></body></html>\n\n",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Class-conditional control OpenAI exposes test-time compute control to users via a “reasoning_efort” API parameter with three possible settings: low, medium, and high.3 The OpenAI documentation also states that “Reducing reasoning efort can result in faster responses and fewer tokens used on reasoning in a response.\" suggesting that they are unable to control test-time compute with guarantees. Thus, maybe OpenAI simply adjusts the prompt or system instruction depending on the reasoning efort desired. In Table 13, we show that separate prompts for short and long thinking allow us to control thinking time to some extent: Prompting the model to think for longer leads to longer thinking. However, it does not reliably improve performance and control is not precise. The current adherence to control may sufce when we only have three classes, but it might not scale to fner-grained classes. To compute Control reported in Table 3 for this method, we assume that prompting the model to think for a short time in Table 13 should produce fewer tokens than the default for AIME24, while the long prompt should produce more. As $8033>6109$ and $9651>6109$ , one out of two follows our expected control thus Control is $50\\%$ . ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "D.2. Examples for rejection sampling ablation ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Table 14. Answers generated by an earlier version of s1-32B when rejection sampling on AIME24. The black text is the question and the yellow text is the solution. Based solely on the question we generate the light blue reasoning trace and blue response with s1-32B using a temperature of 1. We generate until the reasoning trace has less than 4000 tokens for the frst and less than 8000 tokens for the second example. It required on average 199 (1) tries per example until thinking was $\\leq4000$ $\\leq8000$ tokens). ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Let $O(0,0),A(\\frac{1}{2},0)$ , and $B(0,\\textstyle{\\frac{\\sqrt{3}}{2}})$ be points in the coordinate plane. Let ${\\boldsymbol{\\mathcal{F}}}$ be the family of segments $\\overline{{P Q}}$ of unit length lying in the frst quadrant with $P$ on the $x$ -axis and $Q$ on the $y_{\\mathrm{~\\,~}}$ -axis. There is a unique point $C$ on $\\overline{{A B}}$ , distinct from $A$ and $B$ , that does not belong to any segment from $\\mathcal{F}$ other than $\\overline{{A B}}$ . Then $\\begin{array}{r}{O C^{2}=\\frac{p}{q}}\\end{array}$ , where $p$ and $q$ are relatively prime positive integers. Find $p+q$ . ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "23 ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "[Correct generation with 3981 thinking tokens and 1055 answer tok ",
        "text_level": 1,
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "The problem asks for a point $C$ on the segment $\\overline{{A B}}$ that is not contained in any other segment from the family $\\mathcal{F}$ , except for $\\overline{{A B}}$ itself.   \nThe points $O,A,B$ form a triangle in the frst quadrant.   \n$\\begin{array}{l}{O=(0,0)}\\\\ {A=(\\frac{1}{2},0)}\\\\ {B=(0,\\frac{\\sqrt{3}}{2})}\\end{array}$   \nThe segment $\\overline{{A B}}$ connects $A(\\frac{1}{2},0)$ and $B(0,\\frac{\\sqrt{3}}{2})$   \nThe equation of the line containing $\\overline{{A B}}$ can be found using the two points:   \nSlope = √023 −−120 = −√3   \nUsing the point-slope form $y-y_{1}=m(x-x_{1})$ with point $A(\\frac{1}{2},0)$ :   \n$\\begin{array}{l}{{y-0=-\\sqrt{3}(x-\\frac{1}{2})}}\\\\ {{y=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}}}\\end{array}$   \nThe segment $\\overline{{A B}}$ lies in the frst quadrant and connects $({\\frac{1}{2}},0)$ to $(0,{\\frac{\\sqrt{3}}{2}})$ .   \nThe family $\\gamma^{*}$ consists of segments $P Q$ of unit length, where $P$ is on the $\\mathcal{X}$ -axis and $Q$ is on the 𝑦-axis. Let $P=(a,0)$ and $Q=(0,b)$ .   \nThe length of $P Q$ is ${\\sqrt{(a-0)^{2}+(0-b)^{2}}}={\\sqrt{a^{2}+b^{2}}}=1.$   \nSo $a^{2}+b^{2}=1$ .   \nSince $P$ is on the positive $\\mathcal{X}$ -axis and $Q$ is on the positive 𝑦-axis, we have $a>0$ and $b>0$ .   \nThe segment $\\overline{{P Q}}$ connects $(a,0)$ and $(0,b)$ . The equation of the line containing $P Q$ is $\\textstyle{\\frac{x}{a}}+{\\frac{y}{b}}=1$ .   \nThe region covered by the family $\\gamma^{*}$ is the area bounded by the lines containing these segments. The endpoints $(a,0)$ and $(0,b)$ vary such that $a^{2}+b^{2}=1$ .   \nConsider the parametrization $a=\\cos\\theta$ , $b=\\sin\\theta$ , where $\\begin{array}{r}{0<\\theta<\\frac{\\pi}{2}}\\end{array}$ .   \nThe segment $\\overline{{P Q}}$ connects $(\\cos\\theta,0)$ and $(0,\\sin\\theta)$ . The equation of the line is $\\begin{array}{r}{\\frac{x}{\\cos\\theta}+\\frac{y}{\\sin\\theta}=1}\\end{array}$ .   \nWe are looking for a unique point $\\textrm{C}$ on $\\overline{{A B}}$ , distinct from $A$ and $B$ , that does not belong to any segment from $\\gamma^{*}$ other than $\\overline{{A B}}$ .   \nLet $\\scriptscriptstyle\\mathcal{C}$ be a point on $A B$ . We can parametrize the points on $A B$ as $(1-t)A+t B$ for $0\\leq t\\leq1$ .   \n$\\begin{array}{r}{C=(1-t)(\\frac{1}{2},0)+t(0,\\frac{\\sqrt{3}}{2})=(\\frac{1}{2}-\\frac{1}{2}t,\\frac{\\sqrt{3}}{2}t)}\\end{array}$ .   \nSince $\\scriptscriptstyle\\mathcal{C}$ is distinct from $\\tilde{A}$ and $B$ , we have $0<t<1$ .   \nSuppose the point $\\scriptscriptstyle\\mathcal{C}$ lies on some segment $P Q\\in{\\mathcal{F}}$ , where $P=(\\cos\\theta,0)$ and $Q=(0,\\sin\\theta)$ .   \nThe point $\\textrm{C}$ satisfes the equation $\\begin{array}{r}{\\frac{x}{\\cos\\theta}+\\frac{y}{\\sin\\theta}=1}\\end{array}$ . Plugging in the coordinates of $C$ : 12 − 12 𝑡 √23 𝑡 cos 𝜃 sin 𝜃 1−𝑡 + √3𝑡   \n2M cuosl t𝜃iply 2 bsiyn  𝜃2: $\\begin{array}{r}{\\frac{1-t}{\\cos\\theta}+\\frac{\\sqrt{3}t}{\\sin\\theta}=2}\\end{array}$ $(1-t)\\sin\\theta+\\sqrt{3}t\\cos\\theta=2\\sin\\theta\\cos\\theta$ $\\sin\\theta-t\\sin\\theta+{\\sqrt{3}}t\\cos\\theta=\\sin(2\\theta)/2$   \nWe are looking for a point $\\textrm{C}$ on $A B$ such that the only segment from $\\mathcal{F}$ containing $\\textrm{C}$ is $A B$ itself. The segment $\\overline{{A B}}$ is part of the family $\\gamma^{*}$ .   \nFor $A B$ , $P=A=(\\textstyle{\\frac{1}{2}},0)$ and $\\textstyle Q=B=(0,{\\frac{\\sqrt{3}}{2}})$   \nHere 𝑎 = 12 , 𝑏 = √23 . $\\textstyle a^{2}+b^{2}=({\\frac{1}{2}})^{2}+({\\frac{\\sqrt{3}}{2}})^{2}={\\frac{1}{4}}+{\\frac{3}{4}}=1.$   \nThe line segment $\\overline{{A B}}$ connects $({\\frac{1}{2}},0)$ and $(0,{\\frac{\\sqrt{3}}{2}})$ .   \nThe equation of the line is $\\begin{array}{r}{\\frac{x}{1/2}+\\frac{y}{\\sqrt{3}/2}=1}\\end{array}$ , which simplifes to $2x+\\frac{2}{\\sqrt{3}}y=1$ , or ${\\sqrt{3}}x+y={\\frac{\\sqrt{3}}{2}}$   \nThis matches the equation 𝑦 = −√3𝑥 + √23 .   \nCDiofnesirdeenrt itahtien eg nwveitlho pree sopfe tcht et of m: ily of lines $\\begin{array}{r}{\\frac{x}{\\cos\\theta}+\\frac{y}{\\sin\\theta}=1}\\end{array}$ . $\\theta$ $\\begin{array}{r}{-\\frac{x\\sin\\theta}{\\cos^{2}\\theta}+\\frac{y\\cos\\theta}{\\sin^{2}\\theta}=0}\\end{array}$ $\\begin{array}{r}{\\frac{y\\cos\\theta}{\\sin^{2}\\theta}=\\frac{x\\sin\\theta}{\\cos^{2}\\theta}}\\end{array}$   \n𝑦 cos3 𝜃 = 𝑥 sin3 𝜃 𝑦 = tan3 𝜃   \ntan 𝜃 = ( 𝑦 )1∕3 From the line equation: $y\\sin\\theta=\\sin\\theta\\cos\\theta-x\\sin\\theta$   \n$\\begin{array}{r l}&{y=\\cos\\theta-x}\\\\ &{\\mathrm{Substitute~cos}\\,\\theta=\\frac{1}{\\sqrt{1+\\tan^{2}\\theta}}=\\frac{1}{\\sqrt{1+(y/x)^{2/3}}}}\\\\ &{\\cos\\theta=\\frac{x^{1/3}}{(x^{2/3}+y^{2/3})^{1/2}}}\\\\ &{\\sin\\theta=\\tan\\theta\\cos\\theta=(\\frac{y}{x})^{1/3}\\frac{x^{1/3}}{(x^{2/3}+y^{2/3})^{1/2}}=\\frac{y^{1/3}}{(x^{2/3}+y^{2/3})^{1/2}}}\\\\ &{\\sim\\frac{1}{(x^{2/3}+y^{2/3})^{1/2}}.}\\end{array}$   \nSubstitute back into the line equation: $\\begin{array}{r}{\\frac{x}{\\cos\\theta}+\\frac{y}{\\sin\\theta}=1}\\end{array}$ $x(x^{2/3}+y^{2/3})^{1/2}/x^{1/3}+y(x^{2/3}+y^{2/3})^{1/2}/y^{1/3}=1$ 1 $x^{2/3}(x^{2/3}+y^{2/3})^{1/2}+y^{2/3}(x^{2/3}+y^{2/3})^{1/2}=1$ $(x^{2/3}+y^{2/3})^{3/2}=1$ $x^{2/3}+y^{2/3}=1$   \nThe envelope of the family $\\gamma^{*}$ is the astroid $x^{2/3}+y^{2/3}=1$ . The segments from $\\gamma^{\\ast}$ are tangent to this astroid. The point $\\textrm{C}$ on $\\overline{{A B}}$ that we are looking for is the point where $A B$ is tangent to the astroid.   \nThe equation of 𝐴𝐵 is 𝑦 = −√3𝑥 + √23 .   \nSubstitute into the astroid equation: $x^{2/3}+(-\\sqrt{3}x+\\frac{\\sqrt{3}}{2})^{2/3}=1$   \nLet $u=x^{1/3}$ and $v=y^{1/3}$ . The astroid is $u^{2}+v^{2}=1$ . The point $\\textrm{C}$ lies on both the line $\\overline{{A B}}$ and the astroid. Diferentiating the astroid equation implicitly: $\\begin{array}{r}{\\frac{2}{3}x^{-1/3}+\\frac{2}{3}y^{-1/3}y^{\\prime}=0}\\end{array}$ $\\begin{array}{r}{y^{\\prime}=-\\frac{y^{1/3}}{x^{1/3}}=-(\\frac{y}{x})^{1/3}}\\end{array}$ The slope of the tangent to the astroid at a point $(x,y)$ is $-\\sqrt[3]{\\frac{y}{x}}$   \nThe slope of the line $\\overline{{A B}}$ is $-\\sqrt{3}$ .   \nSo, at the point of tangency, $-\\sqrt[3]{\\frac{y}{x}}=-\\sqrt[]{3}$ .   \n${\\sqrt[{3}]{\\frac{y}{x}}}={\\sqrt{3}}$   \nThe point $C$ lies on the line $y=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}$ and satisfes ${\\frac{y}{x}}=3{\\sqrt{3}}$ .   \nSo 𝑦 = 3√3𝑥.   \nSubstitute into the line equation:   \n3√3𝑥 = −√3𝑥 + √3   \n3𝑥 = −𝑥 + 21   \n4𝑥 = 12   \n𝑥 = 81   \nThen 𝑦 = 3√3( 81 ) = 3√8 3 .   \nThe point $\\textrm{C}$ is $({\\frac{1}{8}},{\\frac{3{\\sqrt{3}}}{8}})$ ).   \nWe need to check if this point lies on the segment $A B$ .   \nThe point $\\textrm{C}$ is of the form $({\\textstyle{\\frac{1}{2}}}-{\\textstyle{\\frac{1}{2}}}t,{\\textstyle{\\frac{\\sqrt{3}}{2}}}t)$ .   \n$\\begin{array}{r}{\\frac{1}{2}-\\frac{1}{2}t=\\frac{1}{8}\\implies4-4t=1\\implies3=4t\\implies t=\\frac{3}{4}.}\\end{array}$   \n$\\begin{array}{r}{\\frac{\\sqrt{3}}{2}t=\\frac{\\sqrt{3}}{2}\\cdot\\frac{3}{4}=\\frac{3\\sqrt{3}}{8}.}\\end{array}$   \nSo $\\textrm{C}$ lies on $\\overline{{A B}}$ .   \nWe need to make sure $0<t<1$ , which is true for $\\begin{array}{r}{t=\\frac{3}{4}}\\end{array}$ .   \nNow, we need to check if this point $\\scriptscriptstyle\\mathcal{C}$ does not belong to any other segment from $\\gamma^{\\ast}$ .   \nThe point $\\textrm{C}$ lies on the astroid. The segment $\\overline{{A B}}$ is tangent to the astroid at $C$ .   \nAny other segment from $f^{\\star}$ that contains $\\scriptscriptstyle\\mathcal{C}$ would have $\\textrm{C}$ as an interior point, not a tangent point.   \nConsider the parametrization of the family $\\gamma^{\\ast}$ .   \nA point $(x,y)$ belongs to some segment in $\\gamma^{*}$ if there exists $\\theta$ such that $\\begin{array}{r}{\\frac{x}{\\cos\\theta}+\\frac{y}{\\sin\\theta}=1}\\end{array}$ .   \nFor the point $\\begin{array}{r}{C=(\\frac{1}{8},\\frac{3\\sqrt{3}}{8})}\\end{array}$ , we have:   \n$\\begin{array}{r}{\\frac{1/8}{\\cos\\theta}+\\frac{3\\sqrt{3}/8}{\\sin\\theta}=1}\\end{array}$   \n$\\textstyle{\\frac{1}{\\cos\\theta}}+{\\frac{3{\\sqrt{3}}}{\\sin\\theta}}=8$   \n$\\sin\\theta+3{\\sqrt{3}}\\cos\\theta=8\\sin\\theta\\cos\\theta$   \n$\\sin\\theta+3{\\sqrt{3}}\\cos\\theta=4\\sin(2\\theta)$   \nWe know that $\\textrm{C}$ lies on $\\overline{{A B}}$ , which corresponds to some specifc segment in $\\gamma^{*}$ .   \nThe problem statement assumes the existence and uniqueness of such a point $\\textrm{C}$ .   \nThe unique point $\\textrm{C}$ on $\\overline{{A B}}$ , distinct from $A$ and $B$ , that does not belong to any segment from $\\gamma^{\\ast}$ other than $\\overline{{A B}}$ . This means that if $\\scriptscriptstyle\\mathcal{C}$ lies on another segment ${\\overline{{P^{\\prime}Q^{\\prime}}}}\\in{\\mathcal{F}}$ , then ${\\overline{{P^{\\prime}Q^{\\prime}}}}={\\overline{{A B}}}$ .   \nThe point $\\textrm{C}$ is the point of tangency of $\\overline{{A B}}$ with the astroid.   \nNow we need to fnd $O C^{2}$ .   \n$\\begin{array}{c c c}{{O C^{2}=(\\frac{1}{8})^{2}+(\\frac{3\\sqrt{3}}{8})^{2}=\\frac{1}{64}+\\frac{27}{64}=\\frac{28}{64}=\\frac{7}{16}.}}\\end{array}$   \n$p=7$ $q=16$   \n$\\boldsymbol{\\rho}$ and $q$ are relatively prime.   \n$p+q=7+16=23$ .   \nFinal check of the reasoning:   \nThe family $\\gamma^{*}$ consists of unit-length segments with endpoints on the axes.   \nThe envelope of this family is the astroid $x^{2/3}+y^{2/3}=1$ .   \nThe segments from $f^{\\star}$ are tangent to the astroid.   \nThe segment $A B$ is also a member of the family $\\gamma^{*}$ .   \nWe are looking for a point 𝐶 on 𝐴𝐵 that lies on the astroid. 39   \nThis point $\\textrm{C}$ is the point of tangency of $A B$ with the astroid. If a point on $A B$ lies on the astroid, it belongs to the tangent segment at that point. Since $\\overline{{A B}}$ is the tangent segment, this point does not belong to any other segment from $\\gamma^{*}$ .   \nConsider the case when the segment $\\overline{{A B}}$ intersects the astroid at two points.   \nIf the segment $\\overline{{A B}}$ is not tangent to the astroid, then there are two points of intersection.   \nLet the points of intersection be $C_{1}$ and $C_{2}$ .   \nIf $C_{1}$ lies on $\\overline{{A B}}$ , then the tangent to the astroid at $C_{1}$ is the segment $\\overline{{A B}}$ .   \nIf $C_{2}$ lies on $\\overline{{A B}}$ , then the tangent to the astroid at $C_{2}$ is the segment $A B$ .   \nThe slope of $A B$ is $-\\sqrt{3}$ .   \nWe found the point on the astroid with tangent slope $-\\sqrt{3}$ .   \nThe problem statement guarantees the uniqueness of such a point $\\textrm{C}$ .   \nThe point $\\textrm{C}$ must be interior to $\\overline{{A B}}$ , so $0<t<1$ .   \nThe parameter $\\begin{array}{r}{t=\\frac{3}{4}}\\end{array}$ is in this range.   \nThe point $\\textrm{C}$ is distinct from $A$ and $B$ .   \nIf $C=A$ , then $t=0$ , $C=(\\textstyle{\\frac{1}{2}},0)$ .   \nIf $C=B$ , then $t=1$ , $C=(0,\\frac{\\sqrt{3}}{2})$ .   \nThe point of tangency $\\scriptscriptstyle\\mathcal{C}$ is the unique point on $\\overline{{A B}}$ (excluding endpoints) that lies on the astroid.   \nSolution: Let the family of segments $\\gamma^{\\ast}$ be defned by segments $P Q$ of unit length with $P$ on the positive $x$ -axis and $Q$ on the positive $y$ -axis. Let $P=(a,0)$ and $Q=(0,b)$ . The length condition gives $a^{2}+b^{2}=1$ . The segment $\\overline{{P Q}}$ lies on the line $\\begin{array}{r}{\\frac{x}{a}+\\frac{y}{b}=1}\\end{array}$ .   \nUsing the parametrization $a=\\cos\\theta$ , $b=\\sin\\theta$ with $\\begin{array}{r}{0<\\theta<\\frac{\\pi}{2}}\\end{array}$ , the equation of the line is $\\begin{array}{r}{\\frac{x}{\\cos\\theta}+\\frac{y}{\\sin\\theta}=1}\\end{array}$ .   \nThe envelope of this family of lines is the astroid $x^{2/3}+y^{2/3}=1$ .   \nThe segments from $\\boldsymbol{\\cdot}$ are tangent to this astroid.   \nThe segment $\\overline{{A B}}$ connects $A(\\frac{1}{2},0)$ and $B(0,\\frac{\\sqrt{3}}{2})$ . The equation of the line containing $\\overline{{A B}}$ is $y=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}$ . This segment is part of the family $\\gamma^{\\ast}$ .   \nWe are looking for a unique point $\\scriptscriptstyle\\mathcal{C}$ on $\\overline{{A B}}$ , distinct from $A$ and $B$ , that does not belong to any segment from $_{\\mathcal{F}}$ other than $\\overline{{A B}}$ . This means $\\scriptscriptstyle\\mathcal{C}$ is the point where $\\overline{{A B}}$ is tangent to the astroid.   \nThe slope of the tangent to the astroid at a point $(x,y)$ is $-\\sqrt[3]{\\frac{y}{x}}$ . The slope of $\\overline{{A B}}$ is $-\\sqrt{3}$ .   \nSetting the slopes equal: $-{\\sqrt[3]{\\frac{y}{x}}}=-{\\sqrt{3}}\\implies{\\frac{y}{x}}=3{\\sqrt{3}}$ .   \nThe point $\\scriptscriptstyle\\mathcal{C}$ lies on the line $y=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}$ and satisfes $y=3\\sqrt{3}x$ .   \nSubstituting, $\\begin{array}{r}{3\\sqrt{3}x=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}\\implies4\\sqrt{3}x=\\frac{\\sqrt{3}}{2}\\implies x=\\frac{1}{8}.}\\end{array}$ .   \nThen 𝑦 = 3√3( 81 ) = 3√8 3 .   \nThe point $C$ is $(\\textstyle{\\frac{1}{8}},\\textstyle{\\frac{3{\\sqrt{3}}}{8}})$ .   \nWe need to check if $\\scriptscriptstyle\\mathcal{C}$ lies on $\\overline{{A B}}$ . The points on $A B$ are $({\\textstyle{\\frac{1}{2}}}-{\\textstyle{\\frac{1}{2}}}t,{\\textstyle{\\frac{\\sqrt{3}}{2}}}t)$ for $0\\le t\\le1$ .   \nFor $\\begin{array}{r}{x=\\frac{1}{8}}\\end{array}$ $\\begin{array}{r}{=\\frac{1}{8},\\,\\frac{1}{2}-\\frac{1}{2}t=\\frac{1}{8}\\implies t=\\frac{3}{4}.}\\end{array}$ .   \nFor $\\begin{array}{r}{y=\\frac{3\\sqrt{3}}{8},\\frac{\\sqrt{3}}{2}t=\\frac{3\\sqrt{3}}{8}\\implies t=\\frac{3}{4}}\\end{array}$ .   \nSince $\\begin{array}{r}{0<\\frac{3}{4}<1}\\end{array}$ , $\\scriptscriptstyle\\mathcal{C}$ is an interior point of $A B$ .   \nThe point $C$ is the unique point on $\\overline{{A B}}$ (excluding endpoints) that lies on the astroid. The tangent to the astroid at $C$ is the segment $\\overline{{A B}}$ . Thus, $C$ does not belong to any other segment from $\\boldsymbol{\\cdot}$ .   \nFWien ahllayv, e caunladt $\\begin{array}{c c c}{{O C^{2}=(\\frac{1}{8})^{2}+(\\frac{3\\sqrt{3}}{8})^{2}=\\frac{1}{64}+\\frac{27}{64}=\\frac{28}{64}=\\frac{7}{16}}}\\end{array}$ .   \n$p=7$ $q=16$ $p+q=7+16=23$ .   \nFinal Answer: The fnal answer is 23 ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "ncorrect generation with 5923 thinking tokens and 754 answer toke ",
        "text_level": 1,
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "The problem asks for a point $\\textrm{C}$ on the segment $A B$ such that $C$ is not on any other segment from the family $\\mathcal{F}$ , except $\\overline{{A B}}$ itself. The family $\\gamma^{*}$ consists of segments $P Q$ of unit length, with $P$ on the $x$ -axis and $Q$ on the $y_{-}$ -axis in the frst quadrant. Let $P=(x,0)$ and $Q=(0,y)$ , where $x>0$ and $y>0$ . The length of the segment $\\overline{{P Q}}$ is 1, so the distance between $P$ and $Q$ is $1$ . $\\sqrt{(x-0)^{2}+(0-y)^{2}}=1$ $\\sqrt{x^{2}+y^{2}}=1$ $x^{2}+y^{2}=1$ The segment $P Q$ lies in the frst quadrant, so $x>0$ and $y>0$ . The equation $x^{2}+y^{2}=1$ represents the quarter circle in the frst quadrant, with endpoints at $(1,0)$ and $(0,1)$ . The points on the segment $\\overline{{P Q}}$ can be parameterized as $(1-t)x+t(0),(1-t)0+t y)=(x(1-t),y t),$ , where $0\\le t\\le1$ . The segment $A B$ connects $A(\\frac{1}{2},0)$ and $B(0,\\frac{\\sqrt{3}}{2})$ ). Slope $\\overline{{A B}}$ $\\begin{array}{r}{m=\\frac{\\frac{\\sqrt{3}}{2}-0}{0-\\frac{1}{2}}=\\frac{\\frac{\\sqrt{3}}{2}}{-\\frac{1}{2}}=-\\sqrt{3}.}\\end{array}$ $A(\\frac{1}{2},0)$ $\\begin{array}{l}{{y-0=-\\sqrt{3}(x-\\frac{1}{2})}}\\\\ {{y=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}}}\\end{array}$ The segment $\\overline{{A B}}$ lies in the frst quadrant, between $x=0$ and $\\begin{array}{r}{x=\\frac{1}{2}}\\end{array}$ Alternatively, we can parameterize the points on $A B$ . Let $\\scriptscriptstyle\\mathcal{C}$ be a point on $\\overline{{A B}}$ . Then $C=(1-\\lambda)A+\\lambda B$ for some $0\\leq\\lambda\\leq1$ . $\\begin{array}{r}{C=(1-\\lambda)(\\frac{1}{2},0)+\\lambda(0,\\frac{\\sqrt{3}}{2})=(\\frac{1}{2}-\\frac{1}{2}\\lambda,\\frac{\\sqrt{3}}{2}\\lambda)}\\end{array}$ . We are looking for a unique point $\\textrm{C}$ on $A B$ , distinct from $A$ and $B$ , that does not belong to any segment from $\\gamma^{*}$ other than $A B$ . Consider a segment from $\\gamma^{*}$ , with endpoints $P=(x,0)$ and $Q=(0,y)$ , where $x^{2}+y^{2}=1$ . A point on this segment is of the form $(x(1-t),y t)$ , where $0\\leq t\\leq1$ . We want to fnd a point $\\textrm{C}$ on $\\overline{{A B}}$ , $C\\neq A,B$ , such that if $\\scriptscriptstyle\\mathcal{C}$ lies on a segment ${\\overline{{P Q}}}\\in{\\mathcal{F}}$ , then ${\\overline{{P Q}}}={\\overline{{A B}}}$ . Let $\\begin{array}{r}{C=(\\frac{1}{2}-\\frac{1}{2}\\lambda,\\frac{\\sqrt{3}}{2}\\lambda)}\\end{array}$ , where $0<\\lambda<1$ . If $\\scriptscriptstyle\\mathcal{C}$ lies on a segment $P Q$ from $\\gamma^{*}$ , then there exist 𝑥, $y>0$ with $x^{2}+y^{2}=1$ , and $t\\in[0,1]$ such that: ${\\textstyle\\frac{1}{2}}-{\\textstyle\\frac{1}{2}}\\lambda=x(1-t)$ √3 𝜆 = 𝑦𝑡 From the frst equation, 1 − 𝑡 = 2 − 2 𝜆 = 1−𝜆 . From the second equation, $\\begin{array}{r}{t=\\frac{\\frac{\\sqrt{3}}{2}\\lambda}{y}=\\frac{\\sqrt{3}\\lambda}{2y}}\\end{array}$ Since $t\\in[0,1]$ , we have $1-t\\in[0,1]$ . Also, $x>0$ , so $\\begin{array}{r}{\\frac{1-\\lambda}{2x}>0}\\end{array}$ , which implies $1-\\lambda>0$ , or $\\lambda<1$ . This is consistent with the condition for point on $A B$ . We have 1 − 𝑡 = 12−𝜆 and 𝑡 = √23𝜆 . Adding these equations: 1 = 1−𝜆 + √3𝜆 $2x y=(1-\\lambda)y+\\sqrt{3}\\lambda x$ ",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "We know $x^{2}+y^{2}=1$ .   \nIf the point $C$ belongs to the segment $A B$ itself, then the endpoints of the segment are $A$ and $B$ .   \nSo, the segment is from $({\\frac{1}{2}},0)$ to $(0,{\\frac{\\sqrt{3}}{2}})$ .   \nHere, 𝑥 = 21 and 𝑦 = √23 .   \nCheck if $x^{2}+y^{2}=1$ : $\\begin{array}{r}{(\\frac{1}{2})^{2}+(\\frac{\\sqrt{3}}{2})^{2}=\\frac{1}{4}+\\frac{3}{4}=1.\\mathrm{~So~}\\overline{{A B}}\\in\\mathcal{F}.}\\end{array}$   \nIf $\\scriptscriptstyle\\mathcal{C}$ belongs to another segment $P Q$ from $\\gamma^{*}$ , with endpoints $P=(x,0)$ and $Q=(0,y)$ , where $(x,y)\\neq(\\textstyle{\\frac{1}{2}},\\textstyle{\\frac{\\sqrt{3}}{2}})$ . Consider the geometric interpretation. The family $\\gamma^{*}$ consists of segments of unit length with endpoints on the axes. The midpoints of these segments lie on the circle $\\begin{array}{r}{(x-\\frac{1}{2})^{2}+(y-\\frac{1}{2})^{2}=\\frac{1}{2}}\\end{array}$ .   \nLet the point $\\scriptscriptstyle\\mathcal{C}$ correspond to some $\\lambda$ .   \nIf $\\scriptscriptstyle\\mathcal{C}$ lies on $\\overline{{P Q}}$ , then $C=(x(1-t),y t)$ .   \nSo, ${\\textstyle{\\frac{1}{2}}}-{\\textstyle{\\frac{1}{2}}}\\lambda=x(1-t)$ and ${\\frac{\\sqrt{3}}{2}}\\lambda=y t$ .   \nWe need to fnd a $\\lambda\\in(0,1)$ such that the only solution to these equations, with $x^{2}+y^{2}=1,x,y>0,$ , is $\\begin{array}{r}{x=\\frac{1}{2}}\\end{array}$ and $y=\\frac{\\sqrt{3}}{2}$ . From the equations, 1 − 𝑡 = 12−𝜆 and 𝑡 = √23𝜆 .   \nSubstitute these into $x^{2}+y^{2}=1$ .   \nTry a diferent approach. Consider the tangents to the family of segments.   \nThe segments are formed by points $(x,0)$ and $(0,y)$ with $x^{2}+y^{2}=1$ .   \nThe equation of the line containing the segment $P Q$ is $\\textstyle{\\frac{x}{x}}+{\\frac{y}{y}}=1$ , or $\\textstyle{\\frac{x}{x}}+{\\frac{y}{y}}=1$ .   \nThe equation of the line is $\\textstyle{\\frac{x}{x}}+{\\frac{y}{y}}=1$ .   \nConsider the equation of the segment in parametric form: $(x(1-t),y t)$ .   \nIf a point $\\scriptscriptstyle\\mathcal{C}$ lies on this segment, then $C=(x_{c},y_{c})$ , where $x_{c}=x(1-t)$ and $y_{c}=y t$ .   \nSo, $\\begin{array}{r}{\\frac{x_{c}}{x}=1-t}\\end{array}$ and $\\begin{array}{r}{\\frac{y_{c}}{y}=t}\\end{array}$ .   \n$\\begin{array}{r}{\\frac{x_{c}}{x}+\\frac{y_{c}}{y}=1-t+t=1.}\\end{array}$ .   \nThe segments from $f^{\\star}$ are parts of the lines $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ , where $x_{0}^{2}+y_{0}^{2}=1$ , $x_{0},y_{0}>0$ .   \nConsider the point $C=(\\textstyle{\\frac{1}{2}}-\\textstyle{\\frac{1}{2}}\\lambda,\\textstyle{\\frac{\\sqrt{3}}{2}}\\lambda)$ .   \nIf $\\scriptscriptstyle\\mathcal{C}$ lies on a segment from $\\mathcal{F}$ with endpoints $(x,0)$ and $(0,y)$ , then:   \n$\\begin{array}{r l}&{\\frac{\\frac{1}{2}-\\frac{1}{2}\\lambda}{x}+\\frac{\\frac{\\sqrt{3}}{2}\\lambda}{y}=1}\\\\ &{\\frac{1-\\lambda}{2x}+\\frac{\\sqrt{3}\\lambda}{2y}=1}\\end{array}$   \nWe need to fnd a unique $\\lambda\\in(0,1)$ such that the only solution $(x,y)$ to this equation, with $x^{2}+y^{2}=1,x,y>0,$ , is $({\\frac{1}{2}},{\\frac{\\sqrt{3}}{2}})$ Substitute $\\begin{array}{r}{x=\\frac{1}{2}}\\end{array}$ and $y=\\frac{\\sqrt{3}}{2}$ into the equation:   \n21( −21 𝜆) + 2(√√33𝜆 ) = 1   \n$1-\\lambda+\\lambda=1$   \n$1=1$   \nThis shows that any point on $A B$ lies on the segment $A B$ itself.   \nConsider the line containing 𝐴𝐵: 𝑦 = −√3𝑥 + √23 .   \nIf a point $\\scriptscriptstyle\\mathcal{C}$ on $\\overline{{A B}}$ also lies on another segment from $\\gamma^{*}$ with endpoints $(x,0)$ and $(0,y)$ , then:   \n$\\begin{array}{r}{\\frac{x_{C}}{x}+\\frac{y_{C}}{y}=1}\\end{array}$ .   \nLet the point $\\scriptscriptstyle\\mathcal{C}$ divide $\\overline{{A B}}$ in the ratio $\\mu:(1-\\mu)$ , where $0<\\mu<1$ .   \n$\\begin{array}{r}{C=(1-\\mu)A+\\mu B=(1-\\mu)(\\frac12,0)+\\mu(0,\\frac{\\sqrt{3}}{2})=(\\frac12-\\frac12\\mu,\\frac{\\sqrt{3}}{2}\\mu).}\\end{array}$ .   \nIf $\\scriptscriptstyle\\mathcal{C}$ lies on a segment with endpoints $(x,0)$ and $(0,y)$ , then:   \n12 − 12 𝜇 + √23 𝜇 = 1.   \nWe are looking for a point $\\textrm{C}$ such that the line passing through $\\scriptscriptstyle\\mathcal{C}$ with the form $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ , where $x_{0}^{2}+y_{0}^{2}=1$ , is unique. Consider the tangents to the unit circle centered at the origin. The lines of the form $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ are tangents to the circle $x^{2}+y^{2}=r^{2}$ .   \nConsider the dual problem. The family of segments corresponds to points on the quarter circle $x^{2}+y^{2}=1$ in the frst quadrant.   \nThe lines containing the segments are $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ .   \nThe envelope of these lines is the curve whose tangents are these lines.   \nThe equation $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ , with the constraint $x_{0}^{2}+y_{0}^{2}=1$ .   \nLet $x_{0}=\\cos\\theta$ , $y_{0}=\\sin\\theta$ , where $\\begin{array}{r}{0<\\theta<\\frac{\\pi}{2}}\\end{array}$ .   \nThe equations of the lines are $x\\cos\\theta+y\\sin\\theta=1$ .   \nThe envelope of the family of lines $x\\cos\\theta+y\\sin\\theta=1$ is the circle $x^{2}+y^{2}=1$ .   \nThe segments from $f^{\\star}$ are parts of the tangents to the unit circle in the frst quadrant.   \nThe point $\\textrm{C}$ on $A B$ that does not belong to any other segment from $f^{\\star}$ means that the tangent to the unit circle passing through $\\textrm{C}$ is unique.   \nConsider the point $\\begin{array}{r}{C=(\\frac{1}{2}-\\frac{1}{2}\\lambda,\\frac{\\sqrt{3}}{2}\\lambda)}\\end{array}$ .   \nThe tangents to the unit circle passing through $\\textrm{C}$ have the form $x x_{C}+y y_{C}=1$ .   \nIf the tangent is unique, then $\\scriptscriptstyle\\mathcal{C}$ lies on the unit circle.   \n$\\begin{array}{r}{(\\frac{1}{2}-\\frac{1}{2}\\lambda)^{2}+(\\frac{\\sqrt{3}}{2}\\lambda)^{2}=1}\\end{array}$   \n$\\begin{array}{r}{(\\frac{1}{4}-\\frac{1}{2}\\lambda+\\frac{1}{4}\\lambda^{2})+\\frac{3}{4}\\lambda^{2}=1}\\end{array}$   \n1 − 1 𝜆 + 𝜆2 = 1   \n4𝜆2 − 2𝜆 + 1 = 4   \n4𝜆2 − 2𝜆 − 3 = 0   \nSolve for $\\lambda$ :   \n$\\begin{array}{r}{\\lambda=\\frac{-(-2)\\pm\\sqrt{(-2)^{2}-4(4)(-3)}}{2(4)}=\\frac{2\\pm\\sqrt{4+48}}{8}=\\frac{2\\pm\\sqrt{52}}{8}=\\frac{2\\pm2\\sqrt{13}}{8}=\\frac{1\\pm\\sqrt{13}}{4}.}\\end{array}$   \nWe need $0<\\lambda<1$ .   \n$\\sqrt{13}\\approx3.6$ .   \n1√+3.6 = 4.6 > 1.   \n1−43.6 = −42.6 < 0.   \nSomething is wrong. The point $\\scriptscriptstyle\\mathcal{C}$ is inside the unit circle.   \nLet’s rephrase the condition. There is a unique point $\\textrm{C}$ on $A B$ such that if $C$ lies on a segment ${\\overline{{P Q}}}\\in{\\mathcal{F}},$ then ${\\overline{{P Q}}}={\\overline{{A B}}}$ . Let the point $\\scriptscriptstyle\\mathcal{C}$ on $\\overline{{A B}}$ be $({\\textstyle{\\frac{1}{2}}}-{\\textstyle{\\frac{1}{2}}}\\lambda,{\\frac{\\sqrt{3}}{2}}\\lambda)$ .   \nConsider a segment from $\\gamma^{*}$ with endpoints $(x,0)$ and $(0,y)$ , $x^{2}+y^{2}=1$ .   \nThe parametric form of the segment is $(x(1-t),y t)$ ).   \nIf $\\scriptscriptstyle\\mathcal{C}$ is on this segment, then for some $t\\in[0,1]$ :   \n${\\textstyle\\frac{1}{2}}-{\\textstyle\\frac{1}{2}}\\lambda=x(1-t)$   \n√3𝜆 = 𝑦𝑡   \nIf the segment is unique, then the values of $\\mathscr{X}$ and $y$ are unique.   \nThe only solution to 12−𝑥𝜆 $\\begin{array}{r}{\\frac{1-\\lambda}{2x}+\\frac{\\sqrt{3}\\lambda}{2y}=1}\\end{array}$ , with $x^{2}+y^{2}=1$ , $x,y>0,$ should be $\\begin{array}{r}{x=\\frac{1}{2},y=\\frac{\\sqrt{3}}{2}}\\end{array}$ .   \nConsider the intersection of the line containing $A B$ with the boundary of the region formed by the segments in $\\mathcal{F}$ . The boundaries are the $\\mathcal{X}$ -axis, 𝑦-axis, and the quarter circle $x^{2}+y^{2}=1$ .   \nConsider the dual problem again. The family of lines $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ , where $x_{0}^{2}+y_{0}^{2}=1,x_{0},y_{0}>0.$ .   \nThe envelope of these lines is the circle $x^{2}+y^{2}=1$ .   \nA point belongs to a unique segment from $\\gamma^{*}$ if it lies on the circle $x^{2}+y^{2}=1$ .   \nThe point $\\textrm{C}$ on $\\overline{{A B}}$ satisfes $C=t A+(1-t)B.$ .   \nLet’s consider the case where the segment is $A B$ . Here $\\begin{array}{r}{x=\\frac{1}{2},y=\\frac{\\sqrt{3}}{2}}\\end{array}$ .   \nThe point $\\textrm{C}$ can be represented as $(\\frac{1}{2}(1-s),\\frac{\\sqrt{3}}{2}s)$ for $0\\leq s\\leq1$ .   \nWe are looking for a point $\\textrm{C}$ on $A B$ that does not lie on any other segment from $\\gamma^{\\ast}$ . Consider the geometry. The family $\\gamma^{*}$ forms a region bounded by the axes and the quarter circle. The segments are chords of circles centered on the line $y=x$ , tangent to the axes. Let the endpoints of a segment be $(\\cos\\theta,0)$ and $(0,\\sin\\theta)$ ). A point on the segment is $(\\cos\\theta(1-t),\\sin\\theta t).$ .   \nThe point $\\begin{array}{r}{C=(\\frac{1}{2}-\\frac{1}{2}\\lambda,\\frac{\\sqrt{3}}{2}\\lambda)}\\end{array}$ . If $C$ lies on such a segment, then $\\frac{\\frac{1}{2}\\!-\\!\\frac{1}{2}\\lambda}{\\cos\\theta}=1-t$ and ${\\frac{{\\frac{\\sqrt{3}}{2}}\\lambda}{\\sin\\theta}}=t$ $\\begin{array}{r}{\\frac{1-\\lambda}{2\\cos\\theta}+\\frac{\\sqrt{3}\\lambda}{2\\sin\\theta}=1.}\\end{array}$   \nWe need a unique $\\lambda\\in(0,1)$ such that the only solution $(\\cos\\theta,\\sin\\theta)$ is $({\\frac{1}{2}},{\\frac{\\sqrt{3}}{2}})$ , which corresponds to $\\textstyle\\theta={\\frac{\\pi}{3}}$ . Consider the function $\\begin{array}{r}{f(\\theta)=\\frac{1}{2\\cos\\theta}-\\frac{\\lambda}{2\\cos\\theta}+\\frac{\\sqrt{3}\\lambda}{2\\sin\\theta}.}\\end{array}$   \nWe need $f(\\theta)=1$ .   \nIf $\\begin{array}{r}{\\lambda=\\frac{1}{2}.}\\end{array}$ , then $C=(\\frac{1}{4},\\frac{\\sqrt{3}}{4})$ . $\\begin{array}{r l}&{\\frac{1-\\operatorname{tr}\\big(2\\,\\frac{1}{2}\\big)^{2}}{\\operatorname{tr}\\big(\\frac{1}{2}+\\frac{3}{2}\\big)+\\frac{3}{2}}-\\frac{4}{3}\\,\\mathrm{tr}(2)}\\\\ &{\\qquad-\\frac{3}{4}\\,\\mathrm{tr}\\big(3-\\frac{1}{2}\\big)}\\\\ &{\\qquad+\\frac{3}{2}\\,\\mathrm{tr}\\big(1-\\frac{3}{4}\\big)}\\\\ &{\\qquad-\\frac{3}{4}\\,\\mathrm{tr}\\big(3-\\frac{1}{2}\\big)}\\\\ &{\\qquad+\\frac{3}{2}\\,\\mathrm{tr}\\big(2-\\frac{3}{4}\\big)}\\\\ &{\\qquad\\mathrm{in}\\,\\partial\\,\\rho=\\mathrm{thin}\\,\\partial\\,\\mathrm{tr}\\big(\\frac{3}{2}\\big)}\\\\ &{\\qquad\\mathrm{Distic~ln}\\,\\partial\\,\\mathrm{tr}^{2}\\geq\\mathrm{in}\\,\\partial\\,\\mathrm{t}\\partial\\,\\mathrm{t}\\partial\\,\\mathrm{t}\\partial\\,\\mathrm{m}+\\frac{3}{2}\\,\\mathrm{tr}\\big(2\\vartheta\\big)}\\\\ &{\\qquad\\mathrm{in}\\,\\partial\\,\\mathrm{tr}\\big(\\vartheta+\\frac{1}{2}\\big)=\\mathrm{in}\\,\\partial\\,\\mathrm{tr}\\big(\\vartheta\\big)}\\\\ &{\\qquad\\partial\\,\\mathrm{tr}\\big(\\vartheta+\\frac{1}{2}\\big)=\\mathrm{in}\\,\\partial\\,\\mathrm{tr}\\big(\\vartheta+\\frac{3}{2}\\big)=\\mathrm{tr}-2\\vartheta+2\\kappa\\,\\mathrm{tr}}\\\\ &{\\qquad\\partial\\,\\mathrm{tr}\\big(\\vartheta-\\frac{1}{2}\\big)=\\mathrm{tr}\\big(2\\,\\frac{1}{2}\\big)}\\\\ &{\\qquad\\mathrm{in}\\,\\partial\\,\\mathrm{tr}\\big(\\vartheta+\\frac{1}{2}\\big)=0\\,\\partial\\,\\mathrm{tr}\\big(\\vartheta-\\frac{3}{2}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\mathrm{if}\\,\\mathrm{tr}=0\\,\\frac{\\beta}{2}\\,\\mathrm{tr}}\\\\ &{\\qquad\\qquad\\mathrm{if}\\,\\mathrm{tr}=1,\\theta=\\frac{3}{2}\\,\\frac{\\beta}{2}-\\frac{3}{2}\\,\\frac{2\\beta+\\delta}{2}=\\frac{3}{6}\\,\\mathrm{tr}\\big(2-\\$   \nIf $\\begin{array}{r}{\\lambda=\\frac{1}{2}}\\end{array}$ , the point is $({\\frac{1}{4}},{\\frac{\\sqrt{3}}{4}})$ .   \nThis point lies on segments corresponding to $\\textstyle\\theta={\\frac{\\pi}{3}}$ and $\\begin{array}{r}{\\theta=\\frac{2\\pi}{9}}\\end{array}$ . The unique point $\\textrm{C}$ means that for other values of $\\lambda$ , there is more than one solution for $(x,y)$ . Consider the tangent to the unit circle passing through $C$ . If the tangent is unique, $\\scriptscriptstyle\\mathcal{C}$ is on the unit circle.   \nThis leads to $\\begin{array}{r}{\\lambda=\\frac{1\\pm\\sqrt{13}}{4_{.}}}\\end{array}$ , neither of which is in $(0,1)$ . Consider the geometric interpretation of the family $\\gamma^{\\ast}$ . The segments connect points on the axes such that the distance is 1. The locus of the centers of these segments is the circle $\\begin{array}{r}{(x-\\frac{1}{2})^{2}+(y-\\frac{1}{2})^{2}=\\frac{1}{2}}\\end{array}$ , in the frst quadrant.   \nIf a point $\\scriptscriptstyle\\mathcal{C}$ lies on a segment from $\\gamma^{*}$ , then $\\scriptscriptstyle\\mathcal{C}$ is on a line segment connecting $(x,0)$ and $(0,y)$ with $x^{2}+y^{2}=1$ .   \nThe line containing the segment is $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ .   \nIf $\\scriptscriptstyle\\mathcal{C}$ belongs to a unique segment, then there is a unique line of this form passing through $C$ .   \nThe point $\\textrm{C}$ on $\\overline{{A B}}$ is $(\\frac{1}{2}-\\frac{1}{2}\\lambda,\\frac{\\sqrt{3}}{2}\\lambda)$ .   \nWe need a unique (𝑥0, 𝑦0) with 𝑥20 + 𝑦20 = 1, 𝑥0, 𝑦0 > 0, such that 12 −𝑥 12 𝜆 + √2𝑦3 𝜆 = 1.   \nLet the point $\\scriptscriptstyle\\mathcal{C}$ be at the g $\\scriptscriptstyle\\mathcal{C}$ with the form0 $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ is unique, subject to $x_{0}^{2}+y_{0}^{2}=1$ . This line is tangent to the unit circle. The point $\\textrm{C}$ lies on the intersection of the line containing $A B$ and the unit circle. $({\\textstyle{\\frac{1}{2}}}-{\\textstyle{\\frac{1}{2}}}\\lambda)^{2}+({\\textstyle{\\frac{\\sqrt{3}}{2}}}\\lambda)^{2}=1$ . We found no solution for $\\lambda\\in(0,1)$ . The unique point $\\scriptscriptstyle\\mathcal{C}$ means that if any other segment passes through $C$ , it must be $\\overline{{A B}}$ .   \nConsider the slope of the line containing the segment connecting $(x,0)$ and $(0,y)$ . Slope is $-{\\frac{y}{x}}$ .   \nThe equation is $y=-\\textstyle{\\frac{y}{x}}x+y$ .   \nIf 𝐶 lies on the segment, then 1−𝜆 + √3𝜆 = 1.   \nConsider the geometric mean.   \nLet the point $\\scriptscriptstyle\\mathcal{C}$ have coordinates $(a,b)$ . The segment connecting $(x,0)$ and $(0,y)$ passes through $(a,b)$ .   \n$\\textstyle{\\frac{a}{x}}+{\\frac{b}{y}}=1$ .   \nIf 𝐶 is on 𝐴𝐵, then 𝑏 = −√3𝑎 + √3 .   \n𝑎 = 12 − 12 𝜆, 𝑏 = √23 𝜆.   \nThe point $\\textrm{C}$ is tangent to the envelope of the family of segments. ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Solution: ",
        "text_level": 1,
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Let the family of segments be $\\gamma^{\\ast}$ . The segments have endpoints $P=(x,0)$ and $Q=(0,y)$ with $x^{2}+y^{2}=1$ and $x,y>0$ .   \nThe points on the segment $\\overline{{P Q}}$ satisfy $\\begin{array}{r}{\\frac{x}{x}+\\frac{y}{y}=1}\\end{array}$ . The segments lie on the lines $\\begin{array}{r}{\\frac{x}{x_{0}}+\\frac{y}{y_{0}}=1}\\end{array}$ , where $x_{0}^{2}+y_{0}^{2}=1$ , $x_{0},y_{0}>0$ .   \nThe segment $A B$ has endpoints $A(\\frac{1}{2},0)$ and $B(0,\\frac{\\sqrt{3}}{2})$ . The equation of the line containing $\\overline{{A B}}$ is $y=-\\sqrt{3}x+\\frac{\\sqrt{3}}{2}$ .   \nA point $C$ on $\\overline{{A B}}$ can be parameterized as $\\begin{array}{r}{C=(\\frac{1}{2}-\\frac{1}{2}\\lambda,\\frac{\\sqrt{3}}{2}\\lambda)}\\end{array}$ , where $0\\leq\\lambda\\leq1$ .   \nIf 𝐶 belongs to a segment from  with endpoints (𝑥, 0) and (0, 𝑦), then 21 −𝑥12 𝜆 + √2𝑦3 𝜆 We are looking for a unique point $\\scriptscriptstyle\\mathcal{C}$ on $\\overline{{A B}}$ , distinct from $A$ and $B$ , that does not belong to any segment from $\\gamma^{\\ast}$ other than $\\overline{{A B}}$ . This means that the only solution $(x,y)$ to the equation with $x^{2}+y^{2}=1$ , $x,y>0$ , is $({\\frac{1}{2}},{\\frac{\\sqrt{3}}{2}})$ . The segments from $\\boldsymbol{\\cdot}$ are parts of the tangents to the unit circle. The point $C$ must be tangent to the envelope of the family of segments. The envelope is the unit circle itself.   \nThe unique point $\\scriptscriptstyle\\mathcal{C}$ on $\\overline{{A B}}$ , distinct from $A$ and $B$ , that lies on the unit circle. We need to solve: $\\begin{array}{r}{(\\frac{1}{2}-\\frac{1}{2}\\lambda)^{2}+(\\frac{\\sqrt{3}}{2}\\lambda)^{2}=1}\\end{array}$ $\\begin{array}{r}{\\lambda=\\frac{1}{2}}\\end{array}$ . The point $C=(\\frac{1}{4},\\frac{\\sqrt{3}}{4})$ . $\\begin{array}{c c c}{{O C^{2}=(\\frac{1}{4})^{2}+(\\frac{\\sqrt{3}}{4})^{2}=\\frac{1}{16}+\\frac{3}{16}=\\frac{4}{16}=\\frac{1}{4}.}}\\end{array}$ $O C^{2}={\\frac{1}{4}}={\\frac{p}{q}}$ , so $p=1$ , $q=4$ . $p$ and $q$ are relatively prime. $p+q=1+4=5$ . Final Answer: The fnal answer is 5 ",
        "page_idx": 44
    }
]