[
    {
        "type": "text",
        "text": "From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Xingchen Wan1, Han Zhou1 3\\*, Ruoxi $\\mathbf{Sun}^{2^{*}}$ , Hootan Nakhost1, Ke Jiang1 and Sercan Ö. Arık1 1Google Cloud AI Research, 2Google DeepMind, 3University of Cambridge, \\*Work done at Google Cloud AI Research ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefts. However, despite its promise, it is unclear what aspects dominate the benefts and whether simply scaling to more examples is the most efective way of improving many-shot ICL. In this work, we frst provide an analysis of the factors driving many-shot ICL, and we fnd that 1) many-shot performance can still be attributed to often a few disproportionately infuential examples and 2) identifying such infuential examples (“optimize”) and using them as demonstrations to regenerate new examples (“generate\") can lead to further improvements. Inspired by the fndings, we propose bridge, an algorithm that alternates between the optimize step with Bayesian optimization to discover the infuential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of diferent sizes, we show that bridge led to signifcant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1. Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Recent advances in large language models (LLMs) have led to the emergence of in-context learning (ICL) as a promising new learning paradigm (Brown et al., 2020). ICL allows LLMs to learn tasks by simply being presented with a few examples within their context window. A key bottleneck for ICL has been the supported context length of LLMs, but with advancements in novel model architectures, computational infrastructures, and efcient serving methods, state-of-the-art models such as Gemini (Anthropic, 2024; Reid et al., 2024) feature context windows of millions of tokens are overcoming this limitation. Such long-context LLMs open unprecedented avenues for the scaling of ICL – whereas previous LLMs were limited to processing only up to dozens of examples, current LLMs can now accommodate signifcantly more examples. More importantly, beyond merely supporting a longer context, it has also been shown that scaling more examples led to substantial performance improvements across tasks, creating a new promising paradigm known as many-shot learning (Agarwal et al., 2024; Bertsch et al., 2024). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Despite these advances, as a nascent paradigm, many-shot ICL still faces several challenges. Long context windows, while powerful, are computationally expensive and introduce signifcant latency and cost to serving, making it impractical or uneconomical to fully exploit the maximum context length. Some trade-of decisions have to be made under virtually any realistic setting. To leverage the expanded context while controlling the cost and latency under an acceptable limit, existing works typically investigate the experimental setting whereas many examples as costs permit are simply randomly sub-sampled from the pool of all available examples and dumped into the context window. As observed both in prior works (Agarwal et al., 2024) and our investigations (Fig. 1), using the same number of examples but with diferent combinations of examples as demonstrations can lead to dramatically diferent performance for the same task. Across diferent tasks, it has also been noted that the model behaves very diferently when the number of examples is scaled up, with some showing a near-monotonic increase in performance as more examples are added, while others experience performance plateaus (e.g., gray line in the leftmost subfgure of Fig. 1) or even degradation (e.g., red line in the rightmost subfgure of Fig. 4). Understandably, such variability could pose challenges for practitioners and present obstacles to the application of many-shot learning as an efective paradigm in practice. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "To address these, this paper aims to answer key research questions and proposes an efective novel approach. First, we analyze the factors driving the many-shot ICL in the reinforced ICL setup common in challenging reasoning tasks where we are provided with a labeled set of inputs and fnal labels, but the intermediate reasoning path has to be model-generated. We fnd that while ICL performance often increases with the number of shots, that improvement can often be at least partially attributed to a much smaller subset of examples that highly disproportionately contribute to the overall task performance – as we scale the number of examples, the probability of including these examples also increases. In many cases, if, however, we judiciously isolate these infuential examples from the rest, the “many-shot” performance can be matched or even exceeded with this sometimes extremely small subset of well-chosen examples alone while adding more examples beyond this set often provides little beneft or even harms performance. We also argue that the fndings explain some of the phenomena observed. For example, uneven infuence can lead to high variance across diferent combinations of examples, whereas plateauing performance may occur when we run out of good examples with positive performance infuences. One natural implication of these is the efciency gains by reducing redundancy in many-shot ICL and identifying the optimized subsets. However, the natural next question to ask is whether scaling ICL examples in LLMs can still be benefcial after using up all benefcial examples identifed in the previous step. We answer afrmatively to this: to still leverage LLMs’ long context, these optimized, high-performing examples may serve as demonstrations to re-generate the more efective reasoning paths rationales on the train set back into the many-shot regime, which we fnd to often outperform both the original many-shot examples and the optimized examples themselves. Building on these insights, we propose Bayesian Refnement and Iterative Demonstration Generation for Examples (bridge), a search algorithm based on Bayesian optimization to improve many-shot ICL and bridges the few- and many-shot learning paradigms by automating the “optimize” and “generate” steps above iteratively. In the “optimize” step, it frames the problem as a combinatorial optimization task to discover the optimal set of demonstrations (i.e., many-to-few), and in the “generate” step, it uses the optimal set as seed examples to generate more examples for further performance enhancement (i.e., few-to-many). We demonstrate the efectiveness of bridge on Gemini, Mistral, and Claude models across a diverse range of tasks, including symbolic reasoning, numerical reasoning, and text-to-SQL generation. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2. What Drives Many-Shot In-Context Learning Performance? ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Several previous studies on many-shot ICL (Agarwal et al., 2024; Bertsch et al., 2024) have investigated the presence of performance gains when we scale the number of examples. A key question that remains unanswered, though, is what exactly leads to this improvement. For example, it is unknown whether the beneft is from scaling examples itself due to expanded knowledge in the context via more examples or because including more examples increases the probability of selecting a small subset of disproportionately positive examples, or a combination of the above with some task specifcity. We argue that answering this question is critical – if the beneft comes from expanded knowledge from including more examples, it suggests that scaling and addressing long-context understanding challenges would dominate the end-to-end performance improvements, and future studies should aim to either include as many examples as practically possible or to imitate the behavior of the LLM as if many examples are included. If, on the other hand, the performance is dominated by a small efective subset of examples, more intelligent selection aiming to reduce redundancies and identify the high-performing subsets should outweigh näively scaling examples. ",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/bd0425b8da70883daf19c34c117468ce273a168df6316a2bebd1096f92c1f23a.jpg",
        "img_caption": [
            "Figure 1 It does not always take “many shots” to achieve many-shot performance – with judicious selection, it is possible to match or exceed many-shot performance achieved by using all available examples) with much fewer examples: Accuracy on held-out splits against the number of examples on 3 BBH tasks of 1) overall trendline (ftted with locally weighted smoothing (lowess)), 2) using top-K most positive examples, or 3) using bottom-K least positive examples based on the ranking of the importance score described in Sec 2. Dotted lines refer to two many-shot baselines: reinforced ICL: using input, model-generated reasoning and output of all correctly-predicted inputs; All example: using all available input-output pairs from the train set. Lines and error bars show mean $\\pm$ standard deviation across 3 runs with the ordering of the examples shufed each trial. "
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Prior work on few-shot setup has studied related problems such as the sensitivity to examples in the context (Zhao et al., 2021; Zhou et al., 2024b). However, it is presently unknown to what extent the fndings still scale to the many-shot ICL setup because 1) in the many-shot setup, the infuence of each individual example would get much smaller, and 2) it is unknown whether careful example selection in the few-shot setup is still necessary if all examples can be included in the context, since by defnition, any high-performing examples are subsets of all examples – if the long-context LLM is perfectly capable of identifying the most relevant pieces of information. If so, aside from other practical concerns like cost and latency, the need for users to manually curate examples may no longer be required. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Setup. We aim to shed insights on these important questions. We use the Gemini $1.5\\;\\mathrm{Pro}$ (Reid et al., 2024), the state-of-the-art long-context model, to focus on several representative tasks from the BBH tasks. All three tasks, as shown in by the gray lines in Fig. 1, beneft from increasing number of examples to varying degrees (in logical_deduction, the performance initially increases with the number of examples before plateauing and decreasing; in the other two tasks, there is a noisy but near monotonic improvement throughout) – we will test the key fndings in a much more extensive collection of tasks in Sec. 4. Given the increased emphasis of modern LLMs on problem-solving and reasoning, we primarily focus on these tasks and adopt the reinforced ICL (Agarwal et al., 2024) setup, where we assume the availability of a labeled set of inputs and fnal labels to be used as many-shot demonstrations, whereas any intermediate outputs or rationales leading to the fnal answer are model-generated and modifable (although we also conduct preliminary experiments in alternative setups such as low-resource machine translation in App. C.4). Lastly, we primarily focus on the tasks with the number of available labeled data up to 150-200 samples – while modern LLMs can often accommodate even more examples in the context, we focus on this range because 1) we believe it is the most practically relevant and flls an important gap that neither few-shot ICL nor supervised (parameter-efcient) fne-tuning (which usually requires hundreds to thousands of examples) conventionally address, and 2) while possible and of academic value, scaling beyond this range typically starts incurring signifcant latency and computational overhead, which scales quadratically w.r.t the input length for exact attention and is thus often practically less desired for most real-world use cases. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Many-shot performance can still be driven by few high-performing examples. A key test that would distinguish and disentangle the two possible sources of benefts from scaling mentioned at the beginning of this section is whether we can attribute, at least to a large extent, the performance improvement from scaling examples back to a carefully selected, high-performing subset of examples with disproportionate infuence. Formally, given a set of examples $\\mathcal{E}=\\{e_{j}\\}_{j=1}^{m}$ and a performance metric to be maximized $g(\\cdot):\\mathscr{P}(\\mathscr{E})\\rightarrow\\mathbb{R}$ (in this case, the accuracy on the validation set In this setup, the goal is to fnd whether we can construct a subset $\\mathbf{e}^{*}=\\{e_{i}^{*}\\}_{i=1}^{n}\\subset\\mathcal{E},$ , $\\mathbf{s.t.}n\\ll m$ such that $g(\\mathbf{e}^{\\ast})$ is much better than a randomly selected set of examples e of similar size and/or can even be comparable or better than using the full set of examples $g(\\mathcal{E})$ in the context. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Whereas a conclusive test would involve enumerating and evaluating $g(\\cdot)$ on the power set of $\\varepsilon$ with $|\\mathcal{P}(\\mathcal{E})|=2^{|\\mathcal{E}|}$ , it is clearly computationally intractable, and a natural simplifcation is whether we can rank the individual examples in $\\varepsilon$ with some importance scoring function $M(e)$ to construct example subsets based on the example ranking. While many possible formulations of this are possible, here we defne $M(e)$ based on imputed input gradient, which is a concept used in interpretable machine learning for importance attribution (Samek et al., 2021; Selvaraju et al., 2017; Simonyan, 2013; Sundararajan et al., 2017). In our context, directly computing input gradient is impossible as we only assume black-box LLMs without gradient backpropagation and $g(\\cdot)$ is not necessarily diferentiable. To bypass these issues, we use a sample-efcient Gaussian process regressor (GPR) (Williams and Rasmussen, 1995, 2006) to approximate $g(\\cdot)$ with ${\\hat{g}}(\\cdot)$ , whose input gradient $\\nabla_{\\mathbf{e}}\\hat{g}(e)$ is analytically available: we frst randomly sample $n$ subsets of $\\varepsilon$ to give $\\mathbf{e}_{1:n}=[\\mathbf{e}_{1},...,\\mathbf{e}_{n}]$ , where each subset of examples is represented as a $m$ -dimensional binary column vector $\\mathbf{e}_{i}\\in\\{0,1\\}^{m}$ with ${\\bf e}_{i}^{(j)}=1$ if the $j.$ -th example is present or 0 otherwise; we then evaluate the performance metric of each $\\mathbf{e}_{i}$ to obtain $\\mathbf{g}_{1:n}=[g(\\mathbf{e}_{1}),...,g(\\mathbf{e}_{n})]$ . We then compute and average the input gradient w.r.t. each possible $\\{\\boldsymbol{e}_{j}\\}_{j=1}^{m}\\in\\mathcal{E}$ to obtain an approximated marginalized importance of each example in $\\delta^{1}$ . Finally, we sort the examples based on $M(e)$ and construct subsets at regular interval from size 1 to $|\\mathcal{E}|$ in both ascending and descending directions. Formally, we order $\\{e_{i}\\}_{i=1}^{n}$ such that $M(e_{1})\\,\\leq\\,M(e_{2})\\,\\leq\\,\\dots\\,\\leq\\,M(e_{n})$ ; the ascending and descending sets of size $t\\in[1,|\\mathcal{E}|]$ are given by $\\mathbf{a}_{t}=\\mathbf{e}_{1:t}$ and $\\mathbf{d}_{t}=\\mathbf{e}_{n-t:n}$ respectively. We then evaluate $g(\\cdot)$ on these sets and show the results in Fig. 1. ",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/9f9ff79ea97a30fd82fc0c089176bfcb558d284339d9033913e7c4eb502869f0.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "As shown, while the gray lines (overall trend lines) often show a   \npositive correlation between performance and an increasing number   \nof examples, we also observe often large gap between the green   \n(top- $^{\\,k}$ examples) and the red (bottom- $^{\\,k}$ examples) lines, suggesting that diferent sampling strategies ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Figure 2 Good demonstrations lead to better re-generated examples: trendlines between accuracy and # examples; note that the re-generated examples by using top-5 examples sets as demonstrations outperform the original examples (gray line) by at all parts of the curve. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "can lead to performance diferences that far outweigh the efect from naïve scaling – e.g., if we establish an “exchange rate” between diferent example sets based on their imputed ordering, we can observe that including around top-10 examples (green lines) examples is as efective as or more efective than the set containing bottom-30 examples in geometric_shapes. More importantly, in both cases we observe that the green lines, which represent an intelligent selection strategy more sophisticated than random sampling, plateau far before the gray line, suggesting that it is possible to achieve comparable performance with a much smaller number of examples: in disambiguation_qa, we fnd that using fewer than 20 top examples is almost already as good as using all 42 examples whereas subsequent additions only led to a few percent of gain, possibly within the margin of error with reshufing (denoted by error bars on the fgure). In the other tasks, we fnd the performance to peak much earlier and adding more examples to the context actually led to performance deterioration. The results suggest 1) the fact that it is possible to match or outperform using all examples with fewer, carefully selected examples means that intelligent example selection is still relevant even with many-shot ICL, echoing fndings from the recent works (Li et al., 2024b) that retrieval remains valuable for long-context models in the RAG setup; and 2) naïvely including as many examples as possible can be suboptimal both in terms of computing cost and performance – while it is trivially true for the tasks whose performance does not improve monotonically with the number of examples, we show that it can even be true when it apparently does: e.g., on geometric_shapes, the near monotonic improvement overall trend (gray line) may lead someone to conclude that it is benefcial to include as many examples as possible, even though the green line representing intelligent selection saturates and starts to decline earlier. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Can we still beneft from scaling examples? Experiments above demonstrated the presence of redundancy in many-shot ICL, revealing that using a smaller subset of examples can often reduce this redundancy without sacrifcing performance. It is, however, a pruning operation that necessarily reduce the input tokens consumed. This leads to a natural question: can we still beneft from scaling through expanding? For this question, it is important to recognize that under the reinforced ICL setup, while the inputs and labels in many-shot setups are fxed, the model-generated intermediate outputs, which represent reasoning paths, are modifable. Given that these intermediate roles are shown to play a critical role in steering model behaviors (Wan et al., 2024), it is possible that examples previously identifed as non-important or non-benefcial may be again benefcial if the model-generated rationales can be improved. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "To achieve so, we reuse the optimized example set from the previous steps as “seed” demonstrations for LLMs to re-generate the examples on the train set, the same set from which the optimized examples are generated. As shown by Fig. 2 where we use an example set of diferent sizes as the seeds, the regeneration step not only increases the number of shots available but also results in better performance across the accuracy versus number-of-demonstrations trade-of. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3. Methodology ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The fndings presented above highlight a signifcant need for improvements that extend beyond simply increasing the number of examples straightforwardly. Instead, identifying the most useful example subset $\\mathbf{e}^{*}$ is crucial both for efective cost-performance trade-ofs and for better reasoning path generation for more efective examples. Based on these insights, we propose Bayesian Refnement and Iterative Demonstration Generation for Examples, or bridge in short (described in Algorithm 1 and depicted in Fig. 3, an optimization algorithm aiming to enhance many-shot ICL with intelligent example selection and iterative example generation. At a high level, the outer loop of bridge is structured in two alternating steps of “optimize” and “generate”. In the “optimize” step, the algorithm focuses on discovering the optimal subset of examples $\\mathbf{e}^{*}$ via a carefully designed (for low complexity, ",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/59e85496f1ecbfc15b55c2ac329e53852a1bf6aabe5d740a57e5067249fe0783.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Figure 3 | Overview of bridge: With a labeled dataset $\\mathcal{D}$ , exemplifed with 6 samples, at the Generation phase (left half), we generate initial examples by performing LLM inference on the inputs of $\\mathcal{D}$ (“Q1- $6^{\\circ}.$ ) with zero-shot prompting to obtain the initial responses $\\mathsf{^{6}A1-6\\,^{\\circ}}$ , which include any intermediate outputs critical for ICL (Step 1). At Step 2, consistent with reinforced ICL in Agarwal et al. (2024), we flter the responses to retain the subset of $\\mathcal{D}$ where the LLM predicted correctly to ensure the examples include correct reasoning steps to build $\\mathcal{E}_{k}$ , the pool of examples at round $k$ which form the search space for the subsequent Optimize step. At the Optimize step (right half), we initialize the proposed Bayesian optimizer by randomly sampling subsets $\\mathbf{e}^{(0)}\\subseteq\\mathcal{E}_{k}$ as demonstrations to be Step 3 evaluated on a held-out validation dataset ( $\\mathcal{D}$ can be reused for this purpose) to obtain a performance metric Step 4. The Bayesian optimizer (BO) is then updated with binary vector representations of e that led to this validation performance as input and the metric itself as output and suggests a new subset of examples to be used as demonstrations for the next step Step 5; Steps 4-5 are repeated (inner loop) until the BO budget is exhausted, after which the best evaluated set $\\mathbf{e}_{k}^{*}$ is returned (Step 6). This set is then used as a demonstration to generate the example pool for the next round $\\delta_{k+1}$ (Step 7). ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "robustness to overftting and budget control) Bayesian optimization algorithm that naturally leverages the GPR surrogate used in Sec. 2; in the “generate” step, bridge utilizes the optimized subset as seed demonstrations to align the model with the best-performing examples seen so far to re-generate new reasoning paths as an integral part of more efective examples back to the many-shot regime to leverage the long context. The two steps are iteratively repeated to progressively refne the examples. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Optimize step. While efective, directly using the importance scoring approach from Sec. 2 to identify the $\\mathbf{e}^{*}$ would require us to set the optimal number of examples to select $||\\mathbf{e}^{*}||$ as a hyperparameter, the optimal value of which is task specifc. Furthermore, a key motivation for the importance-based ranking in Sec. 2 is to attribute performance to individual examples; this is, however, not required if we simply would like to fnd an optimal subset $\\mathbf{e}^{*}$ . To nevertheless use the GPR surrogate in Sec. 2 which has shown an impressive sample-efcient, modeling capability, we propose to use Bayesian optimization (BO) (Frazier, 2018; Garnett, 2023), a sample-efcient black-box optimization algorithm that has recently shown promise in combinatorial problems (Daulton et al., 2022; Wan et al., 2021); it naturally synergizes with the GP surrogate yet automatically strikes a balance between exploration and exploitation to discover $\\mathbf{e}^{*}$ without requiring us to set $||\\mathbf e^{*}||$ beforehand, although bridge is also compatible with alternative methods as a drop-in replacement of the “Optimize” step, which we investigate in detail in App. C.1. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Instead of consuming the entire query budget by sampling randomly, as illustrated by Algorithm 2, ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Algorithm 1 bridge. ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "1: Input: train set ${\\mathcal{D}}_{\\mathrm{t}}$ , validation set ${\\mathcal{D}}_{\\mathrm{v}}$ (can be the same as the train set), number of iteration rounds $K\\in\\mathbb{N}$ (outerloop), evaluation budget for BO per iteration $n_{\\mathrm{eval}}$ (innerloop).   \n2: Output: Optimized set of examples $\\varepsilon^{\\ast}$ .   \n3: [Generate] Generate the pool of initial examples $\\mathcal{E}_{0}$ by predicting the LLM on the train set with zero-shot prompting or few-shot prompting (if handwritten fewshot demonstrations are available). Each instance in $\\mathcal{E}_{0}$ is a concatenation of {input, model-generated reasoning, fnal outputs} for the subset of the train set where the model obtained the correct prediction.   \n4: for $k\\in\\{1,...,K\\}$ (Outer loop) do   \n5: [Optimize] Run Bayesian optimization (calling subroutine Algorithm 2 on the validation set to obtain $\\mathbf{e}_{k}^{*}\\leftarrow\\mathrm{BayesOpt}(n_{\\mathrm{eval}}{=}n_{\\mathrm{eval}},\\mathcal{E}{=}\\mathcal{E}_{k}).$   \n6: [Generate] Re-generate examples $\\mathcal{E}_{k}$ by re-predicting the LLM on the train set, but with the optimized examples $\\mathbf{e}_{k}^{\\ast}$ from the previous step as demonstrations; the {inputs, model-generated reasoning, output}-tuples are concatenated to form the new set of examples $\\mathit{{E}}_{k}$ for the next [Optimize] step.   \n7: end for   \n8: return Optimized example set $\\varepsilon^{\\ast}$ after $K$ rounds. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Algorithm 2 Budget-controlled BO subroutine with random scalarization (BayesOpt). ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "1: Input: Evaluation budget for BO per iteration $n_{\\mathrm{eval}}$ (inner-loop), full set of available samples $\\varepsilon$ , number of random initializations $n_{\\mathrm{init}}=\\operatorname*{min}(16,n_{\\mathrm{eval}}/2)$ .   \n2: Output: Optimized set of examples $\\mathbf{e}^{*}\\subseteq\\mathcal{E}_{t}$ .   \n3: Randomly generate 𝑛init subsets e1:𝑛init $\\begin{array}{r l r}{\\mathbf{e}_{1:n_{\\mathrm{init}}}}&{{}}&{:=}\\end{array}$ $\\{\\mathbf{e}_{1},...,\\mathbf{e}_{n_{\\mathrm{init}}}\\}$ with each e 0, 1 | E𝑡 | s.t. $|\\mathbf{e}|\\sim\\mathrm{Uniform}(1,|\\mathcal{E}_{t}|)$ .   \n4: Evaluate $\\mathbf{g}_{1:n_{\\mathrm{init}}}=\\,[g(\\mathbf{e}_{1},...,\\mathbf{e}_{n_{\\mathrm{init}}}]^{\\intercal}$ and ft a ${\\varrho\\mathcal{P}}$ on $\\mathbf{e}_{1:n_{\\mathrm{init}}}$ as inputs and ${\\bf g}_{1:n_{\\mathrm{init}}}$ as outputs. Set $\\mathcal{D}_{0}\\ \\gets$ $\\{\\mathbf{e}_{1:n_{\\mathrm{init}}},\\mathbf{g}_{1:n_{\\mathrm{init}}}\\}$   \n5: for $t\\in\\{n_{\\mathrm{init}},...,n_{\\mathrm{eval}}\\}$ (Inner loop) do   \n6: Sample a random scalarization value $\\beta_{t}\\quad\\sim$ Uniform $(0,1)$ and compute the scalarized objective of this iteration $h_{t}(\\mathbf{e})=\\mathrm{TCH}(\\beta_{t},[g(\\mathbf{e}),|\\mathbf{e}|])$ .   \n7: Compute $\\mathbf{h}_{1:t}$ for all previously evaluated points $\\mathcal{D}_{t-1}$ , ft a GPR $\\mathcal{G P}_{t}$ on $[\\mathbf{e}_{1:t},\\mathbf{h}_{1:t}]$ and obtain the next confguration to evaluate by maximizing the acquisition function $\\alpha(\\cdot)$ : $\\mathbf{e}_{t}\\ =\\ \\arg\\operatorname*{max}_{\\mathbf{e}\\subseteq\\mathcal{E}}\\alpha(\\mathbf{e}\\ \\mid$ $\\mathcal{G P}_{t})$ .   \n8: Evaluate $g(\\cdot)$ with $\\mathbf{e_{t}}$ and augment $\\mathcal{D}_{t}\\gets\\mathcal{D}_{t-1}\\cup$ $(\\mathbf{e_{t}},g(\\mathbf{e_{t}}))$   \n9: end for   \n10: return $\\mathbf{e}^{*}=\\arg\\operatorname*{max}_{\\mathbf{e}\\in\\mathcal{D}}g(\\mathbf{e})$ . ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "BO only requires some initializing samples to warm-start (Step 3). Afterward, it guides exploration by iteratively (re)ftting a GPR with the previously observed inputs and outputs so far. Formally, at iteration $t\\,\\in\\,[1,T]$ , we have evaluated $g(\\cdot)\\ t$ times at $\\mathbf{e}_{1:t}=[\\mathbf{e}_{1},...,\\mathbf{e}_{t}]^{\\intercal}$ with observed values ${\\bf g}_{1:t}$ Whereas a straightforward application of BO would directly train a GP on $[{\\bf e}_{1:t},{\\bf g}_{1:t}]$ as inputs-outputs and perform BO with $g(\\cdot)$ as the objective function directly, a subtle but important distinction here is that our goal is to identify a subset $\\mathbf{e}^{*}$ that, when used as demonstrations on the train set, generates to the most efective examples on the validation set, rather to simply fnd the highest-performing $\\mathbf{e}^{*}$ on the validation set. While we expect the two objectives to be correlated (i.e., e that led to high validation performance is also likely to generate better samples on the train set), we also empirically fnd it is desirable to encourage $\\mathbf{e}^{*}$ to have a smaller cardinality akin to a $\\ell_{0}$ regularization to reduce overftting on the validation set and to discourage memorization in subsequent generations from the previous example set $\\delta_{t-1}$ of which $\\mathbf{e}^{*}$ is a subset. To achieve so, we augment the performance maximization max $g(\\mathbf{e})$ with a sparsity objective which counts the number of non-zero elements in e: $\\begin{array}{r}{\\operatorname*{min}\\sum_{j}e^{(j)}-}\\end{array}$ this transforms the problem into a bi-objective optimization problem , where instead of maximizing for the validation performance only, we also encourage sparsity as regularization. Practically, we solve the problem with random scalarization (Knowles, 2006; Paria et al., 2020). Specifcally, as hinted in Step 7 of Algorithm 2, at each BO iteration, we frst sample a random scalar $\\beta_{t}\\sim\\operatorname{Unif}(\\beta_{\\mathrm{LB}},\\beta_{\\mathrm{UB}})$ that determines the weight of the performance objective $g(\\cdot)$ of the $t$ -th BO iteration (the weight of the sparsity objective is given by $1-\\beta_{t})$ and $\\{\\beta_{\\mathrm{LB}},\\beta_{\\mathrm{UB}}\\}$ denote the lower and upper bounds of the weight for $g(\\cdot)$ which are set to $\\{0.25,\\,1\\}$ by default. With this $\\beta_{t}$ , we then aggregate the vector objective $[g(\\mathbf{e}),\\sum_{j}e^{(j)}]$ back to a scalar $h_{t}(\\mathbf{e})$ via Tchebyshev scalarization (TCH), a theoretically well-founded scalarization scheme common in multi-objective optimization (Bowman Jr, 1976; Chugh, 2020; Steuer and Choo, 1983) given by: ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\nh_{t}(\\mathbf{e})=\\operatorname*{max}\\Big\\{\\beta_{t}\\big(g(\\mathbf{e})-\\operatorname*{max}\\{g(\\mathbf{e}_{1}),...,g(\\mathbf{e}_{\\mathrm{t}})\\}\\big),-(1-\\beta_{t})\\sum_{j}e^{(j)}\\Big\\},\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "where the minus sign before the last term is to cast the sparsity objective as maximization. We opt for random scalarization that difers from step to step instead of a fxed scalarization weight or any hard constraint on $\\textstyle\\sum_{j}e^{(j)}$ to retain the fexibility of exploring the entire Pareto front since the exact relation between the number of samples and performance can difer across tasks. Since $\\beta_{t}$ is in general diferent for each $t$ , we then compute $\\mathbf{h}_{t}=[h_{t}(\\mathbf{e}_{1}),...,h_{t}(\\mathbf{e}_{t})]$ on previously evaluated outputs and ft a GP on $\\mathcal{H}_{t}:=[\\mathbf{e}_{{1:t}},\\mathbf{h}_{t}]$ , which induces a Gaussian posterior predictive distribution with mean and variance at any $\\mathbf{e}\\subseteq{\\mathcal{E}}$ (we use $\\hat{h}_{t}$ to denote that it is the GP approximation of the actual function $h_{t.}^{\\ }$ ): ",
        "page_idx": 7
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\hat{h}_{t}(\\mathbf{e})\\mid\\mathcal{H}_{t}}[\\hat{h}_{t}(\\mathbf{e})]=\\mathbf{k}_{t}(\\mathbf{K}+\\eta^{2}\\mathbf{I})^{-1}\\mathbf{h}_{t},\\;\\mathbb{V}_{\\hat{h}_{t}(\\mathbf{e})\\mid\\mathcal{H}_{t}}[\\hat{h}_{t}(\\mathbf{e})]=k(\\mathbf{e},\\mathbf{e})-\\mathbf{k}_{t}(\\mathbf{K}+\\eta^{2}\\mathbf{I})^{-1}\\mathbf{k}_{t}^{\\top},}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "where $\\mathbf{k}_{t}=[k(\\mathbf{e},\\mathbf{e}_{1}),...,k(\\mathbf{e},\\mathbf{e}_{t})]$ and $k(\\cdot,\\cdot)$ is the covariance function of the GP (we use Matern 2.5 by default) which measures the similarity between two inputs – in our case, it is a function of the number of overlapping examples between two subsets of examples e, $\\mathbf{e}^{\\prime}\\subseteq\\mathcal{E}$ . To select the next confguration to evaluate ${\\bf e}_{k}$ , the BO optimizes an acquisition function, another key component of BO that automatically trade-of exploration and exploitation. At each inner-loop BO iteration, we choose the maximizer of the expected improvement (EI) (Zhan and Xing, 2020) for the next iteration $\\begin{array}{r}{\\mathbf{e}_{t};\\;\\mathbf{e}_{t}=\\arg\\operatorname*{max}_{\\mathbf{e}\\subset\\mathcal{E}}\\alpha(\\mathbf{e})=\\arg\\operatorname*{in}_{\\mathbf{ax}_{\\mathbf{e}\\subset\\mathcal{E}}}\\bar{\\mathbb{E}_{\\hat{h}_{t}(\\mathbf{e})|\\mathcal{H}_{t}}}\\big[\\operatorname*{max}\\{0,\\hat{h}_{t}(\\mathbf{e})-\\operatorname*{max}_{t^{\\prime}\\in\\{1,t\\}}\\hat{h}_{t}(\\mathbf{e}_{\\mathbf{t^{\\prime}}})\\}\\big].}\\end{array}$ ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Generate step. At each outer-loop round $k\\in\\{1,...,K\\}$ , given the optimized $\\mathbf{e}_{k}^{*}$ as demonstrations, we regenerate and replace the example pool with the correct predictions and their generated rationales $\\mathcal{E}_{k}\\leftarrow f_{\\mathrm{LLM}}(\\mathcal{D}_{t},\\mathbf{e}_{k}^{*}\\subseteq\\mathcal{E}_{k-1})$ for subsequent optimize step. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4. Experiments ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Model and evaluation data. We conduct experiments on an extensive collection of tasks requiring a diferent set of skills task difculty on two Gemini 1.5 models (gemini-1.5-pro-001 and gemini-1.5-flash-001) while also testing key fndings on Mistral family of models: Mistral NeMo (mistral-nemo-12b) and Mistral Large (mistral-large-2407), and Claude 3.5 Sonnet: 1) BIG-Bench Hard (BBH) tasks encompassing a wide range of challenging numerical reasoning, commonsense problem-solving, logical deduction and tabular reasoning tasks – we particularly focus on the subset of 16 BBH tasks where the model performances have not saturated; 2) Hendryck’s MATH (Hendrycks et al., 2021), a challenging numerical reasoning dataset; 3) GSM-Hard (Gao et al., 2022), a more challenging variant of the classical grade-school GSM-8K (Cobbe et al., 2021) with the numbers in the questions replaced with much larger and rarer ones. To further probe the utility of many-shot learning and bridge in coding tasks, we also experiment on 4) BIRD (Li et al., 2024a), a challenging large-scale text-to-SQL generation benchmark where the LLM has to generate SQLite programs from natural language instructions that are executed on real-world databases. For all datasets, when ofcial train-test split is not available, we randomly split the data into train and test splits; unless stated otherwise, a single unifed train split is used both for the generation of demonstrations and is reused for validation (i.e., the objective of the optimize step in Algorithm 1; the test splits are held-out and only used for evaluation of the algorithm. We refer the readers to App. B for detailed descriptions, prompt templates, and evaluation protocols used. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Experimental setup. For all tasks, we run bridge with $K=3$ rounds (i.e., the number of outer-loop iterations in Algorithm 1) and within each round, we allow for $n_{\\mathrm{eval}}=32$ evaluations on the validation set (i.e., the number of inner-loop iterations in Algorithm 2) and we report the results at the end of each “optimize” and “generate” steps to visualize the iteration process. For baselines, we consider 1) using all provided examples and we consider three variants: a) using query-target only without any generated rationales (Direct), b) frst prompt the LLM to generate rationales and answers, and use the concatenation of query-rationale-target as demonstrations, regardless of whether the rationale led to the correct answer (CoT), and c) prompting the LLM with both the query and the fnal, ground-truth ",
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/48f2e2d0a111c4c389c156cf4214792f5bb9a5199dd2860c1a51669ef749c4b0.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td colspan=\"3\">All</td><td>Reinf.</td><td colspan=\"3\">Iterative</td><td colspan=\"5\">BRIDGE</td></tr><tr><td></td><td>Direct</td><td>CoT</td><td>Infill</td><td>ICL</td><td></td><td>Reinf.</td><td>10</td><td>1G</td><td>(Ours) 20</td><td></td><td></td><td>30</td></tr><tr><td>#Iterations</td><td></td><td>0</td><td>0</td><td>0</td><td>1</td><td>2</td><td></td><td></td><td></td><td></td><td>2G</td><td></td></tr><tr><td>causal_judgement</td><td>61.04.7</td><td>62.72.1</td><td>68.02.8</td><td>66.34.8</td><td>68.71.9</td><td>69.32.7</td><td>68.31.5</td><td></td><td>62.71.6</td><td>59.71.5</td><td>72.00.0</td><td>70.02.0</td></tr><tr><td>date_understanding</td><td>87.22.0</td><td>86.02.3</td><td>94.81.8</td><td>88.82.5</td><td>93.01.0</td><td>94.91.3</td><td>92.21.5</td><td></td><td>97.00.7</td><td>94.81.9</td><td>95.01.2</td><td>95.51.8</td></tr><tr><td>disambiguation_qa</td><td>74.22.2</td><td>63.31.1</td><td>72.32.0</td><td>76.82.4</td><td>74.61.4</td><td>75.11.5</td><td>71.82.4</td><td></td><td>77.53.6</td><td>80.51.8</td><td>81.32.9</td><td>78.81.5</td></tr><tr><td>dyck_languages</td><td>16.82.9</td><td>39.03.7</td><td>24.52.9</td><td>55.53.6</td><td>64.45.3</td><td>74.43.6</td><td></td><td>49.22.7</td><td>76.23.8</td><td>80.02.7</td><td>77.51.1</td><td>76.83.8</td></tr><tr><td>formal_fallacies</td><td>82.83.7</td><td>86.81.3</td><td>84.32.8</td><td>86.21.1</td><td>88.10.9</td><td>89.41.4</td><td>86.02.1</td><td></td><td>85.02.5</td><td>90.82.3</td><td>90.82.8</td><td>88.22.3</td></tr><tr><td>geometric_shapes</td><td>69.04.1</td><td>61.84.2</td><td>73.52.3</td><td>80.22.8</td><td>81.02.5</td><td>82.31.7</td><td>78.52.1</td><td></td><td>82.53.6</td><td>89.23.8</td><td>92.31.1</td><td>89.20.8</td></tr><tr><td>hyperbaton</td><td>70.84.1</td><td>93.23.1</td><td>89.52.6</td><td>90.21.1</td><td>91.52.2</td><td>86.22.5</td><td></td><td>96.50.9</td><td>94.21.5</td><td>94.82.8</td><td>96.50.5</td><td>97.20.4</td></tr><tr><td>logical_deduction (7) movie_recommendation</td><td>56.84.4</td><td>63.07.4</td><td>69.85.9</td><td>65.83.5</td><td>68.92.6</td><td>69.52.9</td><td></td><td>70.21.5</td><td>70.84.5</td><td>71.73.7</td><td>71.51.8</td><td>69.22.2</td></tr><tr><td>multistep_arithmetic_two</td><td>75.01.0</td><td>63.72.2</td><td>68.02.8</td><td>65.21.6</td><td>68.82.0</td><td>82.01.9</td><td></td><td>67.01.2</td><td>69.50.5</td><td>69.33.1</td><td>72.81.8 98.00.7</td><td>67.01.2</td></tr><tr><td></td><td>86.52.2</td><td>96.80.8</td><td>88.81.8</td><td>96.50.5</td><td>95.90.8</td><td>94.51.3</td><td>96.20.8</td><td></td><td>94.51.1</td><td>97.00.7</td><td></td><td>96.81.8 95.00.7</td></tr><tr><td>object_counting ruin_names</td><td>92.52.3</td><td>84.84.3</td><td>95.31.3</td><td>95.50.9</td><td>95.82.2</td><td>95.11.6</td><td></td><td>96.20.4</td><td>96.01.9</td><td>94.51.1</td><td>94.20.4</td><td></td></tr><tr><td></td><td>85.23.1</td><td>85.52.1</td><td>89.81.6</td><td>89.81.9</td><td>88.61.5</td><td>90.50.9</td><td></td><td>90.81.1</td><td>88.81.7</td><td>89.21.5</td><td>88.82.4</td><td>90.30.8</td></tr><tr><td>salient_translation_error_detection</td><td>66.02.4</td><td>56.21.5</td><td>72.50.5</td><td>69.01.6</td><td>73.81.1</td><td>73.41.3</td><td></td><td>68.80.8</td><td>71.00.7</td><td>69.52.2</td><td>74.00.7 96.91.5</td><td>74.51.1 97.61.8</td></tr><tr><td>snarks</td><td>94.11.8 93.81.3</td><td>95.52.3 94.21.3</td><td>95.10.6 95.00.7</td><td>92.73.2 93.01.4</td><td>94.31.9 94.10.9</td><td>95.51.5 95.41.2</td><td></td><td>93.43.0 92.81.9</td><td>95.80.0 97.01.2</td><td>95.11.6 96.20.8</td><td>95.80.4</td><td>95.80.8</td></tr><tr><td>sports_understanding tracking_shuffled_objects (7)</td><td>76.07.2</td><td>52.52.1</td><td>64.32.8</td><td>62.34.2</td><td>64.52.2</td><td>65.54.6</td><td></td><td>95.80.4</td><td>95.01.2</td><td>100.00.0</td><td>97.00.7</td><td>99.50.5</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Average</td><td>74.22</td><td>74.06</td><td>78.70</td><td>79.61</td><td>81.61</td><td>82.37</td><td></td><td>82.11</td><td>84.61</td><td>85.77</td><td>87.13</td><td>86.33</td></tr></table></body></html>\n\n",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Table 1 | Test accuracy of gemini-1.5-pro-001 on selected BBH tasks with diferent prompting approaches. “All” refers to using the entire labeled set of 75 examples as demonstrations (“Direct”: using all input-fnal answer pairs without any model-generated content; “CoT”: using all inputrationale-fnal answer triplet, where the rationale is model-generated; “Infll”: using all input-rationalefnal answer triplet, where the rationale is flled in by prompting the model to generate the intermediate steps given the inputs and ground-truth answers); “Reinf. ICL” refers to reinforced many-shot ICL where we include the subset of train set that the LLM answered correctly under zero-shot as demonstrations; “Iterative Reinf.” refers to the iterative variant of reinforced many-shot ICL where we directly use all the generated correct examples from the previous round as demonstrations for the next round without the optimize step, and the diferent columns of bridge show the evolution of test accuracy at diferent milestones: e.g., 1o refers the results with optimized $\\mathbf{e}_{1}^{\\ast}$ from initial examples $\\mathcal{E}_{0}$ as demonstrations (in general, we have $\\mathbf{e}_{k}^{*}\\subseteq\\mathcal{E}_{k-1})$ , and 1g refers to the results using $\\mathcal{E}_{1}$ generated by re-evaluating the train set with $\\mathbf{e}_{1}^{\\ast}$ as demonstrations. All results shown are averaged across 4 random seeds with the standard deviation (stdev) denoted in the subscript. The best and second-best results along each row are bolded and underlined, respectively (ties are broken by favoring the result with lower stdev). ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "answer to fll in the rationale – this technique has been variously referred to as, e.g., inflling (Hu et al., 2023), rationalization (Zelikman et al., 2022), or more generally, teacher forcing (Chen et al., 2025) due to its conceptual similarity to teacher forcing in recurrent neural network (RNN) training (Lamb et al., 2016) (Infll); 2) reinforced ICL (Agarwal et al., 2024), where all available input-output pairs from the correct predictions on the train set with zero-shot prompting are used; and 3) an iterative variant of reinforced ICL which can also be seen as bridge without the optimize step: while we repeat the generation process on the train set $K=3$ times, we do not frst aim to select the optimized subset but instead use the entire generated examples from the previous step as demonstrations $\\mathcal{E}_{k}\\leftarrow f_{\\mathrm{LLM}}(\\mathcal{D}_{t},\\mathcal{E}_{k-1})$ . ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Results and discussions. We show the test accuracy on the BBH tasks in Table 1 (Gemini $1.5\\;\\mathrm{Pro}$ ; the number of examples for each entry in Table 1 are shown in Table 14 in App. C.2), Table 3 (Gemini 1.5 Flash), Tables 4 and 5 (Mistral Large and Mistral NeMo) and Table 6 (Claude 3.5 Sonnet). On MATH and GSM-Hard datasets, we show the Gemini 1.5 Pro results in Table 2. We observe that naïve many-shot scaling is in general inefective and is outperformed by reinforced ICL; bridge, however, outperforms the base reinforced many-shot ICL by more than $7\\%$ and $3\\%$ on Tables 1 and 3, respectively, and the extent of outperformance over the “Iterative reinforced $\\mathrm{ICL}^{\\gamma}$ , which leads to ",
        "page_idx": 8
    },
    {
        "type": "table",
        "img_path": "images/7fe14fef3ffa66426f2788842fcf0059590aeb38755f750af255c3ae91617466.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td>Reinf. ICL</td><td colspan=\"2\">Iterative Reinf.</td><td colspan=\"5\">BRIDGE</td></tr><tr><td>#Iterations</td><td>0</td><td>1</td><td>2</td><td>10</td><td>1G</td><td>(Ours) 20</td><td>2G</td><td>30</td></tr><tr><td>Hendryck's MATH GSM-Hard</td><td>63.750.5 69.880.8</td><td>63.600.9 69.840.4</td><td>63.601.1 69.330.3</td><td>62.601.3 71.890.4</td><td>63.001.2 71.310.4</td><td>63.851.1 71.810.4</td><td>64.650.3 73.320.4</td><td>64.400.9 72.500.6</td></tr></table></body></html>\n\n",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Table 2 Test accuracy of gemini-1.5-pro-001 on MATH and GSM-Hard datasets. Refer to the captions of Table 1 for detailed explanations. ",
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/7d18a1b0ddb96510da1400401115039203b4da49e853eef7b59200944269bef4.jpg",
        "img_caption": [
            "Figure 4 Benefts from scaling examples naïvely (red lines) is very task-specifc, but each iteration of bridge addresses it to a considerable degree by continually improving upon the previous round: We randomly sample subsets of example pool $\\mathcal{E}_{k}\\forall k$ ∈ {0 (i.e., original examples generated with handcraft few-shot or zero-shot), 1, 2} and evaluate them on a held-out set in four representative tasks exhibiting diferent model behavior to example scaling. The trendlines are moving regressions ftted with lowess. Refers to additional fgures in App. C.3. "
        ],
        "img_footnote": [],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "moderate improvements on BBH with Gemini Pro but no signifcant performance gains on MATH, GSM-Hard and BBH with Gemini Flash. Both demonstrate that optimize is an integral component of bridge and implicitly validates the fndings in Sec. 3 that many-shot performance can be driven by few disproportionately infuential examples, which constitutes a core motivation for our method. Barring some expected task-specifc fuctuations, in both Tables 1 and 3, we also observe consistent and monotonic performance improvement as bridge progresses over the successive optimize and generate steps, eventually peaking at 2g on Gemini Pro and 2o on Gemini Flash (although the performance diference between 2g and $20$ on Gemini Flash is negligible and likely within margin of error) – based on the overall results, we recommend stopping bridge at $2\\upalpha$ or 2o. Interestingly, we observe that in both cases, an additional optimize step (i.e., the 3o column) somewhat degrades performance – our hypothesis is that as bridge progresses, the generated examples become more aligned with the optimal behavior and the degree of redundancy as we observed in Sec. 2 reduces, and it becomes more difcult to squeeze the number of examples without harming task performance – indeed, from Fig.4 where we concretely analyze the behavior of the LLM in diferent tasks by evaluating the LLM under random subsets of $\\mathcal{E}_{0},...,\\mathcal{E}_{2}$ as demonstrations in held-out splits, we observe that the beneft from naïvely scaling examples under the base reinforced many-shot ICL (denoted by red lines) can be highly unstable across tasks: from the diferent subfgures of Fig. 4, we fnd the performance to consistently improve with more examples (leftmost), improve then plateau (middle two fgures) and even simply deteriorate with more examples (rightmost) – whereas the latter two cases are direct manifestations that not all examples contribute positively to many-shot ICL and naïvely scaling examples is suboptimal, we note that it remains true even in the former case where there is an apparent strong, positive correlation between number of demos and performance, as we demonstrated in Sec. 2. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Remarkably, bridge alleviates the instability with each round of bridge continually improving upon the previous round – in cases where scaling examples is already benefcial (geometric_shapes, leftmost fgure), subsequent rounds of bridge led to much better performance-cost trade-ofs with the blue and green lines dominating over the red, whereas in other cases, bridge often “delays” the saturation point (e.g., salient_translation) or at least ensure more examples does not lead to deterioration (e.g., tracking_shuffled_objects). ",
        "page_idx": 9
    },
    {
        "type": "table",
        "img_path": "images/4cd15420dde85eb7764d10912126abef79a493bcd92496d82b553a848113c8fd.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td colspan=\"2\">All</td><td>Reinf.</td><td colspan=\"2\">Iterative</td><td colspan=\"5\">BRIDGE</td></tr><tr><td></td><td colspan=\"2\">Direct</td><td>ICL</td><td colspan=\"2\">Reinf.</td><td colspan=\"5\">(Ours)</td></tr><tr><td>#Iterations</td><td></td><td>CoT 0</td><td>0</td><td>1</td><td>2</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>55.05.0</td><td>57.71.1</td><td>66.03.6</td><td>67.72.0</td><td>66.71.6</td><td>69.32.7</td><td>66.02.0</td><td>63.31.5</td><td>65.01.6</td><td>65.31.5</td></tr><tr><td>date_understanding</td><td>84.84.2</td><td>83.31.3</td><td>84.52.3</td><td>86.80.8</td><td>87.30.8</td><td>85.01.3</td><td>90.50.5</td><td>91.50.4</td><td>90.80.7</td><td>92.50.8</td></tr><tr><td>disambiguation_qa</td><td>68.87.2</td><td>54.21.5</td><td>75.50.5</td><td>77.81.6</td><td>78.53.5</td><td>77.51.3</td><td>79.01.1</td><td>77.51.2</td><td>76.30.8</td><td>74.31.1</td></tr><tr><td>dyck_languages</td><td>46.09.5</td><td>19.57.0</td><td>66.81.9</td><td>61.32.6</td><td>60.01.9</td><td>63.32.0</td><td>62.01.7</td><td>64.51.8</td><td>62.82.4</td><td>61.83.8</td></tr><tr><td>formalfallacies</td><td>75.81.9</td><td>74.01.2</td><td>77.30.4</td><td>74.81.9</td><td>72.51.7</td><td>78.31.3</td><td>77.31.5</td><td>75.51.7</td><td>78.31.8</td><td>76.30.8</td></tr><tr><td>geometric_shapes</td><td>45.81.5</td><td>74.24.1</td><td>86.01.9</td><td>93.80.8</td><td>93.31.5</td><td>93.82.5</td><td>94.04.2</td><td>95.51.1</td><td>97.00.0</td><td>98.00.0</td></tr><tr><td>hyperbaton</td><td>87.03.1</td><td>88.51.5</td><td>88.51.5</td><td>95.51.1</td><td>93.31.5</td><td>86.57.6</td><td>95.51.1</td><td>95.80.8</td><td>94.80.4</td><td>93.31.5</td></tr><tr><td>logical_deduction (7)</td><td>37.53.3</td><td>41.01.9</td><td>59.53.4</td><td>61.91.9</td><td>57.54.7</td><td>61.85.1</td><td>57.51.1</td><td>70.50.9</td><td>66.51.1</td><td>75.00.7</td></tr><tr><td>movie_recommendation</td><td>80.53.3</td><td>56.20.8</td><td>67.01.2</td><td>75.81.3</td><td>75.82.9</td><td>70.32.3</td><td>73.32.3</td><td>77.31.5</td><td>78.82.0</td><td>72.83.2</td></tr><tr><td>multistep_arithmetic_two</td><td>55.021.3</td><td>84.02.9</td><td>91.30.8</td><td>94.01.4</td><td>92.51.8</td><td>96.32.3</td><td>96.80.4</td><td>97.80.4</td><td>94.80.8</td><td>95.80.4</td></tr><tr><td>object_counting</td><td>66.02.7</td><td>91.32.0</td><td>93.30.4</td><td>93.51.5</td><td>92.51.1</td><td>92.81.9</td><td>93.82.3</td><td>95.50.5</td><td>93.01.2</td><td>93.80.4</td></tr><tr><td>ruin_names</td><td>83.21.3</td><td>86.21.3</td><td>86.51.8</td><td>89.50.9</td><td>86.80.8</td><td>89.30.4</td><td>89.30.8</td><td>87.01.2</td><td>90.30.8</td><td>90.01.2</td></tr><tr><td>salient_translation_error_detection</td><td>62.03.7</td><td>58.82.0</td><td>64.81.5</td><td>71.52.2</td><td>64.02.9</td><td>62.80.8</td><td>71.00.7</td><td>69.82.0</td><td>69.00.7</td><td>67.30.4</td></tr><tr><td>snarks</td><td>81.20.7</td><td>92.01.2</td><td>89.21.8</td><td>88.92.2</td><td>86.51.5</td><td>88.92.0</td><td>89.91.8</td><td>89.60.7</td><td>90.60.6</td><td>83.73.5</td></tr><tr><td>sports_understanding</td><td>92.51.5</td><td>91.50.5</td><td>95.80.8</td><td>95.50.5</td><td>96.31.1</td><td>93.31.1</td><td>95.30.4</td><td>91.80.4</td><td>95.01.2</td><td>95.00.0</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>63.35.4</td><td>72.36.0</td><td>92.23.1</td><td>83.51.1</td><td>80.01.6</td><td>98.00.7</td><td>93.82.2</td><td>98.00.0</td><td>97.80.4</td><td>97.50.5</td></tr><tr><td>Average</td><td>67.77</td><td>70.29</td><td>80.25</td><td>81.91</td><td>80.72</td><td>81.61</td><td>82.79</td><td>83.79</td><td>83.77</td><td>83.25</td></tr></table></body></html>\n\n",
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/c7a3a58ac7d37ff3af75ea76e66a6a4b9554a2d6096cb7e2d1be1c6882e0092a.jpg",
        "table_caption": [
            "Table 3 Test accuracy of gemini-1.5-flash-001 on BBH tasks. Refers to captions of Table 1 for detailed explanations. ",
            "Table 4 | Test accuracy of Mistral Large (mistral-large-2407) on BBH tasks. Refer to captions of Table 1 for detailed explanations. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td>Reinf. ICL</td><td colspan=\"2\">Iterative Reinf.</td><td colspan=\"5\">BRIDGE</td></tr><tr><td>#Iterations</td><td>0</td><td colspan=\"2\">1</td><td>10</td><td>1G</td><td>(Ours) 20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>69.3</td><td>66.7</td><td>2 72.0</td><td>68.0</td><td>65.3</td><td>69.3</td><td>64.0</td><td>73.3</td></tr><tr><td>date_understanding</td><td>92.0</td><td>92.0</td><td>96.0</td><td>93.0</td><td>94.0</td><td>95.0</td><td>92.0</td><td>96.0</td></tr><tr><td>disambiguation_qa</td><td>82.0</td><td>82.0</td><td>79.0</td><td>81.0</td><td>87.0</td><td>87.0</td><td>84.0</td><td>86.0</td></tr><tr><td>dyck_language</td><td>56.0</td><td>62.0</td><td>56.0</td><td>70.0</td><td>59.0</td><td>70.0</td><td>63.0</td><td>71.0</td></tr><tr><td>formal fallacies</td><td>90.0</td><td>82.0</td><td>86.0</td><td>89.0</td><td>89.0</td><td>90.0</td><td>83.0</td><td>85.0</td></tr><tr><td>geometric_shapes</td><td>87.0</td><td>80.0</td><td>93.0</td><td>88.0</td><td>85.0</td><td>95.0</td><td>71.0</td><td>94.0</td></tr><tr><td>hyperbaton</td><td>99.0</td><td>96.0</td><td>100.0</td><td>100.0</td><td>98.0</td><td>100.0</td><td>100.0</td><td>99.0</td></tr><tr><td>logical_deduction (7)</td><td>81.0</td><td>85.0</td><td>76.0</td><td>82.0</td><td>88.0</td><td>90.0</td><td>86.0</td><td>92.0</td></tr><tr><td>movie_recommendation</td><td>74.0</td><td>71.0</td><td>74.0</td><td>77.0</td><td>66.0</td><td>78.0</td><td>80.0</td><td>79.0</td></tr><tr><td>multistep_arithmetic_two</td><td>88.0</td><td>92.0</td><td>93.0</td><td>91.0</td><td>89.0</td><td>88.0</td><td>86.0</td><td>93.0</td></tr><tr><td>object_counting</td><td>99.0</td><td>99.0</td><td>99.0</td><td>98.0</td><td>98.0</td><td>98.0</td><td>100.0</td><td>98.0</td></tr><tr><td>ruin_names</td><td>88.0</td><td>90.0</td><td>92.0</td><td>86.0</td><td>89.0</td><td>87.0</td><td>89.0</td><td>89.0</td></tr><tr><td>salient_translation_error_detection</td><td>66.0</td><td>68.0</td><td>70.0</td><td>78.0</td><td>69.0</td><td>75.0</td><td>72.0</td><td>73.0</td></tr><tr><td>snarks</td><td>95.8</td><td>95.8</td><td>97.2</td><td>94.4</td><td>95.8</td><td>95.8</td><td>95.8</td><td>93.1</td></tr><tr><td>sports_understanding</td><td>94.0</td><td>97.0</td><td>98.0</td><td>93.0</td><td>95.0</td><td>96.0</td><td>97.0</td><td>96.0</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>96.0</td><td>68.0</td><td>100.0</td><td>100.0</td><td>73.0</td><td>100.0</td><td>57.0</td><td>100.0</td></tr><tr><td>Average</td><td>84.82</td><td>83.22</td><td>87.08</td><td>86.65</td><td>83.70</td><td>88.07</td><td>82.80</td><td>88.52</td></tr></table></body></html>\n\n",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "On the BIRD dataset, we show the results in Table 7. Given the presence of a large training set (more than 9000 samples), we also compare against parameter-efcient supervised fne-tuning (PEFT) (Han et al., 2024), where we fne-tune the same target LLM with LoRA (Hu et al., 2021) on either the entire training set or using a number of train samples sub-sampled from the full training set. We observe that whereas the few-shot chase prompt efectively improves upon the baseline zero-shot direct prompting, additional rounds of bridge led to further gains. The comparison against LoRA also demonstrates the potential of bridge as an alternative to PEFT at least in certain scenarios. When provided with a similar number of labeled samples (i.e., $n_{\\mathrm{train}}=256)$ , we observe that LoRA performs much worse, and it only outperforms bridge when using up the entire train set for training. ",
        "page_idx": 10
    },
    {
        "type": "table",
        "img_path": "images/1d946c7fd69f7e926d06fa89b5ff1e8762dd14c80878792496cf245a47fd00e1.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Tasks #Iterations</td><td rowspan=\"2\">Reinf. ICL 0</td><td colspan=\"2\">Iterative Reinf.</td><td colspan=\"5\">BRIDGE</td></tr><tr><td>1</td><td>2</td><td>10</td><td>1G</td><td>(Ours) 20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>53.3</td><td>65.3</td><td>62.7</td><td>60.0</td><td>58.7</td><td>62.7</td><td>64.0</td><td>64.0</td></tr><tr><td>date_understanding</td><td>66.0</td><td>71.0</td><td>68.0</td><td>69.0</td><td>69.0</td><td>78.0</td><td>70.0</td><td>75.0</td></tr><tr><td>disambiguation_qa</td><td>58.0</td><td>60.0</td><td>64.0</td><td>63.0</td><td>60.0</td><td>61.0</td><td>66.0</td><td>72.0</td></tr><tr><td>dyck_languages</td><td>17.0</td><td>21.0</td><td>22.0</td><td>18.0</td><td>27.0</td><td>26.0</td><td>22.0</td><td>30.0</td></tr><tr><td>formal_fallacies</td><td>64.0</td><td>55.0</td><td>53.0</td><td>63.0</td><td>59.0</td><td>52.0</td><td>51.0</td><td>59.0</td></tr><tr><td>geometric_shapes</td><td>65.0</td><td>65.0</td><td>69.0</td><td>72.0</td><td>72.0</td><td>60.0</td><td>69.0</td><td>68.0</td></tr><tr><td>hyperbaton</td><td>77.0</td><td>72.0</td><td>65.0</td><td>80.0</td><td>81.0</td><td>83.0</td><td>75.0</td><td>86.0</td></tr><tr><td>logical_deduction (7)</td><td>47.0</td><td>54.0</td><td>53.0</td><td>45.0</td><td>49.0</td><td>62.0</td><td>44.0</td><td>51.0</td></tr><tr><td>movie_recommendation</td><td>59.0</td><td>45.0</td><td>54.0</td><td>68.0</td><td>61.0</td><td>63.0</td><td>64.0</td><td>70.0</td></tr><tr><td>multistep_arithmetic_two</td><td>36.0</td><td>50.0</td><td>20.0</td><td>47.0</td><td>20.0</td><td>66.0</td><td>12.0</td><td>77.0</td></tr><tr><td>object_counting</td><td>81.0</td><td>81.0</td><td>82.0</td><td>83.0</td><td>79.0</td><td>85.0</td><td>75.0</td><td>87.0</td></tr><tr><td>ruin_names</td><td>69.0</td><td>60.0</td><td>57.0</td><td>76.0</td><td>57.0</td><td>72.0</td><td>57.0</td><td>70.0</td></tr><tr><td>salient_translation_error_detection</td><td>47.0</td><td>47.0</td><td>45.0</td><td>59.0</td><td>49.0</td><td>53.0</td><td>49.0</td><td>48.0</td></tr><tr><td>snarks</td><td>69.4</td><td>76.4</td><td>79.2</td><td>72.2</td><td>75.0</td><td>72.2</td><td>73.6</td><td>77.8</td></tr><tr><td>sports_understanding</td><td>86.0</td><td>75.0</td><td>69.0</td><td>91.0</td><td>72.0</td><td>91.0</td><td>74.0</td><td>93.0</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>70.0</td><td>69.0</td><td>70.0</td><td>91.0</td><td>88.0</td><td>94.0</td><td>81.0</td><td>93.0</td></tr><tr><td>Average</td><td>60.30</td><td>60.42</td><td>58.30</td><td>66.08</td><td>61.04</td><td>67.56</td><td>59.16</td><td>70.05</td></tr></table></body></html>\n\n",
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/549ba769bfafe9d21c5a5d84912ea6cf5bfaa33e432ec4d353cc0714df14cdec.jpg",
        "table_caption": [
            "Table 5 Test accuracy of Mistral NeMo (mistral-nemo-12b) on BBH tasks. Refer to captions of Table 1 for detailed explanations. ",
            "Table 6 Test accuracy of Claude 3.5 Sonnet (claude-3-5-sonnet@20240620) on BBH tasks. Refer to captions of Table 1 for detailed explanations. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td>Reinf. ICL</td><td colspan=\"2\">Iterative Reinf.</td><td colspan=\"5\">BRIDGE</td></tr><tr><td>#Iterations</td><td>0</td><td>1</td><td>2</td><td>10</td><td>1G</td><td>(Ours) 20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>64.0</td><td>68.0</td><td>65.3</td><td>62.7</td><td>69.3</td><td>73.3</td><td>70.7</td><td>65.3</td></tr><tr><td>date_understanding</td><td>94.0</td><td>95.0</td><td>96.0</td><td>97.0</td><td>94.0</td><td>95.0</td><td>96.0</td><td>95.0</td></tr><tr><td>disambiguation_qa</td><td>73.0</td><td>82.0</td><td>79.0</td><td>81.0</td><td>87.0</td><td>87.0</td><td>84.0</td><td>86.0</td></tr><tr><td>dyck_language</td><td>68.0</td><td>68.0</td><td>65.0</td><td>74.0</td><td>85.0</td><td>90.0</td><td>92.0</td><td>87.0</td></tr><tr><td>formal fallacies</td><td>93.0</td><td>94.0</td><td>97.0</td><td>96.0</td><td>95.0</td><td>98.0</td><td>96.0</td><td>95.0</td></tr><tr><td>geometric_shapes</td><td>92.0</td><td>94.0</td><td>98.0</td><td>88.0</td><td>90.0</td><td>85.0</td><td>96.0</td><td>89.0</td></tr><tr><td>hyperbaton</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td></tr><tr><td>logical_deduction (7)</td><td>92.0</td><td>96.0</td><td>96.0</td><td>89.0</td><td>95.0</td><td>97.0</td><td>91.0</td><td>93.0</td></tr><tr><td>movie_recommendation</td><td>87.0</td><td>90.0</td><td>92.0</td><td>89.0</td><td>90.0</td><td>88.0</td><td>93.0</td><td>90.0</td></tr><tr><td>multistep_arithmetic_two</td><td>99.0</td><td>99.0</td><td>99.0</td><td>99.0</td><td>99.0</td><td>99.0</td><td>100.0</td><td>100.0</td></tr><tr><td>object_counting</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td></tr><tr><td>ruin_names</td><td>93.0</td><td>93.0</td><td>94.0</td><td>91.0</td><td>94.0</td><td>94.0</td><td>92.0</td><td>94.0</td></tr><tr><td>salient_translation_error_detection</td><td>71.0</td><td>71.0</td><td>73.0</td><td>71.0</td><td>72.0</td><td>73.0</td><td>73.0</td><td>73.0</td></tr><tr><td>snarks</td><td>97.2</td><td>97.2</td><td>97.2</td><td>95.8</td><td>95.8</td><td>98.6</td><td>98.6</td><td>97.2</td></tr><tr><td>sports_understanding</td><td>92.0</td><td>91.0</td><td>94.0</td><td>93.0</td><td>94.0</td><td>94.0</td><td>93.0</td><td>91.0</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td><td>100.0</td></tr><tr><td>Average</td><td>88.45</td><td>89.89</td><td>90.35</td><td>89.16</td><td>91.26</td><td>92.00</td><td>92.20</td><td>90.97</td></tr></table></body></html>\n\n",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Claude and Mistral results. From Tables4, 5, and 6, we fnd that while the base capabilities of the tested models difer signifcantly (e.g., Claude 3.5 Sonnet has a higher accuracy across the board), the high-level fndings primarily derived from Gemini results largely hold. On Claude 3.5 Sonnet, we observe an almost identical high-level trend to Gemini, where each round of bridge incrementally improves performance up to 2g. On the other hand, while Mistral models seemingly beneft less from scaling demonstrations especially in the smaller Mistral NeMo (e.g., sometimes the generate step leads to drops in performance) directly, the improved quality of the generated demonstrations still enables successive optimize step to improve on the preceding round, demonstrating the efectiveness of bridge even when the model does not beneft from scaling examples directly. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Additional experiments. We performed additional experiments to further validate the design of bridge and tested the applicability of bridge beyond reasoning-heavy tasks. We: ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "1. Perform extensive ablation studies in App. C.1, where we studied the importance of Bayesian optimization (Algorithm 2) and compared and combined bridge with heuristic learning-free and learning-based demonstration selection such as retrieval and diversity-based learning-free criteria; we found that in all cases bridge outperformed the alternative approaches, but bridge may also be complementary to various approaches proposed in previous works. We also conduct further experiments confrming the importance of the optimize step by restricting bridge to perform refnement on the subset of the train set where the model predicted correctly initially only in Table 10, and confrmed that the optimize step meaningfully improves upon the variant without it; ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "2. Perform transfer learning analysis in App. C.5 where we used the many-shot examples generated on GSM-Hard on GSM-8K, and we found that the generated examples are generalizable to some extent to distributional shifts; ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "3. Perform cost analysis in App. D where we provide a detailed cost breakdown of bridge in each step. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "5. Related work ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "table",
        "img_path": "images/5d3fd87acb123c86b66cd24a3d802e43bc5c9f4efa61e6bb6c0d88a8c24f1a56.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Exec. Acc.</td><td colspan=\"3\">Breakdown</td></tr><tr><td>S</td><td>M</td><td>C</td></tr><tr><td>Direct CHAsE prompt</td><td>57.7 60.1</td><td>64.0 67.2</td><td>49.4 51.9</td><td>44.1 40.7</td></tr><tr><td>CHASE + BRIDGE Round o</td><td>59.1</td><td>65.7</td><td>51.3</td><td>42.1</td></tr><tr><td>Round 1 Round 2</td><td>61.2 62.0</td><td>68.6 68.5</td><td>50.6 53.0</td><td>48.3 49.0</td></tr><tr><td>PEFT (LoRA)</td><td></td><td></td><td></td><td></td></tr><tr><td>ntrain =256</td><td>58.2</td><td>64.0</td><td>52.2</td><td>40.7</td></tr><tr><td>ntrain = 1024</td><td>60.2</td><td>66.6</td><td>53.0</td><td>42.1</td></tr><tr><td>ntrain = 4096</td><td>61.3</td><td>67.5</td><td>53.9</td><td>46.2</td></tr><tr><td>ntrain =9428 (Al1)</td><td>63.8</td><td>68.6</td><td>58.8</td><td>48.9</td></tr></table></body></html>\n\n",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Table 7 Execution accuracy on the BIRD dev set with gemini-1.5-pro-001. {S, M, C} refer to the accuracy aggregated across {Simple, Moderate, Challenging}- level problems based on assigned difculty. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Scaling ICL. Before the advent of the long-context LLMs, early eforts in scaling ICL often studied LLMs customized for long context (Li et al., 2023) or required architectural changes assuming white-box model access (Hao et al., 2022). However, the tasks considered are often limited, e.g., to conventional, discriminative tasks like sentiment classifcation rather than generative tasks as considered in this work. Furthermore, these often study LLMs that are merely capable of handling many examples, but their behavior may difer signifcantly to modern, natively longcontext LLMs that may actively take advantage of the context – indeed, both these works show mixed results, even signifcant performance deterioration when scaling up the number of examples, a phenomenon not seen in modern long-context LLMs like Gemini and Claude. Recent works like Agarwal et al. (2024) and Bertsch et al. (2024), on the other hand, reported signifcant gains in scaling ICL to hundreds or more examples and provided important motivation for our work. However, as mentioned in Sec. 2, these works primarily demonstrate the existence of the beneft from scaling but do not focus on investigate the sources of the gain or improving the costefectiveness of many-shot ICL. Additionally, there have also been works focusing on applications of many-shot ICL to multi-modalities (Jiang et al., 2024), LLM jail-breaking (Anil et al., 2024), detecting the risk of capturing incorrect skills (Lin and Lee, 2024), and analyzing memorization (Golchin et al., 2024). ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Example selection and generation. bridge combines the “optimize” and “generate” steps, and there have been existing works sharing similar high-level ideas to each of the components. First, the “optimize” step can be seen as a method to improve the data quality with pruning and selection; in this regard, given that data quality is known to be one of the most infuential factors for training LLMs (Xia et al., 2024), many previous works have utilized some favor of pruning to remove redundant or harmful data samples at diferent stages of training, including pre-training (Marion et al., 2023) and instruction tuning (Xia et al., 2024). In ICL, as mentioned in Sec. 2, given the sensitivity of LLMs to examples, there have been numerous works analyzing prompt sensitivity and proposing example selection techniques (Lu et al., 2022; Wan et al., 2024; Zhao et al., 2021; Zhou et al., 2024b). Recent work also explored heuristic-based prompt optimization based on similarity (Liu et al., 2022; Rubin et al., 2022), diversity (Levy et al., 2023; Xu et al., 2024), uncertainty (Wan et al., 2023a,b), fairness (Zhou et al., 2024a) etc. Our “generate” step, on the other hand, aims to acquire high-quality examples with the LLM itself. In this area, STaR (Zelikman et al., 2022) frst proposes to bootstrap rationales from LLM with a small number of seed examples, followed by fne-tuning on the rationales that lead to correct predictions; Self-Instruct (Wang et al., 2023) bootstraps LLMs to instruction data. The “Reinforced ICL” technique introduced in Agarwal et al. (2024), upon which this work improves, and several recent works (Chen et al., 2023; Khattab et al., 2023; Opsahl-Ong et al., 2024) use a similar technique to acquire and refne model-generated examples for ICL. Notwithstanding the similarities described, there are a few crucial diferences with respect to these prior works: Almost all ICL works mentioned consider the few-shot setup, where selection is made necessary due to the constraint on the number of examples allowed in the context. However, we show that even in the many-shot setup where that constraint is relaxed and example selection is no longer a necessity, it can still be highly benefcial for performance and efciency. Unlike the few-shot setup, bridge is tailored for the many-shot setup with design decisions inspired by fndings in Sec. 2, such as the implementation of sparsity regularization in the optimization objective to enable scaling. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "6. Conclusion ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "This paper focuses on understanding and enhancing the core factors underlying scaling ICL. We frst provide an analysis of the nascent paradigm of many-shot ICL in LLMs and show that notwithstanding the long-context abilities of LLMs, the common practice of naïvely dumping as many examples as practically possible into the context can be both inefcient in cost and suboptimal in performance. Instead, the beneft from scaling examples can often be realized by identifying a subset of infuential examples, and that subset can be used as demonstrations themselves to re-generate even more examples. Inspired by the fndings, we propose bridge by automatically executing the “optimize” and “generate” steps iteratively. We demonstrate that bridge perform competitively on a wide range of tasks, signifcantly outperforming alternatives. We believe that this work builds the foundation for future research in many-shot ICL. First, we mainly focused on the restrictive black-box LLM setup, which is the most general and model-agnostic. However, for a more relaxed, white-box setup with access to LLM weights, it may be possible to perform optimization more efciently – for example, it may be possible to take advantage of the internal representations of the model in reducing the cost of iterative optimization. Second, we currently focus on the “reinforced ICL” setup typical for reasoningheavy tasks – while we have conducted experiments (e.g., low resource translation tasks) beyond this setup, further validations on other types of tasks would be valuable. Lastly, after optimization, the examples generated by bridge are currently static at test time, and it would also be interesting to combine with a mechanism for sample-dependent ICL optimization to further enhance performance and reduce cost – we defer these important directions to future work. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Acknowledgment ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "We thank all colleagues from Google Cloud AI Research for their valuable feedback. We also thank the anonymous ICLR reviewers and area chairs whose feedback has been instrumental in improving this work. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes, E. Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024.   \nC. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford, et al. Many-shot jailbreaking. Anthropic, April, 2024.   \nAnthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.   \nM. Balandat, B. Karrer, D. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. Botorch: A framework for efcient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020.   \nA. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024.   \nV. J. Bowman Jr. On the relationship of the tchebychef norm and the efcient frontier of multiplecriteria objectives. In Multiple Criteria Decision Making: Proceedings of a Conference Jouy-en-Josas, France May 21–23, 1975, pages 76–86. Springer, 1976.   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are fewshot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.   \nK. Chen, Y. Chen, X. Yu, and N. Koudas. Reliable text-to-sql with adaptive abstention. arXiv preprint arXiv:2501.10858, 2025.   \nW.-L. Chen, C.-K. Wu, Y.-N. Chen, and H.-H. Chen. Self-ICL: Zero-shot in-context learning with self-generated demonstrations. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15651–15662, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.968. URL https://aclanthology.org/2023.emnlp-main.968.   \nT. Chugh. Scalarizing functions in bayesian multiobjective optimization. In 2020 IEEE Congress on Evolutionary Computation (CEC), pages 1–8. IEEE, 2020.   \nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \nR. Das, M. Zaheer, D. Thai, A. Godbole, E. Perez, J.-Y. Lee, L. Tan, L. Polymenakos, and A. McCallum. Case-based reasoning for natural language queries over knowledge bases. arXiv preprint arXiv:2104.08762, 2021.   \nS. Daulton, X. Wan, D. Eriksson, M. Balandat, M. A. Osborne, and E. Bakshy. Bayesian optimization over discrete and mixed spaces via probabilistic reparameterization. Advances in Neural Information Processing Systems, 35:12760–12774, 2022.   \nP. I. Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.   \nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.   \nJ. Gardner, G. Pleiss, K. Q. Weinberger, D. Bindel, and A. G. Wilson. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. Advances in neural information processing systems, 31, 2018.   \nR. Garnett. Bayesian optimization. Cambridge University Press, 2023.   \nS. Golchin, M. Surdeanu, S. Bethard, E. Blanco, and E. Rilof. Memorization in in-context learning. arXiv preprint arXiv:2408.11546, 2024.   \nF. Guzmán, P.-J. Chen, M. Ott, J. Pino, G. Lample, P. Koehn, V. Chaudhary, and M. Ranzato. The FLORES evaluation datasets for low-resource machine translation: Nepali–English and Sinhala– English. In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6098–6111, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1632. URL https: //aclanthology.org/D19-1632.   \nJ. M. Han, I. Babuschkin, H. Edwards, A. Neelakantan, T. Xu, S. Polu, A. Ray, P. Shyam, A. Ramesh, A. Radford, et al. Unsupervised neural machine translation with generative language models only. arXiv preprint arXiv:2110.05448, 2021.   \nZ. Han, C. Gao, J. Liu, S. Q. Zhang, et al. Parameter-efcient fne-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.   \nY. Hao, Y. Sun, L. Dong, Z. Han, Y. Gu, and F. Wei. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713, 2022.   \nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   \nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nE. J. Hu, M. Jain, E. Elmoznino, Y. Kaddar, G. Lajoie, Y. Bengio, and N. Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363, 2023.   \nY. Jiang, J. Irvin, J. H. Wang, M. A. Chaudhry, J. H. Chen, and A. Y. Ng. Many-shot in-context learning in multimodal foundation models. arXiv preprint arXiv:2405.09798, 2024.   \nO. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023.   \nJ. Knowles. Parego: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE transactions on evolutionary computation, 10(1):50–66, 2006.   \nA. M. Lamb, A. G. ALIAS PARTH GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. Advances in neural information processing systems, 29, 2016.   \nJ. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, et al. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024.   \nI. Levy, B. Bogin, and J. Berant. Diverse demonstrations improve in-context compositional generalization. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401–1422, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.78. URL https://aclanthology.org/2023.acl-long.78.   \nJ. Li, B. Hui, G. Qu, J. Yang, B. Li, B. Li, B. Wang, B. Qin, R. Geng, N. Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024a.   \nM. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong. In-context learning with many demonstration examples. arXiv preprint arXiv:2302.04931, 2023.   \nZ. Li, C. Li, M. Zhang, Q. Mei, and M. Bendersky. Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach. arXiv preprint arXiv:2407.16833, 2024b.   \nZ. Lin and K. Lee. Dual operating modes of in-context learning. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 30135– 30188. PMLR, 21–27 Jul 2024. URL https://proceedings.mlr.press/v235/lin24l.html.   \nJ. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100–114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics.   \nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to fnd them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086–8098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556.   \nM. Marion, A. Üstün, L. Pozzobon, A. Wang, M. Fadaee, and S. Hooker. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023.   \nK. Opsahl-Ong, M. J. Ryan, J. Purtell, D. Broman, C. Potts, M. Zaharia, and O. Khattab. Optimizing instructions and demonstrations for multi-stage language model programs. arXiv preprint arXiv:2406.11695, 2024.   \nB. Paria, K. Kandasamy, and B. Póczos. A fexible framework for multi-objective bayesian optimization using random scalarizations. In Uncertainty in Artifcial Intelligence, pages 766–776. PMLR, 2020.   \nA. Patel, B. Li, M. S. Rasooli, N. Constant, C. Rafel, and C. Callison-Burch. Bidirectional language models are also few-shot learners. arXiv preprint arXiv:2209.14500, 2022.   \nM. Pourreza, H. Li, R. Sun, Y. Chung, S. Talaei, G. T. Kakkar, Y. Gan, A. Saberi, F. Ozcan, and S. O. Arik. Chase-sql: Multi-path reasoning and preference optimized candidate selection in text-to-sql. International Conference on Learning Representations (ICLR), 2025.   \nM. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   \nB. Ru, X. Wan, X. Dong, and M. Osborne. Interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels. International Conference on Learning Representations (ICLR), 2021.   \nO. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. In M. Carpuat, M.-C. de Marnefe, and I. V. Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655–2671, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.191. URL https://aclanthology.org/2022.naacl-main. 191.   \nW. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R. Müller. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE, 109(3): 247–278, 2021.   \nR. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618–626, 2017.   \nK. Simonyan. Deep inside convolutional networks: Visualising image classifcation models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.   \nR. E. Steuer and E.-U. Choo. An interactive weighted tchebychef procedure for multiple objective programming. Mathematical programming, 26:326–344, 1983.   \nM. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In International conference on machine learning, pages 3319–3328. PMLR, 2017.   \nX. Wan, V. Nguyen, H. Ha, B. Ru, C. Lu, and M. A. Osborne. Think global and act local: Bayesian optimisation over high-dimensional categorical and mixed search spaces. In International Conference on Machine Learning, pages 10663–10674. PMLR, 2021.   \nX. Wan, R. Sun, H. Dai, S. Arik, and T. Pfster. Better zero-shot reasoning with self-adaptive prompting. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 3493–3514, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.fndings-acl.216. URL https://aclanthology.org/2023.findings-acl.216.   \nX. Wan, R. Sun, H. Nakhost, H. Dai, J. Eisenschlos, S. Arik, and T. Pfster. Universal self-adaptive prompting. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7437–7462, Singapore, Dec. 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.461. URL https: //aclanthology.org/2023.emnlp-main.461.   \nX. Wan, R. Sun, H. Nakhost, and S. O. Arik. Teach better or show smarter? on instructions and exemplars in automatic prompt optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id $\\equiv$ IdtoJVWVnX.   \nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484–13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https: //aclanthology.org/2023.acl-long.754.   \nC. Williams and C. Rasmussen. Gaussian processes for regression. Advances in neural information processing systems, 8, 1995.   \nC. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.   \nM. Xia, S. Malladi, S. Gururangan, S. Arora, and D. Chen. LESS: selecting infuential data for targeted instruction tuning. In Forty-frst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= PG5fV50maR.   \nX. Xu, Y. Liu, P. Pasupat, M. Kazemi, et al. In-context learning with retrieved demonstrations for language models: A survey. arXiv preprint arXiv:2401.11624, 2024.   \nE. Zelikman, Y. Wu, J. Mu, and N. Goodman. STar: Bootstrapping reasoning with reasoning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_3ELRdg2sgI.   \nD. Zhan and H. Xing. Expected improvement for expensive optimization: a review. Journal of Global Optimization, 78(3):507–544, 2020.   \nZ. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id $\\cdot$ 5NTt8GFjUHkr.   \nZ. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 12697–12706, 2021.   \nH. Zhou, X. Wan, Y. Liu, N. Collier, I. Vulić, and A. Korhonen. Fairer preferences elicit improved human-aligned large language model judgments. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1241–1252, Miami, Florida, USA, Nov. 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.72. URL https://aclanthology.org/2024.emnlp-main. 72/.   \nH. Zhou, X. Wan, L. Proleev, D. Mincu, J. Chen, K. A. Heller, and S. Roy. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id $\\equiv$ L3FHMoKZcS. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "A. Derivation of the Approximated Importance Score ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In this section, we give detailed derivation of the importance score used in Sec. 2 to rank the examples – on a high level, we use a similar approach to Ru et al. (2021) in determining the importance from the GP surrogate: Recalling that we are given a pool of examples $\\varepsilon$ with $|\\mathcal{E}|=m$ , a collection of $T$ subsets of $\\mathbf{e}_{i}$ , each represented as a binary vector $\\mathbf{e}_{i}\\,\\in\\,\\{0,1\\}^{m}$ and their corresponding scores on the validation set $g(\\cdot)\\,:\\,\\{0,1\\}^{m}\\,\\to\\,\\mathbb{R}_{!}$ , we frst ft a GP regression with $\\mathbf{e}_{1:T}\\,=\\,[\\mathbf{e}_{1},...,\\mathbf{e}_{T}]^{\\intercal}$ and $\\begin{array}{r}{\\mathbf{g}_{1:T}=[g(\\mathbf{e}_{1},...,g(\\mathbf{e}_{T})]^{\\intercal}}\\end{array}$ , as presented in Eq. 2, the mean of the posterior GP ${\\hat{g}}(\\cdot)$ is given by: ",
        "page_idx": 19
    },
    {
        "type": "equation",
        "text": "$$\n\\mathbb{E}_{\\hat{g}(\\mathbf{e})\\mid\\mathcal{G}_{T}}[\\hat{g}(\\mathbf{e})]=\\mathbf{k}_{1:T}(\\mathbf{K}+\\eta^{2}\\mathbf{I})^{-1}\\mathbf{g}_{1:T},\n$$",
        "text_format": "latex",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "where we defne $\\mathcal{G}_{T}$ as the shorthand of $\\left[\\mathbf{e}_{1:T},\\mathbf{g}_{1:T}\\right]$ to denote that the ftted function $\\hat{g}(\\mathbf{e})$ is ftted on the observed input-output pairs; $\\mathbf{k}_{t}=[k(\\mathbf{e},\\mathbf{e}_{1}),...,k(\\mathbf{e},\\mathbf{e}_{t})]$ and $k(\\cdot,\\cdot)$ is the covariance function of the GP (we use Matern 2.5 by default). As mentioned in Sec. 2, whereas we do not assume any diferentiability property from $g(\\cdot)$ on e, since the approximated function ${\\hat{g}}(\\cdot)$ follows a posterior GP, its gradient w.r.t e is analytically available and is itself a GP, given by: ",
        "page_idx": 19
    },
    {
        "type": "equation",
        "text": "$$\n\\nabla_{\\mathbf{e}}g={\\frac{\\partial g(\\mathbf{e})}{\\partial\\mathbf{e}}}={\\frac{\\partial\\mathbf{k}_{1:T}}{\\partial\\mathbf{e}}}(\\mathbf{K}+\\eta^{2}\\mathbf{I})^{-1}\\mathbf{g}_{1:T},\n$$",
        "text_format": "latex",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "noting that the expensive matrix inversion term, $(\\mathbf{K}\\!+\\!\\eta^{2}\\mathbf{I})^{-1}$ does not have a dependence on e and can be directly cached from Eq. 3 when we compute the posterior mean. The derivative term is essentially a diferentiation operation of the covariance function to the input and can be easily computed either analytically for common kernel choices or via automatic diferentiation for popular GP or BO packages like gpytorch (Gardner et al., 2018) or botorch (Balandat et al., 2020). ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "With the computed $\\nabla_{\\mathbf{e}}g\\in\\mathbb{R}^{m}$ , we can in principle compute the estimated derivative at any $\\mathbf{e}\\subseteq{\\mathcal{E}}$ . However, in practice, we fnd the derivative estimate to be more reliable at the training points of the GP (i.e., $[\\mathbf{e}_{1},...,\\mathbf{e}_{T}]$ . We then evaluate the derivative at each of the training points, and the fnal importance score is marginalized by averaging across the training points: ",
        "page_idx": 19
    },
    {
        "type": "equation",
        "text": "$$\nM(e^{(j)})=\\frac{1}{T}\\sum_{t=1}^{T}\\nabla_{\\mathbf{e}}\\hat{g}|_{\\mathbf{e}=\\mathbf{e}_{t}}^{(j)},\n$$",
        "text_format": "latex",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "where we use the superscript $(j)$ to denote that the estimated importance of the $j$ -th individual example (note the regular font $e\\,\\in\\,{\\mathcal{E}}$ denoting an individual example instead of the bold-face e denoting a set of examples in $\\varepsilon$ ). We then compute the importance score of all examples in $\\varepsilon$ , which is then used to generate the assigned ranking in the analysis of Sec. 2 such as Fig. 4. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "B. Implementation Details ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "B.1. Datasets. ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "In the section below, we give detailed implementation details for the availability, data splitting protocol, input prompts, and licensing information of the datasets used. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "BIG-Bench Hard (BBH). BBH is a collection of 26 challenging reasoning tasks and a task is selected if either 1) if it is studied in the seminal work on many-shot ICL (Agarwal et al., 2024) or 2) if the zero-shot performance of gemini-1.5-pro-001 is below $90\\%$ , which indicates non-saturation of performance – these criteria led to a set of 16 tasks that we consider in Sec. 4. For all tasks, we randomize the data points and reserve $40\\%$ (usually 100 samples, but some sub-tasks of BBH benchmark have fewer data-points) as held-out sets for testing, whose inputs and labels are not revealed to the model except for fnal evaluation. For the rest of the dataset, in Sec. 2, we use $50\\%$ ( $30\\%$ of all available data points including the held-out test set) as the “train-set” from which the examples are generated and the other $50\\%$ for validation (i.e., the split where results in Fig. 4 is generated). In Sec. 4, we do not use the aforementioned validation set and use performance on the same set that generates the examples as the optimization objective. The BBH dataset is publicly available at https://github.com/suzgunmirac/BIG-Bench-Hard under an MIT license. For all BBH tasks, we use the prompt templates below: ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "1 You will be given a question . Think step by step before giving a final answer to this question . Show your final answer {{ TASK_SPECIFIC_CONSTRAINTS }} between answer > and <\\ answer >   \n2   \n3 {{ EXAMPLES }}   \n$_{4}==$   \n5   \n6 {{ QUESTION }}   \n7 {{ llm () }} ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "where we use a Jinja2-style syntax and the upper-cased blocks bracketed between double braces are variables that are replaced at inference time: TASK_SPECIFIC_CONSTRAINTS denote the constraint instruction specifc to the type of the task. For example, for a multiple-choice task, this is replaced with “answer option letter only”; for a binary choice question, this is replaced with “Yes or No only” and for a free-form generation task, this is replaced by an empty string. EXAMPLES denote the concatenation of any examples e added to the input – for the initial generation step (i.e., Step 3 in Algorithm 1), we use zero-shot prompting and EXAMPLES is an empty string. For the subsequent generation step, this is replaced with the concatenation of the examples selected by bridge; fnally, llm() denotes the place where an LLM response is solicited; the answer is then extracted and postprocessed to match with a ground-truth answer to measure accuracy. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "MATH and GSM-Hard. In MATH and GSM-Hard, we similarly adopt the unifed train set setup as mentioned in the previous paragraph. To minimize the chance of data contamination where the training inputs and outputs were leaked to the model during pre-training or instruction fnetuning, we randomly sample 128 samples as the ofcial test set as the train set from which the examples are generated and use the rest of the ofcial test set for testing. The MATH dataset is available at https://github.com/hendrycks/math and GSM-Hard is available at https://huggingface. co/datasets/reasoning-machines/gsm-hard. Both datasets are licensed under an MIT license. On GSM-Hard, we use the same prompt as the BBH dataset mentioned above. On MATH, we use the inner monologue prompt consists of a human-annotated few-shot prompt given by Agarwal et al. (2024) with an added preamble to ensure that the LLM generation follows the style of the examples given, noting that any model-generated examples will be added to the initial human-annotated examples: ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "You will be given several example math questions and their solutions . At the end , You will be given another question that you will need to solve . Make sure you follow the examples and always finish your answer with ’Final Answer : The final answer is X. I hope it is correct .’ where X is the correct answer . 2 3 Problem : 4 Find the domain of the expression \\$\\ frac {\\ sqrt {x -2}}{\\ sqrt {5-x}}\\$. 5 6 Solution : 7 8 I need to find the domain of this expression . 10 This expression has two square root expressions . 11 12 I know the values under each square root must be non - negative . 13 14 So let me start by ensuring that. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "16 I set $\\Phi x-2\\setminus\\deg\\e\\ \\ 0\\Phi$ , which simplifies to $\\Phi\\mathbf{x}\\setminus\\mathbf{g}\\,{\\mathsf{e}}\\,2\\Phi$ .   \n17   \n18 Then , I set $\\Phi5\\mathrm{~\\texttt~{~-~}~}\\mathbf{x}\\setminus\\mathbf{g}\\mathbf{e}\\mathrm{~\\texttt~{~0~}~}$ , which simplifies to $\\Phi\\mathbf{x}$ \\le $^{5\\,\\Phi}$ .   \n19   \n20 But I need to make sure the denominator is not zero.   \n21   \n22 So , $\\Phi_{}5-\\mathbf{x}\\,\\Phi$ must also be greater than zero , which gives me $\\Phi\\,{\\sf x}<5\\,\\Phi$ .   \n23   \n24 Therefore , the domain of the expression is \\$\\ boxed {[2 ,5) }\\$.   \n25   \n26 Answer : $\\Phi\\left[2\\right.,5\\right)\\Phi$ .   \n27   \n28 Final Answer : The final answer is $\\Phi\\left[2\\right],5)\\,\\mathfrak{F}$ . I hope it is correct .   \n$\\mathrm{~\\textit~{~29~}~}==\\mathbf{=}$   \n30   \n31 Problem :   \n32 If $\\mathfrak{F}\\setminus$ det \\ mathbf $\\{\\mathbb{A}\\}\\ =\\ 2\\Phi$ and \\$\\det \\ mathbf $\\begin{array}{r l r}{\\{\\mathbb{B}\\}}&{{}=}&{12{\\it\\Delta\\phi},\\Phi}\\end{array}$ then find \\$\\det (\\ mathbf {A} \\   \nmathbf {B}). $\\Phi$   \n33   \n34 Solution :   \n35   \n36 I need to find the determinant of the product of matrices A and B.   \n37   \n38 I remember that the determinant of the product of two matrices equals the product of   \ntheir determinants .   \n39   \n40 So , \\$\\ det (\\ mathbf {A} \\ mathbf {B}) $=$ (\\ det \\ mathbf {A}) (\\ det \\ mathb $:\\{B\\}\\}\\ =\\ \\ (\\ 2)\\ (\\ 12)=\\ \\ \\backslash$   \nboxed {24} \\$.   \n41   \n42 Answer : $\\Phi_{24}\\Phi$ .   \n43   \n44 Final Answer : The final answer is $\\Phi_{2}4\\Phi$ . I hope it is correct .\"\"\" ,   \n45 r \"\"\" Problem :   \n46 Terrell usually lifts two 20- pound weights 12 times . If he uses two 15- pound weights   \ninstead , how many times must Terrell lift them in order to lift the same total   \nweight ?   \n47   \n48 Solution :   \n49   \n50   \n51 Okay , so Terrell lifts a total of \\$2\\ cdot 12\\ cdot $20=480\\Phi$ pounds of weight with the   \n20- pound weights .   \n52   \n53 Well , if he switches to 15- pound weights , the total weight lifted will be \\$2\\   \ncdot15 \\ cdot $\\mathtt{n}\\!=\\!30\\,\\mathtt{n}\\,\\Phi$ pounds , where n is the number of lifts .   \n54   \n55 I want to find the number of lifts , n, for the total weight lifted to be the same.   \n56   \n57 I equate $\\Phi30{\\,\\mathrm{n}}\\Phi$ to 480 pounds and solve for n.   \n58   \n59 \\ begin { align \\*}   \n60 30n &=480\\\\   \n61 \\ Rightarrow \\ qquad n &=480/30=\\ boxed {16}   \n62 \\ end { align \\*}   \n63   \n64 Answer : $\\Phi_{^{16}}\\Phi$ .   \n65   \n66 Final Answer : The final answer is $\\Phi\\bot6\\Phi$ . I hope it is correct .   \n$67\\_{\\mathrm{~\\,~}}==$   \n68   \n69 Problem :   \n70 If the system of equations   \n71   \n72 \\ begin { align \\*}   \n73 6x -4y $\\&=\\mathtt{a}$ ,\\\\   \n74 6y -9x $\\&{}=\\mathbf{b}$ .   \n75 \\ end { align \\*}   \n76   \n77 has a solution $\\Phi\\left(\\textbf{x},\\ \\textbf{y}\\right)\\Phi$ where \\$x\\$ and \\$y\\$ are both nonzero , find \\$\\frac {a}{b},\\$   \nassuming \\$b\\$ is nonzero .   \n78   \n79 Solution :   \n80   \n81 I’m given a system of two equations .   \n82   \n83 I see that if I multiply the first equation by \\$ -\\ frac {3}{2}\\$, $\\mathtt{T}\\phantom{\\rule{0.3em}{0ex}}^{\\textnormal{\\scriptsize>}}\\!\\!\\!\\perp\\mathtt{T}$ get another   \nequation that has the same left - hand side as the second equation , $\\Phi6\\mathtt{y}-9\\,\\mathtt{x}\\,\\mathfrak{P}$ .   \n84   \n85 Let me try that $\\oint\\oint\\oint\\oint\\oint\\mathbf{x}=-\\left\\langle\\oint\\mathbf{r}\\,\\mathbf{a}\\,\\mathbf{c}\\;\\{3\\,\\}\\,{\\left\\{\\,2\\,\\right\\}}\\,\\mathbf{a}\\;.\\;\\Phi\\Phi$   \n86   \n87 Ah , I also know that $\\Phi6\\mathrm{y}-\\9\\,\\mathrm{x}=\\mathrm{b}\\Phi$ , so I can equate these two equations .   \n88   \n89 So , \\$\\$ -\\ frac {3}{2} a $=$ b\\ Rightarrow \\ frac {a}{b} $\\r=$ \\ boxed { -\\ frac {2}{3}}. \\$\\$   \n90   \n91 Answer : \\$ -\\ frac {2}{3} \\$.   \n92   \n93 Final Answer : The final answer is \\$ -\\ frac {2}{3} \\$. I hope it is correct .   \n$94~==$   \n95   \n96 {{ EXAMPLES }}   \n97   \n$98\\ \\mathrm{~}=\\mathbf{=}$   \n99 Problem :   \n100 {{ QUESTION }}   \n101   \n102 Solution :   \n103   \n104 {{ llm () }} ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "BIRD On BIRD, we randomly sample 128 samples from the train split as the unifed train and validation set and use the ofcial test set (of 1534 data points) for testing. Since BIRD is a code generation task, the execution accuracy is computed not via a simple string match between the predicted and the ground-truth SQLs but by actually executing both SQLs on the database provided, and a score of 1 is only assigned when the predicted SQL is both executable and if whose results exactly match the execution results from the ground-truth SQL. All data, including the databases, schemas, and ground-truth gold SQL are available at the ofcial repo: https://bird-bench.github.io under a CC BY-SA 4.0 license. With reference to Table 7, use two prompt versions for diferent rows. The direct prompt is a standard, zero-shot prompt to elicit the SQL prediction directly; it is used both for the “Direct” row to directly extract LLM answer and is also used as the prompt template for fnetuning in the diferent “LoRA” rows: ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "an SQL expert tasked with answering user ’s questions about SQL tables by generating SQL queries in the SQLite dialect . 2 Use only the following tables to answer the question : 4 5 {{ SCHEMA }} 6 7 Question : {{ QUESTION }} 8 Hint: {{ HINT }} 9 SQL: {{ llm () }} ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "where SCHEMA refers to the table schema, which can be generated automatically by querying the database, QUESITON is the natural language question that we would like the LLM to convert to a SQL command and HINT is a hint which additionally explains the question provided by the BIRD dataset. For the chase and chase $+$ bridge rows, we use the prompt template proposed in Pourreza et al. (2025) to invoke reasoning and divide-and-conquer before the LLM gives the fnal answer: ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "1 You are an experienced database expert .   \n2 Now you need to generate a SQL query given the database information , a question and some additional information .   \n3 The database structure is defined by the following table schemas ( comments after ’--’ provide additional column descriptions ).   \n4 Note that the \" Example Values \" are actual values from the column . Some column might contain the values that are directly related to the question . Use it to help you justify which columns to use.   \n6 Given the table schema information description and the ‘Question ‘. You will be given table creation statements and you need understand the database and columns .   \n8 You will be using a way called \" recursive divide -and - conquer approach to SQL query generation from natural language \".   \n10 Here is a high level description of the steps .   \n11 1. \\*\\* Divide ( Decompose Sub - question with Pseudo SQL):\\*\\* The complex natural language question is recursively broken down into simpler sub - questions . Each sub - question targets a specific piece of information or logic required for the final SQL query .   \n12 2. \\*\\* Conquer ( Real SQL for sub - questions ):\\*\\* For each sub - question (and the main question initially ), a \" pseudo - SQL \" fragment is formulated . This pseudo - SQL represents the intended SQL logic but might have placeholders for answers to the decomposed sub - questions .   \n13 3. \\*\\* Combine ( Reassemble ) :\\*\\* Once all sub - questions are resolved and their corresponding SQL fragments are generated , the process reverses . The SQL fragments are recursively combined by replacing the placeholders in the pseudo - SQL with the actual generated SQL from the lower levels .   \n14 4. \\*\\* Final Output :\\*\\* This bottom -up assembly culminates in the complete and correct SQL query that answers the original complex question .   \n15   \n16 Database admin instructions ( violating any of the following is punishable to death !)   \n17 1. \\*\\* SELECT Clause :\\*\\*   \n18 Only select columns mentioned in the user ’s question .   \n19 - Avoid unnecessary columns or values .   \n20 2. \\*\\* Aggregation ( MAX / MIN ) :\\*\\*   \n21 - Always perform JOINs before using MAX () or MIN ().   \n22 3. \\*\\* ORDER BY with Distinct Values :\\*\\*   \n23 - Use ‘GROUP BY <column $>^{\\epsilon}$ before ‘ORDER BY <column $>$ ASC|DESC ‘ to ensure distinct values .   \n24 4. \\*\\* Handling NULLs :\\*\\*   \n25 - If a column may contain NULL values ( indicated by \"None \" in value examples or explicitly ), use ‘JOIN ‘ or ‘WHERE <column > IS NOT NULL ‘.   \n26 5. \\*\\* FROM / JOIN Clauses :\\*\\*   \n27 - Only include tables essential to answer the question .   \n28 6. \\*\\* Strictly Follow Hints :\\*\\*   \n29 Adhere to all provided hints .   \n30 7. \\*\\* Thorough Question Analysis : $^{\\ast\\ast}$   \n31 - Address all conditions mentioned in the question .   \n32 8. \\*\\* DISTINCT Keyword :\\*\\*   \n33 - Use ‘SELECT DISTINCT ‘ when the question requires unique values (e.g., IDs , URLs ).   \n34 - Refer to column statistics (\" Value Statics \") to determine if ‘DISTINCT ‘ is necessary .   \n35 9. \\*\\* Column Selection :\\*\\*   \n36 - Carefully analyze column descriptions and hints to choose the correct column when similar columns exist across tables .   \n37 10. \\*\\* String Concatenation :\\*\\*   \n38 - Never use ‘|| ’ ’ ||‘ or any other method to concatenate strings in the ‘ SELECT ‘ clause .   \n39 11. \\*\\* JOIN Preference :\\*\\*   \n40 Prioritize ‘INNER JOIN ‘ over nested ‘SELECT ‘ statements .   \n41 12. \\*\\* SQLite Functions Only :\\*\\*   \n42 - Use only functions available in SQLite .   \n43 13. \\*\\* Date Processing :\\*\\*   \n44 - Utilize ‘STRFTIME () ‘ for date manipulation (e.g., ‘STRFTIME (’%   \n45   \n46 When you get to the final query , output the query string ONLY inside the xml delimiter <FINAL_ANSWER > </ FINAL_ANSWER >.   \n47   \n48 Here are some examples :   \n49   \n50 {{ EXAMPLES }}   \n51   \n52 Now is the real question , following the instruction and examples , generate the SQL with Recursive Divide -and - Conquer approach .   \n53 \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "54 [ Table creation statements ]   \n55 {{ SCHEMA }}   \n56   \n57 \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*   \n58 [ Question ]   \n59 Question : {{ QUESTION }}   \n60 Hint: {{ HINT }}   \n61   \n62 \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*   \n63 [ Answer ]   \n64 Repeating the question and hint , and generating the SQL with Recursive Divide -and -   \nConquer .   \n65 {{ llm () }} ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Similar to the inner monologue prompt in the MATH dataset, the EXAMPLES in the template above also include 4 human annotated examples at initialization (an exemplary human-annotated example is shown in the code snippet below); any model-generated examples from bridge are again concatenated to the human annotated examples at inference. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "\\*\\*\\*\\*\\*\\*\\*\\*\\* 2 [ Table creation statements ] 3 CREATE TABLE generalinfo 4 ( 5 id_restaurant INTEGER not null primary key , 6 food_type TEXT null , -- examples : ‘thai ‘| ‘food type ‘ description : the food type 7 city TEXT null , -- description : the city where the restaurant is located in 8 ); 9 10 CREATE TABLE location 11 ( 12 id_restaurant INTEGER not null primary key , 13 street_name TEXT null , - - examples : ‘ave ‘, ‘san pablo ave ‘, ‘pablo ave ‘| ‘street name ‘ description : the street name of the restaurant 14 city TEXT null , -- description : the city where the restaurant is located in 15 foreign key ( id_restaurant ) references generalinfo ( id_restaurant ) on update cascade on delete cascade , 16 ); 17 18 \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* 19 [ Question ] 20 Question : How many Thai restaurants can be found in San Pablo Ave , Albany ? 21 Hint : Thai restaurant refers to food_type $=$ ’thai ’; San Pablo Ave Albany refers to street_name $=$ ’san pablo ave ’ AND T1.city $=$ ’albany ’ 22 23 \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* 24 [ Answer ] 25 Repeating the question and hint , and generating the SQL with Recursive Divide -and - Conquer . 26 \\*\\* Question \\*\\*: How many Thai restaurants can be found in San Pablo Ave , Albany ? 27 $^{\\ast\\ast}$ Hint \\*\\*: Thai restaurant refers to food_type $=$ ’thai ’; San Pablo Ave Albany refers to street_name $=$ ’san pablo ave ’ AND T1.city $=$ ’albany ’ 28 29 \\*\\*1. Divide and Conquer :\\*\\* 30 31 \\* \\*\\* Main Question : $^{\\ast\\ast}$ How many Thai restaurants can be found in San Pablo Ave , Albany ? 32 \\* \\*\\* Analysis :\\*\\* The question asks for a count of restaurants , so we ’ll use ‘ COUNT () ‘ for that . The count should include only Thai restaurants , which we can identify using the ‘food_type ‘ column in the ‘generalinfo ‘ table . The location \"San Pablo Ave , Albany \" spans two columns (‘ street_name ‘ and ‘city ‘) in the ‘ location ‘ table , requiring us to join these two tables . 33 \\* \\*\\* Pseudo SQL :\\*\\* SELECT COUNT (‘T1 ‘.‘ id_restaurant ‘) FROM ‘generalinfo ‘ AS ‘T1 ‘ INNER JOIN ‘location ‘ AS ‘T2 ‘ ON ‘T1 ‘.‘ id_restaurant ‘ = ‘T2 ‘.‘ id_restaurant ‘ WHERE <Thai restaurant $>$ AND <in San Pablo Ave , Albany > 34 35 \\* \\*\\* Sub - question 1:\\*\\* Thai restaurant 36 \\* \\*\\* Analysis :\\*\\* This is a straightforward filter on the ‘generalinfo ‘ table using the ‘food_type ‘ column . 37 \\* \\*\\* Pseudo SQL :\\*\\* ‘T1 ‘. ‘ food_type ‘ $=$ ’thai ’ 38 ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "39 \\* \\*\\*Sub - question 2:\\*\\* in San Pablo Ave , Albany   \n40 \\* \\*\\* Analysis :\\*\\* This location information is spread across two columns in the ‘location ‘ table . We need to combine these conditions with an \"AND\" operator to ensure both are met.   \n41 \\* \\*\\* Pseudo SQL : $^{\\ast\\ast}$ ‘T2 ‘. ‘ street_name ‘ $=$ ’san pablo ave ’ AND ‘T2 ‘.‘city ‘ = ’ albany ’   \n42   \n43 $**2$ . Assembling SQL :\\*\\*   \n44   \n45 \\* \\*\\*Sub - question 1 ( Thai restaurant ):\\*\\*   \n46 \\* \\*\\* SQL :\\*\\* ‘T1 ‘. ‘ food_type ‘ $=$ ’thai ’   \n47   \n48 \\* \\*\\*Sub - question 2 (in San Pablo Ave , Albany ): $^{\\ast\\ast}$   \n49 \\* \\*\\* SQL :\\*\\* ‘T2 ‘. ‘ street_name ‘ $=$ ’san pablo ave ’ AND ‘T2 ‘.‘ city ‘ $=$ ’albany ’   \n50   \n51 \\* \\*\\* Main Question ( count of restaurants ) :\\*\\*   \n52 \\* \\*\\* SQL :\\*\\* SELECT COUNT (‘T1 ‘.‘ id_restaurant ‘) FROM ‘generalinfo ‘ AS ‘T1 ‘ INNER JOIN ‘location ‘ AS ‘T2 ‘ ON ‘T1 ‘.‘ id_restaurant ‘ = ‘T2 ‘.‘ id_restaurant ‘ WHERE ‘T1 ‘. ‘ food_type ‘ $=$ ’thai ’ AND ‘T2 ‘.‘ street_name ‘ $=$ ’san pablo ave ’ AND ‘T2 ‘.‘ city ‘ $=$ ’albany ’   \n53   \n54 $^{**3}$ . Simplification and Optimization :\\*\\*   \n55   \n56 \\* The SQL query from step 2 is already quite efficient . We ’ve used ‘INNER JOIN ‘ to combine the tables based on their relationship , and the ‘WHERE ‘ clause clearly defines our filtering criteria . There ’s no need for nested queries or complex sub - selections in this case .   \n57   \n58 \\*\\* Final Optimized SQL Query :\\*\\*   \n59   \n60 < FINAL_ANSWER >   \n61 SELECT COUNT (T1. id_restaurant ) FROM generalinfo AS T1 INNER JOIN location AS T2 ON T1. id_restaurant $=$ T2. id_restaurant WHERE T1. food_type $=$ ’thai ’ AND T1.city = albany ’ AND T2. street_name $=$ ’san pablo ave ’   \n62 </ FINAL_ANSWER > ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "B.2. Implementation details of the Inflling baseline ",
        "text_level": 1,
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Inflling is a technique of generating the intermediate outputs given both input queries and the ground-truth answer – this is used as a baseline in Tables 1 and 3 where we utilize all available labeled data in the context. Concretely, we use the following prompt adapted from Hu et al. (2023) to generate the intermediate rationales. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "1 You will be given a question and its final , ground - truth correct answer .   \n2 Given the question and the answer , generate the step -by - step reasoning steps that led to the correct answer . Write your intermediate reasoning steps (but NOT the final answer ) leading to the final answer between <answer > and </answer >. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "4 Question : {{ question }}   \n5 Answer : {{ target }}   \n6 Steps : {{ llm ()) }} ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "C. Additional Experiments and Results ",
        "text_level": 1,
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "C.1. Ablation and Sensitivity Studies ",
        "text_level": 1,
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Importance of Bayesian optimization. To ablate bridge, in Table 9 and Table 8, we compare against a simplifed variant of bridge with BO replaced with random search consuming the same evaluation budget (32 per stage) – we fnd that while random search is a remarkably strong baseline, BO nevertheless outperformed it consistently at all stages of the bridge pipeline. ",
        "page_idx": 25
    },
    {
        "type": "table",
        "img_path": "images/a4332ef94f40f447ef1ae924994f955c6932a531b384335cb4d1cf5cbb2054ce.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Tasks #Iterations</td><td colspan=\"5\">BRIDGE-RS</td><td colspan=\"5\">BRIDGE-BO</td></tr><tr><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>59.32.0</td><td>66.71.6</td><td>67.71.5</td><td>63.01.1</td><td>64.01.6</td><td>61.32.7</td><td>66.02.0</td><td>63.31.5</td><td>65.01.6</td><td>65.31.5</td></tr><tr><td>date_understanding</td><td>84.81.3</td><td>90.50.5</td><td>93.30.4</td><td>93.00.7</td><td>94.50.8</td><td>85.01.3</td><td>90.50.5</td><td>91.50.4</td><td>90.80.7</td><td>92.50.8</td></tr><tr><td>disambiguation_qa</td><td>73.81.3</td><td>74.51.1</td><td>74.01.2</td><td>75.30.8</td><td>70.51.1</td><td>77.51.3</td><td>79.01.1</td><td>77.51.2</td><td>76.30.8</td><td>74.31.1</td></tr><tr><td>dyck_languages</td><td>64.51.5</td><td>62.53.6</td><td>65.54.2</td><td>64.81.1</td><td>68.02.5</td><td>63.32.0</td><td>62.01.7</td><td>64.51.8</td><td>62.82.4</td><td>61.83.8</td></tr><tr><td>formal fallacies</td><td>77.31.1</td><td>75.02.6</td><td>74.51.7</td><td>77.51.7</td><td>78.32.5</td><td>78.31.3</td><td>77.31.5</td><td>75.51.7</td><td>78.31.8</td><td>76.30.8</td></tr><tr><td>geometric_shapes</td><td>88.53.8</td><td>93.33.0</td><td>94.52.1</td><td>98.00.0</td><td>95.31.9</td><td>93.82.5</td><td>94.04.2</td><td>95.51.1</td><td>97.00.0</td><td>98.00.0</td></tr><tr><td>hyperbaton</td><td>94.00.7</td><td>94.30.4</td><td>95.00.7</td><td>95.00.7</td><td>88.81.5</td><td>86.57.6</td><td>95.51.1</td><td>95.80.8</td><td>94.80.4</td><td>93.31.5</td></tr><tr><td>logical deduction (7)</td><td>62.83.3</td><td>54.52.2</td><td>67.81.9</td><td>64.02.6</td><td>66.81.9</td><td>61.85.1</td><td>57.51.1</td><td>70.50.9</td><td>66.51.1</td><td>75.00.7</td></tr><tr><td>movie recommendation</td><td>68.54.0</td><td>75.32.6</td><td>72.51.7</td><td>77.51.3</td><td>77.51.8</td><td>70.32.3</td><td>73.32.3</td><td>77.31.5</td><td>78.82.0</td><td>72.83.2</td></tr><tr><td>multistep_arithmetic_two</td><td>82.50.5</td><td>92.31.3</td><td>95.01.4</td><td>89.51.5</td><td>92.52.6</td><td>96.32.3</td><td>96.80.4</td><td>97.80.4</td><td>94.80.8</td><td>95.80.4</td></tr><tr><td>object_counting</td><td>92.01.2</td><td>92.51.5</td><td>92.51.1</td><td>93.00.7</td><td>92.31.1</td><td>92.81.9</td><td>93.82.3</td><td>95.50.5</td><td>93.01.2</td><td>93.80.4</td></tr><tr><td>ruin_names</td><td>89.01.2</td><td>88.00.7</td><td>88.02.4</td><td>87.01.2</td><td>84.51.1</td><td>89.30.4</td><td>89.30.8</td><td>87.01.2</td><td>90.30.8</td><td>90.01.2</td></tr><tr><td>salient translation error detection</td><td>66.32.8</td><td>69.32.5</td><td>67.02.6</td><td>68.51.8</td><td>68.82.1</td><td>62.80.8</td><td>71.00.7</td><td>69.82.0</td><td>69.00.7</td><td>67.30.4</td></tr><tr><td>snarks</td><td>87.23.0</td><td>90.61.2</td><td>88.91.7</td><td>93.41.5</td><td>91.01.6</td><td>88.92.0</td><td>89.91.8</td><td>89.60.7</td><td>90.60.6</td><td>83.73.5</td></tr><tr><td>sports_understanding</td><td>96.51.1</td><td>96.30.4</td><td>97.30.4</td><td>95.80.4</td><td>96.80.8</td><td>93.31.1</td><td>95.30.4</td><td>91.80.4</td><td>95.01.2</td><td>95.00.0</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>98.30.8</td><td>89.50.9</td><td>96.51.1</td><td>92.32.4</td><td>98.51.5</td><td>98.00.7</td><td>93.82.2</td><td>98.00.0</td><td>97.80.4</td><td>97.50.5</td></tr><tr><td>Average</td><td>80.31</td><td>81.55</td><td>83.11</td><td>82.98</td><td>82.97</td><td>81.61</td><td>82.79</td><td>83.79</td><td>83.77</td><td>83.25</td></tr><tr><td>△(BO-RS)</td><td></td><td></td><td></td><td></td><td></td><td>+1.30</td><td>+1.24</td><td>+0.68</td><td>+0.79</td><td>+0.28</td></tr></table></body></html>\n\n",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Table 8 Comparison between bridge with BO (bridge-bo) and bridge with random search (bridge-rs) using gemini-1.5-flash-001 on BBH tasks. The bridge-bo results are lifted from Table 3, and the last row denotes the average improvement due to the use of BO over RS at the milestone in the progression of bridge. Refers to captions of Table 1 for additional explanations. ",
        "page_idx": 26
    },
    {
        "type": "table",
        "img_path": "images/ec6a61621fb23121219c15f293490552e2f2cbd82dc7406b7cd322cff537d698.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Tasks #Iterations</td><td colspan=\"5\">BRIDGE-RS</td><td colspan=\"5\">BRIDGE-BO</td></tr><tr><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>66.23.0</td><td>68.52.0</td><td>70.22.4</td><td>69.52.4</td><td>70.82.2</td><td>68.31.5</td><td>62.71.6</td><td>59.71.5</td><td>72.00.0</td><td>70.02.0</td></tr><tr><td>date_understanding</td><td>88.42.3</td><td>94.31.0</td><td>94.11.2</td><td>90.33.3</td><td>94.31.3</td><td>92.21.5</td><td>97.00.7</td><td>94.81.9</td><td>95.01.2</td><td>95.51.8</td></tr><tr><td>disambiguation_qa</td><td>75.52.1</td><td>79.02.9</td><td>77.41.2</td><td>80.62.3</td><td>78.44.0</td><td>71.82.4</td><td>77.53.6</td><td>80.51.8</td><td>81.32.9</td><td>78.81.5</td></tr><tr><td>dyck_languages</td><td>56.95.4</td><td>59.64.9</td><td>67.54.3</td><td>64.94.0</td><td>70.42.7</td><td>49.22.7</td><td>76.23.8</td><td>80.02.7</td><td>77.51.1</td><td>76.83.8</td></tr><tr><td>formal_fallacies</td><td>87.41.5</td><td>86.82.3</td><td>90.82.1</td><td>88.52.2</td><td>88.82.2</td><td>86.02.1</td><td>85.02.5</td><td>90.82.3</td><td>90.82.8</td><td>88.22.3</td></tr><tr><td>geometric_shapes</td><td>77.83.2</td><td>82.14.0</td><td>81.82.5</td><td>86.53.8</td><td>85.52.4</td><td>78.52.1</td><td>82.53.6</td><td>89.23.8</td><td>92.31.1</td><td>89.20.8</td></tr><tr><td>hyperbaton</td><td>94.31.6</td><td>93.12.4</td><td>94.21.3</td><td>94.91.5</td><td>94.01.2</td><td>96.50.9</td><td>94.21.5</td><td>94.82.8</td><td>96.50.5</td><td>97.20.4</td></tr><tr><td>logical_deduction (7)</td><td>70.93.3</td><td>68.32.7</td><td>66.62.5</td><td>71.93.3</td><td>68.92.1</td><td>70.21.5</td><td>70.84.5</td><td>71.73.7</td><td>71.51.8</td><td>69.22.2</td></tr><tr><td>movie_recommendation</td><td>63.53.2</td><td>67.41.8</td><td>67.42.1</td><td>64.62.3</td><td>63.42.9</td><td>67.01.2</td><td>69.50.5</td><td>69.33.1</td><td>72.81.8</td><td>67.01.2</td></tr><tr><td>multistep_arithmetic_two</td><td>97.31.1</td><td>97.50.7</td><td>96.90.8</td><td>96.11.5</td><td>97.90.3</td><td>96.20.8</td><td>94.51.1</td><td>97.00.7</td><td>98.00.7</td><td>96.81.8</td></tr><tr><td>object_counting</td><td>95.32.4</td><td>98.11.1</td><td>97.31.7</td><td>97.31.9</td><td>95.42.3</td><td>96.20.4</td><td>96.01.9</td><td>94.51.1</td><td>94.20.4</td><td>95.00.7</td></tr><tr><td>ruin_names</td><td>86.61.7</td><td>86.51.9</td><td>88.91.8</td><td>89.91.2</td><td>87.11.7</td><td>90.81.1</td><td>88.81.7</td><td>89.21.5</td><td>88.82.4</td><td>90.30.8</td></tr><tr><td>salient_translation_error_detection</td><td>71.13.2</td><td>73.41.6</td><td>73.92.2</td><td>71.91.5</td><td>70.81.6</td><td>68.80.8</td><td>71.00.7</td><td>69.52.2</td><td>74.00.7</td><td>74.51.1</td></tr><tr><td>snarks</td><td>93.81.6</td><td>95.31.4</td><td>96.01.6</td><td>96.01.1</td><td>95.61.8</td><td>93.43.0</td><td>95.80.0</td><td>95.11.6</td><td>96.91.5</td><td>97.61.8</td></tr><tr><td>sports_understanding</td><td>93.51.7</td><td>94.10.6</td><td>95.10.9</td><td>95.90.9</td><td>96.01.7</td><td>92.81.9</td><td>97.01.2</td><td>96.20.8</td><td>95.80.4</td><td>95.80.8</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>92.43.8</td><td>94.41.2</td><td>99.90.3</td><td>98.40.9</td><td>100.00.0</td><td>95.80.4</td><td>95.01.2</td><td>100.00.0</td><td>97.00.7</td><td>99.50.5</td></tr><tr><td>Average</td><td>81.86</td><td>83.64</td><td>84.86</td><td>84.81</td><td>84.82</td><td>82.11</td><td>84.61</td><td>85.77</td><td>87.13</td><td>86.33</td></tr><tr><td>△(BO-RS)</td><td></td><td></td><td></td><td></td><td></td><td>+0.25</td><td>+0.97</td><td>+0.91</td><td>+2.32</td><td>+1.51</td></tr></table></body></html>\n\n",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Table 9 Comparison between bridge with BO (bridge-bo) and bridge with random search (bridge-rs) using gemini-1.5-pro-001 on BBH tasks. The bridge-bo results are lifted from Table 1, and the last row denotes the average improvement due to the use of BO over RS at the milestone in the progression of bridge. Refers to captions of Table 1 for additional explanations. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Comparison to and combination with heuristic demonstration selection. An alternative to iteratively optimize the demonstrations in the “Optimize” step is using heuristics for demonstration selection which may incur a lower computational cost as we no longer have to repeatedly evaluate on the labeled validation set $m$ times. In this section, we study two representative demonstration selection techniques: retrieval based on similarity in the embedding space and diversity, and we both study them as standalone alternatives to the full bridge pipeline and, given that demonstration selection is not the only component of the bridge framework, it is also straightforward to combine them with bridge by swapping the BO/random search component in the “Optimize” step with these heuristics. Below we describe the implementation details of both techniques: ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "• Retrieval: One popular demonstration selection method is via retrieval (Das et al., 2021; Rubin et al., 2022). Concretely, we may either use an of-the-shelf pretrained embedding model (we use the latest Gecko embedding (Lee et al., 2024) for this purpose) or tune a customized retriever to obtain the nearest examples from an example store, typically by computing the vector embedding for each of the test queries and each of the cached demonstrations followed by a maximum inner product search (MIPS) to retrieve the top- $^{\\,k}$ demonstrations based on cosine similarity. Unlike the optimization-based approach where the number of examples in the context can be determined automatically, $k$ here is a key hyperparameter that needs to be set by the user. In this case, consider 3 diferent $k$ values: $k=\\{10,25\\}$ where the number of examples is fxed, or $k=\\mathrm{All}$ , where we use all available, correctly predicted examples – this essentially uses the same set of examples as Reinforced ICL but in a specifc, input-dependent order: the examples are sorted in an ascending order based on the cosine similarity between the embedding of the test input and the example store and the most similar examples appears as the fnal demonstration that is directly concatenated to the test input. • Diversity: Another popular learning-free demonstration selection method is by selecting diverse examples. While multiple ways to measure diversity exist, here we use the technique similar to the one used in Zhang et al. (2023) by 1) computing the embedding of all the available demonstrations and 2) running the $k$ -means clustering algorithm and selecting the $k$ examples whose vector embeddings are nearest to each of the $k$ centroids. Unlike retrieval, there is no input dependency as the clustering algorithm does not depend on the input query but similar to retrieval, $k$ here is also a hyperparameter to be set and we again use $k=\\{10,25\\}$ . Note that we omit $k=\\mathrm{All}$ , as otherwise the number of clusters would be equal to the number of examples and we would essentially be running Reinforced ICL with all available examples as demonstrations. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Since these demonstration selection baselines purely perform selection (i.e., the “optimize” step of bridge) but neither the subsequent generations nor the iterative process, we frst compare the BO demonstration selection (i.e., bridge at Step 1o) against these baselines and we show the results in Table 11. Overall, we fnd that “Diversity” and “Retrieval”, regardless of their hyperparameters, perform on par or slightly worse than Reinforced ICL. While the hyperparameter choice can sometimes lead to signifcant diferences on a per-task level, we also observe that when aggregated across the tasks, it does not lead to signifcant diferences. On the other hand, the BO selection in bridge outperforms all these baselines. We believe there are two possible explanations leading to this outperformance. Firstly, while the heuristic-based methods have lower computational costs, key hyperparameters, such as the number of demonstrations to retrieve, need to be determined a-priori. However, as we have shown in the main text at, for example, Fig. 4, the optimal number of demonstrations can be highly task-specifc, and while iterative optimization-based selection incurs a higher cost, it is also capable of optimizing the number of demonstrations. Secondly, a key fnding we have in Sec. 2 is that not all examples are equally helpful, and removing some examples as in-context demonstrations can sometimes lead to performance improvement during the “Optimize” stage. Again, while the heuristic-based approaches do not necessarily use all demonstrations, it makes the selection choice purely from a heuristic metric (e.g., similarity to test query) rather than from a validation metric, and hence is incapable of removing these potentially “harmful” demonstrations from the pool of candidate examples. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "However, beyond a simple comparison between a single stage of bridge against these methods, it is also worth noting that bridge is more than a demonstration selection method. As such, it is also possible to combine these methods with bridge by using them as a drop-in replacement of the BO-based demonstration selection, efectively changing the implementation of the “Optimize” step only. To test this, we test two other variants of bridge, named bridge-retrieval and bridge-diversity, where we replace the “Optimize” step in each round with the heuristic-driven demonstration selection mentioned above and the aggregated results are shown in Table 12 whereas the task-specifc breakdown of the best method in Table 13 – for conciseness, we only show the per-task breakdown for the best bridge variant (bridge-retrieval using all examples), which show that bridge also works well with alternative demonstration selection method, although the advantage of optimization-based selection as shown in Table 11 carries over when we use the selection as a component in the overall bridge pipeline. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Additional comparisons against iterative reinforced ICL in a restricted setup. To provide further evidence emphasizing the need for the “Optimize” step and to make sure that the additional gain of bridge does not simply come from the fact that bridge may take advantage of more correctly predicted demonstrations in the validation set due to repeated sampling in the later iterations, we conduct a further experiment comparing bridge and iterative reinforced ICL, but in a restricted setup with the support set restricted to the subset of the train set where the model predicted correctly initially, instead of the entire train set. In other words, in subsequent iterations of bridge and iterative reinforced ICL, both methods are restricted to make use of the subset of the train set initially predicted correctly only as examples; we term these approaches “iterative reinforced ICL (restricted)” and “bridge (restricted)” respectively, and we show the results in Table 10. On a high level, we found the result provides further evidence of the importance of selection: Iterative Reinf ICL (restricted) without the \"optimize\" step actually did not meaningfully improve over standard Reinf ICL (average accuracy: $79.6\\%$ ; bridge (restricted), however, still meaningfully improves with the subsequent optimize and generate steps, although the gain is less than the original bridge which utilizes more examples via the larger train set support. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "C.2. Number of examples ",
        "text_level": 1,
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "We show the number of examples used for each experiment corresponding to Table 1 in Table 14. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "C.3. Additional Visualizations ",
        "text_level": 1,
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "In this section, we show analysis similar to Fig. 4 on tasks not represented in the fgure of the main text. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "C.4. Using bridge for low-resource translation ",
        "text_level": 1,
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "While we have primarily considered the reinforced ICL setup suitable for reasoning and general problem-solving tasks, it is worth noting that the bridge framework may also generalize to other practical settings that beneft from many-shot ICL with some modifcation on the “optimize” and the “generate” steps. In this section, we conduct a preliminary analysis of the applicability of bridge in the context of machine translation (MT) for low-resource languages. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "As noted in Agarwal et al. (2024) and Reid et al. (2024), low-resource machine translation (MT) is one of the task types where many-shot in-context learning (ICL) has demonstrated remarkable performance. In these tasks, there is often a nearly monotonic improvement in translation quality as more source-target language pairs are incorporated into the context – as a notable exception to our observations in Sec. 2 that primarily involve reasoning tasks, in low resource MT, we often observe “more is better” given the information-dense nature of translation tasks – indeed, for translation tasks, barring glaring human errors in the annotation process, the provided data is generally assumed to be of high quality and problems like false positive in model-generated reasoning paths in reasoning tasks are generally negligible for tasks like low resource MT with high quality annotated data. However, in low-resource languages, the model’s inherent knowledge is often weak or non-existent due to the lack of exposure to target languages during pre-training or fne-tuning, which can lead to a bottleneck in data availaility, especially for extremely low-resource languages, where 1) the model lacks zero-shot translation abilities due to insufcient exposure to target languages, and 2) the scarcity of annotated data becomes a critical limiting factor – to address these, previous works often attempt to augment ground-truth translation data with model-synthesized translations (Han et al., 2021; Patel et al., 2022). ",
        "page_idx": 28
    },
    {
        "type": "table",
        "img_path": "images/1ee5bf35061570272c7902f562a164aa16f3ecd2eca052839935ee40aed2e0cc.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td colspan=\"3\">Tasks RestrictedIterative</td><td colspan=\"5\">Restricted BRIDGE</td></tr><tr><td></td><td colspan=\"3\">Reinf.</td><td colspan=\"3\">(Ours)</td><td></td></tr><tr><td># Iterations</td><td>1</td><td>2</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>69.71.1</td><td>65.01.5</td><td>67.72.7</td><td>65.01.1</td><td>66.01.5</td><td>67.01.1</td><td>65.02.0</td></tr><tr><td>date_understanding</td><td>92.51.5</td><td>94.01.0</td><td>92.51.7</td><td>93.01.6</td><td>92.51.1</td><td>93.51.7</td><td>89.31.5</td></tr><tr><td>disambiguation_qa</td><td>74.00.7</td><td>75.83.0</td><td>70.52.7</td><td>72.31.5</td><td>75.53.2</td><td>71.82.4</td><td>76.53.8</td></tr><tr><td>dyck_languages</td><td>59.06.5</td><td>53.02.9</td><td>55.05.2</td><td>52.35.4</td><td>56.53.4</td><td>57.51.7</td><td>60.33.8</td></tr><tr><td>formal_fallacies</td><td>86.83.3</td><td>90.52.2</td><td>85.31.5</td><td>90.51.1</td><td>83.03.2</td><td>83.50.9</td><td>85.52.3</td></tr><tr><td>geometric_shapes</td><td>75.31.8</td><td>78.33.1</td><td>75.02.6</td><td>80.54.5</td><td>81.34.5</td><td>85.02.6</td><td>80.02.6</td></tr><tr><td>hyperbaton</td><td>85.34.0</td><td>84.53.4</td><td>94.01.6</td><td>95.80.8</td><td>91.81.3</td><td>93.02.6</td><td>97.00.7</td></tr><tr><td>logical_deduction (7)</td><td>67.51.8</td><td>69.02.5</td><td>69.52.7</td><td>71.53.2</td><td>66.82.1</td><td>70.52.3</td><td>70.82.2</td></tr><tr><td>movie_recommendation</td><td>64.82.7</td><td>63.32.2</td><td>68.33.3</td><td>62.04.1</td><td>63.01.2</td><td>63.31.3</td><td>61.82.2</td></tr><tr><td>multistep_arithmetic_two</td><td>95.01.2</td><td>95.50.5</td><td>97.80.8</td><td>95.30.8</td><td>95.51.8</td><td>95.81.3</td><td>95.81.5</td></tr><tr><td>object_counting</td><td>94.52.9</td><td>94.52.1</td><td>96.01.6</td><td>94.80.8</td><td>94.31.3</td><td>96.00.7</td><td>96.31.1</td></tr><tr><td>ruin names</td><td>87.31.3</td><td>88.80.8</td><td>91.50.9</td><td>89.51.8</td><td>88.52.1</td><td>90.00.0</td><td>89.81.8</td></tr><tr><td>salient translation</td><td>68.02.1</td><td>67.31.5</td><td>70.02.2</td><td>69.83.3</td><td>72.82.4</td><td>73.52.7</td><td>75.82.2</td></tr><tr><td>snarks</td><td>93.81.2</td><td>93.82.1</td><td>93.82.3</td><td>95.51.2</td><td>95.10.7</td><td>95.50.6</td><td>94.82.1</td></tr><tr><td>sports_understanding</td><td>94.01.4</td><td>95.01.0</td><td>92.81.1</td><td>95.51.5</td><td>97.00.0</td><td>96.00.7</td><td>94.80.8</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>66.83.1</td><td>67.31.9</td><td>99.00.0</td><td>96.31.5</td><td>98.50.5</td><td>97.51.5</td><td>100.00.0</td></tr><tr><td>Average</td><td>79.62</td><td>79.70</td><td>82.40</td><td>82.45</td><td>82.37</td><td>83.08</td><td>83.32</td></tr></table></body></html>\n\n",
        "page_idx": 29
    },
    {
        "type": "table",
        "img_path": "images/8f6fcfb84448bb350b6f2c7324bd0c86e4b3b57d6ba28e30b63908461b459c61.jpg",
        "table_caption": [
            "Table 10 | Comparison of bridge and iterative reinforced ICL in the restricted setup where the methods may only use the subset of the train set that the model initially predicted correctly. Experiments performed on gemini-1.5-pro-001. ",
            "Table 11 Comparison between bridge with one step of demonstration optimization only (i.e., 1o) against Retrieval, Diversity and Reinforced ICL baselines using gemini-1.5-pro-001. Note that the bridge (1o) and Reinforced ICL results are taken from Table 1. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td colspan=\"2\">Diversity</td><td colspan=\"3\">Retrieval</td><td>Reinf.</td><td>BRIDGE</td></tr><tr><td>Details / hyperparams</td><td>k =10</td><td>k=25</td><td>k =10</td><td>k=25</td><td>All</td><td>ICL</td><td>10</td></tr><tr><td>causal_judgement</td><td>66.71.6</td><td>66.32.4</td><td>63.01.5</td><td>67.72.4</td><td>66.72.5</td><td>66.34.8</td><td>68.31.5</td></tr><tr><td>date_understanding</td><td>93.21.3</td><td>93.02.7</td><td>87.03.5</td><td>93.31.5</td><td>93.01.9</td><td>88.82.5</td><td>92.21.5</td></tr><tr><td>disambiguation_qa</td><td>72.23.0</td><td>77.80.8</td><td>76.50.9</td><td>71.20.8</td><td>77.51.1</td><td>76.82.4</td><td>71.82.4</td></tr><tr><td>dyck_languages</td><td>54.015.7</td><td>38.52.6</td><td>39.54.4</td><td>33.23.1</td><td>47.85.2</td><td>55.53.6</td><td>49.22.7</td></tr><tr><td>formal_fallacies</td><td>85.51.5</td><td>85.01.9</td><td>88.50.5</td><td>88.23.0</td><td>84.21.9</td><td>86.21.1</td><td>86.02.1</td></tr><tr><td>geometric_shapes</td><td>71.24.4</td><td>69.31.6</td><td>69.82.8</td><td>68.54.2</td><td>79.23.3</td><td>80.22.8</td><td>78.52.1</td></tr><tr><td>hyperbaton</td><td>95.01.2</td><td>92.22.5</td><td>96.51.1</td><td>97.21.3</td><td>95.21.9</td><td>90.21.1</td><td>96.50.9</td></tr><tr><td>logical_deduction (7)</td><td>65.83.0</td><td>67.54.4</td><td>69.24.4</td><td>66.32.9</td><td>67.32.4</td><td>65.83.5</td><td>70.21.5</td></tr><tr><td>movie_recommendation</td><td>67.32.6</td><td>65.02.5</td><td>68.53.4</td><td>68.01.4</td><td>67.33.3</td><td>65.21.6</td><td>67.01.2</td></tr><tr><td>multistep_arithmetic_two</td><td>92.81.3</td><td>96.20.4</td><td>95.50.9</td><td>94.81.6</td><td>94.31.9</td><td>96.50.5</td><td>96.20.8</td></tr><tr><td>object_counting</td><td>95.81.1</td><td>95.20.8</td><td>97.22.4</td><td>95.21.9</td><td>91.22.2</td><td>95.50.9</td><td>96.20.4</td></tr><tr><td>ruin_names</td><td>87.81.3</td><td>89.81.3</td><td>87.80.8</td><td>91.52.1</td><td>90.52.2</td><td>89.81.9</td><td>90.81.1</td></tr><tr><td>salient_translation_error_detection</td><td>68.52.3</td><td>69.52.1</td><td>68.23.3</td><td>58.22.8</td><td>61.02.1</td><td>69.01.6</td><td>68.80.8</td></tr><tr><td>snarks</td><td>94.82.3</td><td>96.21.2</td><td>94.41.7</td><td>97.61.2</td><td>95.51.2</td><td>92.73.2</td><td>93.43.0</td></tr><tr><td>sports_understanding</td><td>95.01.2</td><td>95.81.1</td><td>95.50.9</td><td>95.80.8</td><td>95.01.9</td><td>93.01.4</td><td>92.81.9</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>55.84.5</td><td>56.85.5</td><td>60.24.3</td><td>67.89.7</td><td>60.22.4</td><td>62.34.2</td><td>95.80.4</td></tr><tr><td>Average</td><td>78.83</td><td>78.38</td><td>78.59</td><td>78.41</td><td>79.12</td><td>79.61</td><td>81.61</td></tr></table></body></html>\n\n",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 29
    },
    {
        "type": "table",
        "img_path": "images/24e4ca93714bbd7c14e596affa0bab6125353547cc0f82a339d97f73b55f14f8.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Method</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>BRIDGE-DIVERSITY (k = 10)</td><td>77.10</td><td>79.47</td><td>78.58</td><td>81.89</td><td>79.50</td></tr><tr><td>BRIDGE-DIVERSITY (k =25)</td><td>78.15</td><td>80.86</td><td>78.74</td><td>80.63</td><td>79.68</td></tr><tr><td>BRIDGE-NEAREST (k =10)</td><td>79.07</td><td>81.80</td><td>81.40</td><td>81.35</td><td>80.39</td></tr><tr><td>BRIDGE-NEAREST (k = 25)</td><td>78.36</td><td>79.49</td><td>80.16</td><td>81.09</td><td>80.10</td></tr><tr><td>BRIDGE-NEAREST (AIl)</td><td>79.65</td><td>82.91</td><td>82.01</td><td>83.20</td><td>84.14</td></tr></table></body></html>\n\n",
        "page_idx": 30
    },
    {
        "type": "table",
        "img_path": "images/1033433240c183a9e7f4a14a0efa013bc238c02a70507fa5174e56fe30e199e7.jpg",
        "table_caption": [
            "Table 12 | Average test accuracy on BBH tasks using gemini-1.5-pro-001 by combining bridge with diferent variants of the heuristic demonstration selection methods. Bold text in this table shows the best algorithm variant at each round of bridge. ",
            "Table 13 | Task-specifc test accuracy on BBH tasks using gemini-1.5-pro-001 with bridgenearest (All) (best method from Table 12). "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Task</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>73.01.1</td><td>62.31.5</td><td>64.70.7</td><td>65.72.2</td><td>63.32.7</td></tr><tr><td>date_understanding</td><td>94.31.3</td><td>92.01.6</td><td>95.01.4</td><td>92.22.3</td><td>92.80.4</td></tr><tr><td>disambiguation_qa</td><td>76.80.4</td><td>75.85.0</td><td>72.01.0</td><td>82.02.7</td><td>82.80.8</td></tr><tr><td>dyck_languages</td><td>58.82.3</td><td>75.04.3</td><td>75.03.3</td><td>78.53.0</td><td>82.01.2</td></tr><tr><td>formal fallacies</td><td>84.20.8</td><td>88.51.7</td><td>90.50.9</td><td>89.51.8</td><td>90.00.7</td></tr><tr><td>geometric_shapes</td><td>75.82.5</td><td>86.23.3</td><td>79.80.8</td><td>84.02.1</td><td>84.51.1</td></tr><tr><td>hyperbaton</td><td>96.00.7</td><td>93.82.3</td><td>97.00.0</td><td>92.53.2</td><td>98.80.4</td></tr><tr><td>logical_deduction (7)</td><td>65.83.7</td><td>73.82.3</td><td>68.03.7</td><td>70.01.9</td><td>71.21.8</td></tr><tr><td>movie_recommendation</td><td>67.01.2</td><td>69.51.7</td><td>63.21.1</td><td>70.02.5</td><td>73.80.8</td></tr><tr><td>multistep_arithmetic_two</td><td>92.50.5</td><td>97.01.2</td><td>96.70.8</td><td>97.50.9</td><td>94.00.0</td></tr><tr><td>object_counting</td><td>91.81.5</td><td>95.01.2</td><td>97.00.7</td><td>96.51.7</td><td>100.00.0</td></tr><tr><td>ruin_names</td><td>88.80.4</td><td>92.00.7</td><td>88.52.1</td><td>89.20.8</td><td>88.21.1</td></tr><tr><td>salient translation error detection</td><td>63.21.5</td><td>70.01.6</td><td>70.20.4</td><td>70.01.2</td><td>70.50.5</td></tr><tr><td>snarks</td><td>95.81.7</td><td>94.81.2</td><td>93.71.2</td><td>96.51.2</td><td>95.80.0</td></tr><tr><td>sports_understanding</td><td>94.00.7</td><td>96.51.5</td><td>93.80.4</td><td>95.51.5</td><td>94.20.4</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>56.81.6</td><td>64.50.9</td><td>67.01.0</td><td>61.54.4</td><td>64.21.6</td></tr><tr><td>Average</td><td>79.65</td><td>82.91</td><td>82.01</td><td>83.20</td><td>84.14</td></tr></table></body></html>\n\n",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "In this section, along this line of work, we investigate the applicability of bridge as a method to iteratively improve the model-synthesized translation so that they can act as more efective augmentations to the scarce ground-truth data. Specifcally, we assume the following in our setup: ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "• Availability of some ground-truth source-target sentence pairs – this pair will both act as the train set from which ground-truth examples are generated and also as the validation set for machine-generated translations. • Abundant source language text – this is almost always true. For example, if we are interested in translating from English to a low-resource language, it is extremely easy to obtain abundant text in English whereas the difculty is to obtain the corresponding translation in the target ",
        "page_idx": 30
    },
    {
        "type": "table",
        "img_path": "images/4db4088007a8478dc520cae54e3f473104e86b791d107d3d130eaa7475cb7a12.jpg",
        "table_caption": [
            "Table 14 Number of examples for each experiment corresponding to Table 1 (gemini-1.5-pro-001 on BBH tasks). Note that the “All” columns always use all 75 examples provided. "
        ],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks #Iterations</td><td rowspan=\"2\">Reinf. ICL 1</td><td colspan=\"2\">Iter. Reinf.</td><td colspan=\"5\">BRIDGE-BO</td></tr><tr><td></td><td>2</td><td>3</td><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>causal_judgement</td><td>36</td><td>40</td><td>43</td><td>11</td><td>43</td><td>4</td><td>39</td><td>39</td></tr><tr><td>date_understanding</td><td>61</td><td>67</td><td>72</td><td>57</td><td>73</td><td>44</td><td>73</td><td>57</td></tr><tr><td>disambiguation_qa</td><td>42</td><td>66</td><td>69</td><td>28</td><td>61</td><td>60</td><td>68</td><td>65</td></tr><tr><td>dyck_languages</td><td>15</td><td>40</td><td>52</td><td>9</td><td>45</td><td>42</td><td>59</td><td>20</td></tr><tr><td>formal_fallacies</td><td>60</td><td>69</td><td>69</td><td>2</td><td>63</td><td>30</td><td>67</td><td>57</td></tr><tr><td>geometric_shapes</td><td>42</td><td>59</td><td>68</td><td>40</td><td>59</td><td>19</td><td>71</td><td>70</td></tr><tr><td>hyperbaton</td><td>70</td><td>75</td><td>75</td><td>4</td><td>75</td><td>69</td><td>75</td><td>59</td></tr><tr><td>logical_deduction (7)</td><td>46</td><td>60</td><td>62</td><td>11</td><td>54</td><td>51</td><td>64</td><td>61</td></tr><tr><td>movie_recommendation</td><td>42</td><td>53</td><td>54</td><td>39</td><td>49</td><td>36</td><td>51</td><td>41</td></tr><tr><td>multistep_arithmetic_two</td><td>65</td><td>74</td><td>74</td><td>38</td><td>74</td><td>28</td><td>72</td><td>38</td></tr><tr><td>object_counting</td><td>65</td><td>75</td><td>75</td><td>60</td><td>75</td><td>48</td><td>75</td><td>14</td></tr><tr><td>ruin_names</td><td>58</td><td>70</td><td>71</td><td>51</td><td>70</td><td>69</td><td>69</td><td>21</td></tr><tr><td>salient_translation_error_detection</td><td>44</td><td>59</td><td>60</td><td>13</td><td>58</td><td>7</td><td>59</td><td>41</td></tr><tr><td>snarks</td><td>47</td><td>50</td><td>51</td><td>19</td><td>49</td><td>5</td><td>48</td><td>39</td></tr><tr><td>sports_understanding</td><td>64</td><td>75</td><td>75</td><td>52</td><td>75</td><td>74</td><td>74</td><td>68</td></tr><tr><td>tracking_shuffled_objects (7)</td><td>58</td><td>60</td><td>53</td><td>2</td><td>75</td><td>1</td><td>75</td><td>22</td></tr><tr><td>Average</td><td>50.94</td><td>62.00</td><td>63.94</td><td>27.25</td><td>62.38</td><td>36.69</td><td>64.94</td><td>44.50</td></tr></table></body></html>\n\n",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "language. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "• LLM for “pseudo-labeling” – we assume the availability of a (strong) LLM that can be queried to generate synthesized data. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Algorithm 3 bridge for MT. ",
        "text_level": 1,
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "1: Input: train set $\\mathcal{D}$ , unlabeled set with source language sentence, $\\boldsymbol{\\mathcal{U}}$ , number of iteration rounds $K\\in\\mathbb N$ (outer-loop), evaluation budget for BO per iteration $n_{\\mathrm{eval}}$ (inner-loop), Generator model used to synthesize examples $M_{g}$ .   \n2: Output: Optimized set of model-synthesized examples $\\varepsilon^{*}$ .   \n3: Partition $\\mathcal{D}$ into two disjoint sets $\\mathcal{D}_{t}$ and ${\\mathcal{D}}_{\\nu}$ via random sampling.   \n4: [Generate] Generate the pool of initial examples $\\mathcal{E}_{0}$ by predicting $\\ensuremath{\\mathcal{M}}_{g}$ on the unlabeled set, using the entire train set $\\mathcal{D}$ as the demonstrations in the context: $\\mathcal{E}_{0}\\gets M_{g}(\\mathcal{U}|\\mathcal{D})$ .   \n5: for $k\\in\\{1,...,K\\}$ (Outer loop) do   \n6: [Optimize] Run Bayesian optimization (calling subroutine Algorithm 2 on the ${\\mathcal{D}}_{\\nu}$ to obtain $\\mathbf{e}_{k}^{*}\\quad\\gets$ $\\mathrm{BayesOpt}(n_{\\mathrm{eval}}\\!=\\!n_{\\mathrm{eval}},\\mathcal{E}\\!=\\!\\mathcal{E}_{k})$ .   \n7: [Generate] Re-generate examples $\\mathit{{E}}_{k}$ by re-predicting the LLM on the unlabeled set, but with the optimized examples $\\mathbf{e}_{k}^{\\ast}$ from the previous step and $\\mathcal{D}_{t}$ as demonstrations; the {inputs, output}-pairs are concatenated to form the new set of examples $\\mathcal{E}_{k}$ for the next [Optimize] step.   \n8: end for   \n9: return Optimized example set $\\varepsilon^{\\ast}$ after $K$ rounds. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "To approach the problem, we propose to retain the high-level framework of bridge but modify the “optimize” and “generate” steps to accommodate the low-resource MT setup. With reference to Algorithm 3 where we have marked the key diferences in blue, the main diference lies in the “generate” step: instead of generating examples with model-generated reasoning paths in the case presented in the main text, here we synthesized examples on the unlabeled set $\\mathcal{U}$ that we assumed to be available. Since we no longer have access to the ground-truth translation of the sentences in $\\mathcal{U}$ , we optimize for the optimal subset $\\mathbf{e}^{*}$ by evaluating diferent combinations of the synthesized examples on the partition of the labeled dataset $\\delta_{\\nu}$ . ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "To test bridge on the MT setup, we consider the English-Bemba translation task in the Flores dataset (Guzmán et al., 2019) that was also considered in Agarwal et al. (2024). We assume the access to 100 labeled examples as $\\mathcal{D}$ and 50 unlabeled examples $\\mathcal{U}$ , and hold out another 400 samples as the test set. We use Gemini Flash as the target model and Gemini Pro as the generator model in Algorithm 3, and we show the result in Table 15. Overall, we observe that running iterative optimization also improves performance on this task, both exemplifed by improvement on the test and validation chrf score, although it seems that an additional optimization round, in this case, led to a small performance degradation. While a more comprehensive evaluation is required, we believe the preliminary result is promising for future eforts in this direction. ",
        "page_idx": 31
    },
    {
        "type": "image",
        "img_path": "images/8386771fb1c0fe84070404b8f72ad71b662514b40d14215e97e7bdf58cbcc9b5.jpg",
        "img_caption": [
            "Figure 5 Additional visualization of the task performance at diferent rounds. Note that in most datasets, additional rounds of bridge led to performance improvement, and some of the exceptions (e.g., multi_arithmetric_two) are possibly caused by visualization artifacts of the extremely small performance variation as shown by the small y-axis ranges. "
        ],
        "img_footnote": [],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 32
    },
    {
        "type": "table",
        "img_path": "images/ecedb5e416b5e7c7782deccfeb9a2fc5fd00f193a681e28e412846830f7894b6.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Tasks #Iterations</td><td rowspan=\"2\">Gold-only</td><td rowspan=\"2\">All 0</td><td colspan=\"5\">BRIDGE-MT</td></tr><tr><td>10</td><td>1G</td><td>20</td><td>2G</td><td>30</td></tr><tr><td>en bem</td><td>37.78</td><td>38.46</td><td>38.33</td><td>39.11</td><td>39.30</td><td>38.90</td><td>39.29</td></tr></table></body></html>\n\n",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Table 15 Test chrf score of gemini-1.5-flash-001. “Gold-only” refers to the result obtained by only using the 100 labeled examples in the context; “All” refers to the result with 100 labeled examples $+~50$ initially generated examples from gemini-1.5-pro-001. Refers to captions of Table 1 for additional explanations. ",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "ansferring learned demonstrations from GSM-Hard to G ",
        "text_level": 1,
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "In this section, we investigate whether the bridge-discovered demonstrations can transfer across related but distinct datasets. Specifcally, we investigate the extent to which the demonstrations found on GSM-Hard (Table 2) generalize to the original GSM-8K and we show the result in Table 16, where we compare the performance of the demonstrations directly transferred from GSM-Hard at diferent stages of bridge against directly optimizing on GSM-8K. We fnd that whereas the demonstrations generated from (iterative) reinforced ICL led to a small deterioration of GSM-8K performance, we found the transferred demonstrations from bridge led to a small improvement even though the Gemini 1.5 Pro performance on GSM-8K has been rather saturated. While optimizing directly on GSM-8K unsurprisingly led to the highest performance given that there is no distribution shift, we also fnd that the GSM-Hard demonstrations exhibit considerable generalizability. ",
        "page_idx": 33
    },
    {
        "type": "table",
        "img_path": "images/4f3191b19954f61d14e4c828ac8ffd12c62de3bf7f161afe7999391de0515a75.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>Tasks</td><td>O-shot</td><td>Reinf.</td><td colspan=\"2\">Iterative Reinf.</td><td colspan=\"5\">BRIDGE</td></tr><tr><td>#Iterations</td><td></td><td>ICL 0</td><td>1</td><td>2</td><td>10</td><td>1G</td><td>(Ours) 20</td><td>2G</td><td>30</td></tr><tr><td>Direct</td><td>91.92</td><td>93.81</td><td>93.06</td><td>92.68</td><td>93.81</td><td>93.18</td><td>94.70</td><td>94.19</td><td>93.94</td></tr><tr><td>Transferred</td><td></td><td>90.66</td><td>91.79</td><td>91.16</td><td>93.81</td><td>92.55</td><td>93.81</td><td>93.18</td><td>91.16</td></tr></table></body></html>\n\n",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Table 16 Comparison of the transferred bridge-generated demonstrations on GSM-Hard vs. directly running bridge on GSM-8K. Runs with performance deteriorations w.r.t. the 0-shot results are marked in red in the table. ",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "D. Computational Cost Analysis ",
        "text_level": 1,
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "In this section, we provide a computational cost analysis of bridge. In general, since bridge consists of multiple rounds of “Optimize” and “Generate” steps, here we analyze each step in detail. ",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "• Optimize: The cost of the “optimize” step depends on the budget allocated ( $\\left<n_{\\mathrm{eval}}\\right>$ in Line 5 of Algorithm 2), which is user-confgurable. If we opt for iterative optimization (such as using Bayesian optimization in the main section of the paper, or random search in App. C.1), each “optimize” step thus entails $n_{\\mathrm{eval}}$ LLM inferences on the validation set. As shown in the App. C.1, it is also possible to use a non-iterative method based on retrieval or embedding diversity, in which case each “optimize” step entails a single round of LLM inferences on the validation set (or the train set, if we use the dataset for both training and validation). • Generate: The “generate” step always involves a single round of LLM inferences on the train set where we simply use the optimized examples from the “optimize” step above as demonstrations and run inference again on the train set. ",
        "page_idx": 33
    }
]