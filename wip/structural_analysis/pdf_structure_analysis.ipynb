{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "pdf_path = \"../data/2502.00330v1.pdf\"\n",
    "doc = fitz.open(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on PDF Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_infos = doc.get_toc(simple=False) or []\n",
    "\n",
    "pdf_toc = []\n",
    "for item in toc_infos:\n",
    "    lvl = item[0] if len(item) > 0 else None\n",
    "    title = item[1] if len(item) > 1 else None\n",
    "    start_page = item[2] if len(item) > 2 else None\n",
    "    end_pos = item[3].get('to') if len(item) > 3 and item[3] else None\n",
    "    nameddest = item[3].get('nameddest') if len(item) > 3 and item[3] else None\n",
    "\n",
    "    if start_page is not None:\n",
    "        page = doc[start_page-1]\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "\n",
    "        lines = \"\"\n",
    "        for block in blocks:\n",
    "            x0, y0, x1, y1, text, _, _ = block\n",
    "            if len(lines) < 100:\n",
    "                if end_pos and x0 >= end_pos[0]:\n",
    "                    lines += text\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        pdf_toc.append({\n",
    "            \"level\": lvl,\n",
    "            \"title\": title,\n",
    "            \"page\": start_page,\n",
    "            \"position\": end_pos,\n",
    "            \"nameddest\": nameddest,\n",
    "            \"text\": lines[:200] + \"...\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pdf_toc: \n",
    "    lvl_1_toc = [item for item in pdf_toc if item[\"level\"] == 1]\n",
    "    sorted_lvl_1_toc = sorted(lvl_1_toc, key=lambda d: d['page'])\n",
    "\n",
    "# 基于level的位置，切分pdf\n",
    "for idx, item in enumerate(lvl_1_toc):\n",
    "    title = item.get('title')\n",
    "    start_page = item.get('page')\n",
    "    start_pos = item.get('position')\n",
    "    if idx < len(lvl_1_toc) - 1:\n",
    "        next_item = lvl_1_toc[idx+1]\n",
    "        end_page = next_item.get('page')\n",
    "        end_pos = next_item.get('position')\n",
    "    else:\n",
    "        end_page = None\n",
    "        end_pos = None\n",
    "    print(title, start_page, start_pos, end_page, end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pdf_toc: \n",
    "    lvl_1_toc = [item for item in pdf_toc if item[\"level\"] == 1]\n",
    "    sorted_lvl_1_toc = sorted(lvl_1_toc, key=lambda d: d['page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted_lvl_1_toc:\n",
    "    print(item.get('nameddest'), item.get('title'), item.get('page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_list = [\"Abstract\",\n",
    "                \"Introduction\", \"Background\", \"Introduction and Motivation\", \"Preliminary\", \n",
    "                \"Related Work\", \"Literature Review\", \"Related Research\",\n",
    "                \"Methods\", \"Methodology\", \"Method\", \"Approach\", \"Work Flow\", \"Materials and Methods\", \"Computation Function\", \"Problem Formulation\", \"Mathmatical Formulation\", \"Psedo Code\",\n",
    "                \"Experiment\", \"Experiment Settings\", \"Experimental Results\", \"Evaluation\", \"Experiments\",\n",
    "                \"Analysis\", \"Results\", \"Findings\", \"Data Analysis\", \"Results and Findings\",\n",
    "                \"Conclusion\", \"Discussion\", \"Results and Discussion\", \"Further Discussion\", \n",
    "                \"References\",\n",
    "                \"Acknowledgments\", \n",
    "                \"FAQ\", \"Frequently Asked Questions\",\n",
    "                \"Implementation Code\", \"Examples\", \"Appendix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdf_meta_det import extract_meta, dump_toml\n",
    "\n",
    "pattern = '|'.join(re.escape(section) for section in section_list)\n",
    "\n",
    "mtch_rslts = []\n",
    "for i in range(min(len(doc), 10)):\n",
    "    tmp_rslt = extract_meta(doc, pattern=pattern, page=i+1)\n",
    "    mtch_rslts.extend(tmp_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size, flags = 0, 0\n",
    "for item in mtch_rslts:\n",
    "    if item.get('size') > size:\n",
    "        size = item.get('size')\n",
    "    if item.get('flags') > flags:\n",
    "        flags = item.get('flags')\n",
    "print(size, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvsd_mtch_rslts = [item for item in mtch_rslts if item.get('size') == size and item.get('flags') == flags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_meta_det import extract_meta, dump_toml\n",
    "\n",
    "auto_level = 1\n",
    "addnl = False\n",
    "tmp_meta_ptrn = [dump_toml(m, auto_level, addnl) for m in rvsd_mtch_rslts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 tmp_meta_ptrn 写入 recipe.toml 文件\n",
    "with open('recipe.toml', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(tmp_meta_ptrn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from pdf_toc_gen import get_file_encoding, gen_toc\n",
    "\n",
    "recipe_file_path = 'recipe.toml'\n",
    "recipe_file = open(recipe_file_path, \"r\", encoding=get_file_encoding(recipe_file_path))\n",
    "recipe = toml.load(recipe_file)\n",
    "toc = gen_toc(doc, recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Paragraph Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_by_paragraph(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, splitting it into paragraphs using PyMuPDF.\n",
    "    Also provides page number and bounding box for each paragraph.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a paragraph\n",
    "              and contains the 'text', 'page', and 'pos' (position) keys.\n",
    "        Returns None if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        paragraphs = []\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for b in blocks:\n",
    "                if b['type'] == 0:\n",
    "                    block_text = \"\"\n",
    "                    block_rects = []  # Collect rectangles for the entire block\n",
    "\n",
    "                    for l in b[\"lines\"]:\n",
    "                        for s in l[\"spans\"]:\n",
    "                            block_text += s[\"text\"]\n",
    "                            block_rects.append(fitz.Rect(s[\"bbox\"]))\n",
    "                    \n",
    "                    # Combine the rects to get the overall block rect\n",
    "                    if block_rects:\n",
    "                        block_rect = block_rects[0]\n",
    "                        for rect in block_rects[1:]:\n",
    "                            block_rect |= rect  # Union of rectangles\n",
    "\n",
    "                    block_paragraphs = block_text.strip().split('\\n\\n') # You can further improve this with regex if needed\n",
    "\n",
    "                    for p in block_paragraphs:\n",
    "                      if p.strip():\n",
    "                        paragraphs.append({\n",
    "                            'text': p.strip(),\n",
    "                            'page': page_num + 1,  # Page numbers start from 1\n",
    "                            'pos': block_rect\n",
    "                        })\n",
    "\n",
    "        return paragraphs\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{pdf_path}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "pdf_file = \"../data/2502.00330v1.pdf\"  # Replace with your PDF file path\n",
    "paragraphs = extract_text_by_paragraph(pdf_file)\n",
    "\n",
    "if paragraphs:\n",
    "    for paragraph in paragraphs:\n",
    "        print(f\"Page: {paragraph['page']}\")\n",
    "        print(f\"Position: {paragraph['pos']}\")\n",
    "        print(f\"Text:\\n{paragraph['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(\"../data/2502.00330v1.pdf\")\n",
    "print(result.text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试minerU API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mineru_api_key = \"eyJ0eXBlIjoiSldUIiwiYWxnIjoiSFM1MTIifQ.eyJqdGkiOiI4NTUwNzEwNiIsInJvbCI6IlJPTEVfUkVHSVNURVIiLCJpc3MiOiJPcGVuWExhYiIsImlhdCI6MTczODgwNTU1NSwiY2xpZW50SWQiOiJsa3pkeDU3bnZ5MjJqa3BxOXgydyIsInBob25lIjoiIiwidXVpZCI6IjFjOWE0NjE5LWMxNWItNDkxNi04MjQ4LWY4YjQ1MjJiZTZiYyIsImVtYWlsIjoiIiwiZXhwIjoxNzQwMDE1MTU1fQ.SCAEEIbeeTXheBOqa78koRcgS0uw0IXRFt9kLq3eA0zBfS0Qeml7vy-VXlg1Hh9dwm9WnLc-GDKZXwys1tGJKg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response success. result:{'code': 0, 'msg': 'ok', 'trace_id': '386768fd68e9d6cff859773ad73ad5c6', 'data': {'batch_id': '79e3e16a-8a1b-4911-9722-ada284c6d7cf', 'file_urls': ['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/79e3e16a-8a1b-4911-9722-ada284c6d7cf/b9917862-36ee-48b1-aabd-7df65948fb91.pdf?Expires=1738919408&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=qIincgRD%2BPs8spX0EXsPKTHoOf4%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/79e3e16a-8a1b-4911-9722-ada284c6d7cf/de74e957-707c-4e48-b282-7540327a802d.pdf?Expires=1738919408&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=nIzYUVhCEcQMPtHpBgecevHLqjo%3D']}}\n",
      "batch_id:79e3e16a-8a1b-4911-9722-ada284c6d7cf,urls:['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/79e3e16a-8a1b-4911-9722-ada284c6d7cf/b9917862-36ee-48b1-aabd-7df65948fb91.pdf?Expires=1738919408&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=qIincgRD%2BPs8spX0EXsPKTHoOf4%3D', 'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/79e3e16a-8a1b-4911-9722-ada284c6d7cf/de74e957-707c-4e48-b282-7540327a802d.pdf?Expires=1738919408&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=nIzYUVhCEcQMPtHpBgecevHLqjo%3D']\n",
      "upload success\n",
      "upload success\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url='https://mineru.net/api/v4/file-urls/batch'\n",
    "header = {\n",
    "    'Content-Type':'application/json',\n",
    "    \"Authorization\":f\"Bearer {mineru_api_key}\"\n",
    "}\n",
    "data = {\n",
    "    \"enable_formula\": True,\n",
    "    \"language\": \"en\",\n",
    "    \"layout_model\":\"doclayout_yolo\",\n",
    "    \"enable_table\": True,\n",
    "    \"files\": [\n",
    "        {\"name\":\"2502.00330v1.pdf\", \"is_ocr\": False, \"data_id\": \"test-20250206-002\"},\n",
    "        {\"name\":\"2502.02508v1.pdf\", \"is_ocr\": False, \"data_id\": \"test-20250206-003\"}\n",
    "    ]\n",
    "}\n",
    "file_pathes = [r\"../data/2502.00330v1.pdf\", r\"../data/2502.02508v1.pdf\"]\n",
    "try:\n",
    "    response = requests.post(url,headers=header,json=data)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print('response success. result:{}'.format(result))\n",
    "        if result[\"code\"] == 0:\n",
    "            batch_id = result[\"data\"][\"batch_id\"]\n",
    "            urls = result[\"data\"][\"file_urls\"]\n",
    "            print('batch_id:{},urls:{}'.format(batch_id, urls))\n",
    "            for idx, file_path in enumerate(file_pathes):\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    res_upload = requests.put(urls[idx], data=f)\n",
    "                if res_upload.status_code == 200:\n",
    "                    print(\"upload success\")\n",
    "                else:\n",
    "                    print(\"upload failed\")\n",
    "        else:\n",
    "            print('apply upload url failed,reason:{}'.format(result.msg))\n",
    "    else:\n",
    "        print('response not success. status:{} ,result:{}'.format(response.status_code, response))\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 0,\n",
       " 'msg': 'ok',\n",
       " 'trace_id': '386768fd68e9d6cff859773ad73ad5c6',\n",
       " 'data': {'batch_id': '79e3e16a-8a1b-4911-9722-ada284c6d7cf',\n",
       "  'file_urls': ['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/79e3e16a-8a1b-4911-9722-ada284c6d7cf/b9917862-36ee-48b1-aabd-7df65948fb91.pdf?Expires=1738919408&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=qIincgRD%2BPs8spX0EXsPKTHoOf4%3D',\n",
       "   'https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/79e3e16a-8a1b-4911-9722-ada284c6d7cf/de74e957-707c-4e48-b282-7540327a802d.pdf?Expires=1738919408&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=nIzYUVhCEcQMPtHpBgecevHLqjo%3D']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'code': 0, 'msg': 'ok', 'trace_id': '15963df2935fb9f13304412207a95e94', 'data': {'batch_id': '79e3e16a-8a1b-4911-9722-ada284c6d7cf', 'extract_result': [{'data_id': 'test-20250206-002', 'file_name': '2502.00330v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/039a6a8b-5f27-4f88-a2df-3988a66e6af9.zip'}, {'data_id': 'test-20250206-003', 'file_name': '2502.02508v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/2af9928d-81a2-4d15-a166-d0e8144c0ca9.zip'}]}}\n",
      "{'batch_id': '79e3e16a-8a1b-4911-9722-ada284c6d7cf', 'extract_result': [{'data_id': 'test-20250206-002', 'file_name': '2502.00330v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/039a6a8b-5f27-4f88-a2df-3988a66e6af9.zip'}, {'data_id': 'test-20250206-003', 'file_name': '2502.02508v1.pdf', 'state': 'done', 'err_msg': '', 'full_zip_url': 'https://cdn-mineru.openxlab.org.cn/pdf/2af9928d-81a2-4d15-a166-d0e8144c0ca9.zip'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "batch_id = \"79e3e16a-8a1b-4911-9722-ada284c6d7cf\"\n",
    "url = f'https://mineru.net/api/v4/extract-results/batch/{batch_id}'\n",
    "header = {\n",
    "    'Content-Type':'application/json',\n",
    "    \"Authorization\":f\"Bearer {mineru_api_key}\"\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=header)\n",
    "print(res.status_code)\n",
    "print(res.json())\n",
    "print(res.json()[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先按page切，暂不考虑acknowledgement, reference及以后的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_section_list = ['References', \"Acknowledgments\", \"Appendix\", \"FAQ\", \"Frequently Asked Questions\"]\n",
    "\n",
    "import re\n",
    "sec_ptrn = '|'.join(re.escape(section) for section in append_section_list)\n",
    "\n",
    "mtch_rslts = []\n",
    "page = len(doc)\n",
    "for item in toc:\n",
    "    if re.match(sec_ptrn, item.title):\n",
    "        if item.pagenum < page:\n",
    "            page = item.pagenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def save_pdf_pages(input_pdf_path, output_pdf_path, page_numbers):\n",
    "    # 打开PDF文件\n",
    "    pdf_document = fitz.open(input_pdf_path)\n",
    "    \n",
    "    # 创建一个新的PDF文档\n",
    "    output_pdf = fitz.open()\n",
    "    \n",
    "    # 添加指定的页面到新的PDF文档\n",
    "    for page_number in page_numbers:\n",
    "        # 将页面添加到新的PDF文档中\n",
    "        output_pdf.insert_pdf(pdf_document, from_page=page_number, to_page=page_number)\n",
    "    \n",
    "    # 保存新的PDF文件\n",
    "    output_pdf.save(output_pdf_path)\n",
    "    output_pdf.close()\n",
    "    pdf_document.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存前n页\n",
    "in_pdf_path = \"/Users/jiezi/Documents/Local Code/Project/PaperPal/dev/tmp/2201.11903v6.pdf\"\n",
    "out_pdf_path = 'tmp.pdf'  # 输出PDF文件路径\n",
    "# save_pdf_pages(in_pdf_path, out_pdf_path, list(range(0, page)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pdf_layout_det import PDF2MARKDOWN\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/jiezi/Packages/PDF-Extract-Kit\")\n",
    "\n",
    "from pdf_extract_kit.utils.config_loader import load_config, initialize_tasks_and_models\n",
    "\n",
    "\n",
    "TASK_NAME = 'pdf2markdown'\n",
    "config_path = \"/home/jiezi/Packages/PDF-Extract-Kit/project/pdf2markdown/configs/pdf2markdown.yaml\"\n",
    "config = load_config(config_path)\n",
    "task_instances = initialize_tasks_and_models(config)\n",
    "\n",
    "# get input and output path from config\n",
    "input_data = out_pdf_path\n",
    "result_path = \"./opt\"\n",
    "\n",
    "layout_model = task_instances['layout_detection'].model if 'layout_detection' in task_instances else None\n",
    "mfd_model = task_instances['formula_detection'].model if 'formula_detection' in task_instances else None\n",
    "mfr_model = None\n",
    "# mfr_model = task_instances['formula_recognition'].model if 'formula_recognition' in task_instances else None\n",
    "ocr_model = None\n",
    "# ocr_model = task_instances['ocr'].model if 'ocr' in task_instances else None\n",
    "\n",
    "pdf2md = PDF2MARKDOWN(layout_model, mfd_model, mfr_model, ocr_model)\n",
    "res_list, final_blocks, md_content = pdf2md.process(input_path=input_data, save_dir=result_path, visualize=True, merge2markdown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = \"\".join(md_content).split(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paras = [item for item in paras if item is not None and item != '' and len(item) >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras_dct = []\n",
    "for idx, item in enumerate(filtered_paras):\n",
    "    paras_dct.append({'id':idx, 'lines':item[0:300]+\"...\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(paras_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_paras_dct = []\n",
    "for idx, item in enumerate(filtered_paras):\n",
    "    tmp_paras_dct.append({'para_id':idx, 'content':item[0:50]+\"...\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(tmp_paras_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_prompt = \"\"\"## INSTRUCTION\n",
    "已知table_of_content记录了章节标题和对应的页面，para中抽取了各个章节的起始句子。\n",
    "对于table_of_content中的每一项，根据section_title和paras中content的内容进行匹配，并将全部匹配到的para_id添加到table_of_content中。\n",
    "注意以下两种情况均构成匹配：\n",
    "- content直接对应section_title；\n",
    "- content是section_title的二级目录下的内容。\n",
    "如无匹配的项，则将置空。\n",
    "\n",
    "## INPUT\n",
    "<toc>\n",
    "{toc}\n",
    "</toc>\n",
    "\n",
    "<paras>\n",
    "{paras}\n",
    "</paras>\n",
    "\n",
    "## OUTPUT\n",
    "Output in json with double quotes in the following format:\n",
    "```json\n",
    "[{{'section_title':xxx, 'page_num':xxx, 'vpos':xxx, 'para_ids':[list of all matched para_id, blank if no match]}}\n",
    ", ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "toc_lst = []\n",
    "for item in toc:\n",
    "    toc_lst.append({'section_title':item.title, 'page_num':item.pagenum, 'vpos':item.vpos})\n",
    "prompt = match_prompt.format(toc=str(toc_lst),paras=str(tmp_paras_dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "def zhipu_llm(sys_prompt, qa_promt):\n",
    "    if not sys_prompt:\n",
    "        sys_prompt = \"You are a helpful assistant.\"\n",
    "    \n",
    "    \n",
    "    client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY_1\")) # 填写您自己的APIKey\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4-flash\",  # 填写需要调用的模型编码\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": qa_promt}\n",
    "        ],\n",
    "    )\n",
    "    opt_result = response.choices[0].message.content\n",
    "    return opt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def convert_quotes(json_str):\n",
    "    # 将单引号替换为双引号，但是需要排除字符串内的单引号\n",
    "    json_str = re.sub(r\"(?<!\\\\)'(.*?)(?<!\\\\)'\", r'\"\\1\"', json_str)\n",
    "    return json_str\n",
    "\n",
    "def get_json(json_str):\n",
    "    # 正则表达式，匹配以 ```json 开头，后面可能跟着换行符，然后是JSON内容，直到 ``` 结尾\n",
    "    pattern = r\"```json\\n?(.*?)\\n?```\"\n",
    "\n",
    "    # 使用正则表达式找到匹配的JSON字符串\n",
    "    matches = re.findall(pattern, json_str, re.DOTALL)\n",
    "\n",
    "    json_data = None\n",
    "    # 如果找到匹配项，尝试将其转换为JSON对象\n",
    "    if matches:\n",
    "        json_str = matches[0].strip()  # 移除字符串前后的空白字符\n",
    "        json_str = convert_quotes(json_str)  # 转换单引号为双引号\n",
    "        try:\n",
    "            json_data = json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No JSON content found.\")\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_rslt = zhipu_llm(sys_prompt=None, qa_promt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_json = get_json(outline_rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download source data or use html to double confirm pdf data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[\"1605.08386v1\"])))\n",
    "# Download the archive to the PWD with a default filename.\n",
    "paper.download_source()\n",
    "# Download the archive to the PWD with a custom filename.\n",
    "paper.download_source(filename=\"downloaded-paper.tar.gz\")\n",
    "# Download the archive to a specified directory with a custom filename.\n",
    "paper.download_source(dirpath=\"./mydir\", filename=\"downloaded-paper.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use html for information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "arxiv_id = \"2410.24175\"\n",
    "url = f\"https://arxiv.org/html/{arxiv_id}\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the links on the page\n",
    "figures = []\n",
    "tables = []\n",
    "\n",
    "figure_images = soup.select('.ltx_figure > img')\n",
    "figure_captions = soup.select('.ltx_figure > figcaption') \n",
    "for figure_image, figure_caption in zip(figure_images, figure_captions):\n",
    "    figure = {\n",
    "        'figure_path': f\"https://arxiv.org/html/{arxiv_id}/{figure_image.get('src')}\",\n",
    "        'figure_caption': figure_caption.text.strip()\n",
    "    }\n",
    "    figures.append(figure)\n",
    "\n",
    "\n",
    "table_contents = soup.select('table.ltx_tabular')\n",
    "table_captions = soup.select('.ltx_table > figcaption')\n",
    "for table_content, table_caption in zip(table_contents, table_captions):\n",
    "    table = {\n",
    "        'table_content': str(table_content),\n",
    "        'table_caption': table_caption.text.strip()\n",
    "    }\n",
    "    tables.append(table)\n",
    "\n",
    "with open('figures.json', 'w') as f:\n",
    "    json.dump(figures, f)\n",
    "\n",
    "with open('tables.json', 'w') as f:\n",
    "    json.dump(tables, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案一：直接使用LLM针对特定章节问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要补充：\n",
    "- 长度控制模块\n",
    "- 段落切分，按段落问答并总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_identify_prompt = \"\"\"## TASK\n",
    "You are an academic researcher in Computer Science and AI field. \n",
    "You are given section title together with initial lines of paragraphs from a paper.\n",
    "Now you are asked to identify the section type. The section type can be one of the following: \n",
    "['Bio', 'Abstraction', 'Introduction',  'Related Works and Literature Review', 'Methodology', 'Experiment and Results', 'Discussion and Conclusion', 'Others']\n",
    "Please identify the section type based on the given section.\n",
    "\n",
    "## PARA\n",
    "{content}\n",
    "\n",
    "## OUTPUT\n",
    "Output in json with double quotes in the following format:\n",
    "```json\n",
    "[{{'id':0, 'sectoin_type':xxx}}, {{'id':1, 'sectoin_type':xxx}}, ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "prompt = section_identify_prompt.format(content=str(paras_dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY_1\")) # 填写您自己的APIKey\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4-flash\",  # 填写需要调用的模型编码\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant whose task is to provide users with professional, accurate, and insightful advice.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    ")\n",
    "opt_result = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_json = get_json(opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract, introduction, method, conclusion = \"\", \"\", \"\", \"\"\n",
    "for idx, item in enumerate(opt_json):\n",
    "    if item['section_type'] == 'Abstraction':\n",
    "        abstract += filtered_paras[idx]\n",
    "    if item['section_type'] == 'Introduction':\n",
    "        introduction += '\\n\\n\\n' + filtered_paras[idx]   \n",
    "    if item['section_type'] == 'Methodology':\n",
    "        method += '\\n\\n\\n' + filtered_paras[idx]\n",
    "    if item['section_type'] == 'Discussion and Conclusion':\n",
    "        conclusion += '\\n\\n\\n' + filtered_paras[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cur_api += 1\n",
    "self.cur_api = 0 if self.cur_api >= len(self.chat_api_list) - 1 else self.cur_api\n",
    "text_token = len(self.encoding.encode(text))\n",
    "clip_text_index = int(len(text) * (self.max_token_num - method_prompt_token) / text_token)\n",
    "clip_text = text[:clip_text_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"You are a researcher in the field of '{subject}' who is good at summarizing papers using concise statements.\"\n",
    "\n",
    "summary_prompt = \"\"\" ## INSTRUCTION\n",
    "Given abstraction and introduction paragraph from the paper, you are asked to:                   \n",
    "1. identify the keywords of this article;\n",
    "2. summarize according to the following four points\n",
    "- (1): What is the research background of this article? What problem is this paper trying to solve? \n",
    "- (2): What are the relevant studies? What are the past methods? What are the issues with them? Is the approach well motivated?\n",
    "- (3): How does the paper solve this problem? What is the research methodology proposed in this paper?\n",
    "- (4): What experiments were done in the paper? On what task and what performance is achieved by the methods in this paper? Can the performance support their goals?\n",
    "- (5): Are there unsolved issues with the paper? What gaps can be explored further? Any suggestions?\n",
    "\n",
    "## CONTEXT\n",
    "Here are abstraction from the paper:\n",
    "<abstraction>\n",
    "{abstraction}\n",
    "</abstraction>\n",
    "\n",
    "Here are introduction from the paper:\n",
    "<introduction>\n",
    "{introduction}\n",
    "</introduction>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output that follows: \n",
    "```text                            \n",
    "1. Keywords: xxx\\n\\n     \n",
    "2. Summary: \\n\\n\n",
    "- (1):xxx;\\n \n",
    "- (2):xxx;\\n \n",
    "- (3):xxx;\\n  \n",
    "- (4):xxx.\\n\\n     \n",
    "- (5):xxx.\\n\\n  \n",
    "```\n",
    "\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not have too much repetitive information, numerical values using the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed.                 \n",
    "\"\"\"\n",
    "\n",
    "method_prompt = \"\"\"## INSTRUCTION\n",
    "Given method paragraph and a summary of a paper, you are asked to describe in detail the methodological idea of this article. \n",
    "- (1):...\n",
    "- (2):...\n",
    "- (3):...\n",
    "- .......\n",
    "\n",
    "## CONTEXT\n",
    "Here are method paragraph:\n",
    "<method>\n",
    "{method}\n",
    "</method>\n",
    "\n",
    "Here are summary of the paper fyi:\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output that follows: \n",
    "```text\n",
    "3. Methods: \\n\\n\n",
    "- (1):xxx;\\n \n",
    "- (2):xxx;\\n \n",
    "- (3):xxx;\\n  \n",
    "....... \\n\\n     \n",
    "```\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not repeat the content of the previous <summary>, the value of the use of the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed, ....... means fill in according to the actual requirements.                 \n",
    "\"\"\"\n",
    " \n",
    "conclusion_prompt = \"\"\"## INSTRUCTION\n",
    "Given conclusion paragraph and a summary of a paper, you are asked to: \n",
    "4. Make the following summary:\n",
    "- (1):What is the significance of this piece of work?\n",
    "- (2):Summarize the strengths and weaknesses of this article in three dimensions: innovation point, performance, and workload.                   \n",
    ".......\n",
    "\n",
    "    \"contribution\": \"What is the contribution of this paper?\",\n",
    "    \"novelty\": \"What is the novelty of this paper?\",\n",
    "    \"strength\": \"What are the strengths of this paper?\",\n",
    "    \"drawback\": \"What are the drawbacks of this paper?\",\n",
    "    \"improvement\": \"What might be the improvements of this paper?\",\n",
    "\n",
    "\n",
    "## CONTEXT\n",
    "Here are conclusion paragraph:\n",
    "<conclusion>\n",
    "{conclusion}\n",
    "</conclusion>\n",
    "\n",
    "Here are summary of the paper fyi:\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output later: \n",
    "```text\n",
    "4. Conclusion: \\n\\n\n",
    "- (1):xxx;\\n                     \n",
    "- (2):Innovation point: xxx; Performance: xxx; Workload: xxx;\\n    \n",
    "- (3):\n",
    "    contribution: What is the contribution of this paper?,\n",
    "    novelty: What is the novelty of this paper?,\n",
    "    strength\": What are the strengths of this paper?,\n",
    "    drawback: What are the drawbacks of this paper?,\n",
    "    improvement\": What might be the improvements of this paper?\n",
    "```\n",
    "\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not repeat the content of the previous <summary>, the value of the use of the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed, ....... means fill in according to the actual requirements.                 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"English\"\n",
    "sum_prompt = summary_prompt.format(abstraction=abs, introduction=intro, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result = zhipu_llm(sys_prompt, sum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"English\"\n",
    "dis_prompt = method_prompt.format(conclusion=dis, summary=opt_result, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_2 = zhipu_llm(sys_prompt, dis_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"English\"\n",
    "met_prompt = method_prompt.format(method=method, summary=opt_result, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(met_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_3 = zhipu_llm(sys_prompt, met_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_result_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二：使用传统RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to-do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案三：使用GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定位图片、表格或公式的详细位置\n",
    "- 'figure', 'figure_caption',\n",
    "- 'table', 'table_caption', 'table_footnote',\n",
    "- 'formula', 'formula_caption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取图片、表格或公式的详细位置\n",
    "def get_bounding_box(poly):\n",
    "    x_coords = poly[0::2]\n",
    "    y_coords = poly[1::2]\n",
    "    return min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
    "\n",
    "def do_boxes_overlap(box1, box2, max_distance=20):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    horizontal_overlap = not (x1_max < x2_min or x1_min > x2_max)\n",
    "    vertical_overlap_or_close = not (y1_max < y2_min - max_distance or y1_min > y2_max + max_distance)\n",
    "\n",
    "    return horizontal_overlap or vertical_overlap_or_close\n",
    "\n",
    "def consolidate_positions(items):\n",
    "    if not items:\n",
    "        return None\n",
    "    x_min = min(get_bounding_box(item['poly'])[0] for item in items)\n",
    "    y_min = min(get_bounding_box(item['poly'])[1] for item in items)\n",
    "    x_max = max(get_bounding_box(item['poly'])[2] for item in items)\n",
    "    y_max = max(get_bounding_box(item['poly'])[3] for item in items)\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "def find_matches(metadata, category_types):\n",
    "    filtered_items = [item for item in metadata if item['category_type'] in category_types]\n",
    "    results = []\n",
    "\n",
    "    while filtered_items:\n",
    "        base_item = filtered_items.pop(0)\n",
    "        base_box = get_bounding_box(base_item['poly'])\n",
    "        group = [base_item]\n",
    "\n",
    "        for other_item in list(filtered_items):  # Use list to avoid modifying during iteration\n",
    "            other_box = get_bounding_box(other_item['poly'])\n",
    "            if do_boxes_overlap(base_box, other_box):\n",
    "                group.append(other_item)\n",
    "                filtered_items.remove(other_item)\n",
    "\n",
    "        consolidated_box = consolidate_positions(group)\n",
    "        concatenated_text = ' '.join(item.get('text', '') for item in group)\n",
    "        results.append({\n",
    "            'output_category': ' & '.join(item['category_type'] for item in group),\n",
    "            'output_poly': consolidated_box,\n",
    "            'output_text': concatenated_text\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将图片、表格或公式保存为图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DPI = 144\n",
    "# since there is a manipulation of image size, we need to map the image coordinates back to the pdf coordinates\n",
    "def map_image_to_pdf(image_x, image_y, pix, dpi=DEFAULT_DPI):\n",
    "    if pix.width <= 3000 and pix.height <= 3000:\n",
    "        scale = dpi / 72\n",
    "        pdf_x = image_x / scale\n",
    "        pdf_y = image_y / scale\n",
    "    else:\n",
    "        pdf_x = image_x\n",
    "        pdf_y = image_y\n",
    "    return pdf_x, pdf_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_types = ['figure', 'figure_caption']\n",
    "results = find_matches(final_blocks[5], category_types)\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还原页面\n",
    "idx = 5\n",
    "page = doc.load_page(idx)\n",
    "pix = page.get_pixmap(matrix=fitz.Matrix(DEFAULT_DPI/72, DEFAULT_DPI/72))\n",
    "area = result['output_poly']\n",
    "x0, y0 = map_image_to_pdf(area[0], area[1], pix)\n",
    "x1, y1 = map_image_to_pdf(area[2], area[3], pix)\n",
    "\n",
    "pix_map = page.get_pixmap(clip=fitz.Rect(x0, y0, x1, y1))\n",
    "pix_map.save(\"output_new.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取对应的段落信息，作为上下文辅助\n",
    "- 思路一：从来源追溯，找寻最契合\n",
    "- 思路二：基于向量匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找对应的章节\n",
    "def find_titles_for_page(toc, page_idx):\n",
    "    titles = []\n",
    "    for i, entry in enumerate(toc):\n",
    "        # 对于最后一个条目，由于没有下一个条目，所以单独处理\n",
    "        if i == len(toc) - 1:\n",
    "            if entry.pagenum <= page_idx:\n",
    "                titles.append(entry.title)\n",
    "        else:\n",
    "            # 对于其他条目，确保页面索引在当前条目和下一个条目之间\n",
    "            if entry.pagenum <= page_idx < toc[i + 1].pagenum:\n",
    "                titles.append(entry.title)\n",
    "            # 如果当前条目和下一个条目的页码相同，则添加当前条目的标题\n",
    "            elif entry.pagenum == page_idx == toc[i + 1].pagenum:\n",
    "                titles.append(entry.title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 源头追溯\n",
    "idx = 5\n",
    "section_titles = find_titles_for_page(toc, idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模糊匹配\n",
    "import difflib\n",
    "\n",
    "def fuzzy_match(short_texts, long_texts):\n",
    "    matches = []\n",
    "    for i, short_text in enumerate(short_texts):\n",
    "        # 使用difflib.get_close_matches获取所有可能的匹配项\n",
    "        close_matches = difflib.get_close_matches(short_text, long_texts, n=len(long_texts), cutoff=0.0)\n",
    "        # 如果有匹配项，选择相似度最高的一个\n",
    "        if close_matches:\n",
    "            # 按相似度排序，取第一个元素（相似度最高）\n",
    "            best_match = max(close_matches, key=lambda x: difflib.SequenceMatcher(None, short_text, x).ratio())\n",
    "            # 获取长文本在列表中的位置\n",
    "            best_match_index = long_texts.index(best_match)\n",
    "            # 将匹配的索引对添加到列表中\n",
    "            matches.append((i, best_match_index))\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rslts = fuzzy_match(section_titles, [item[:50] for item in filtered_paras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conten = filtered_paras[test_rslts[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_extraction_prompt = \"\"\"## TASK\n",
    "Extract key information from context that is relevant to the clues.\n",
    "\n",
    "## CLUES\n",
    "{intro_of_figure_table_formula}\n",
    "\n",
    "## CONTEXT\n",
    "{context}\n",
    "\n",
    "## OUTPUT\n",
    "Related information are: \\n\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = points_extraction_prompt.format(\n",
    "    intro_of_figure_table_formula=result['output_text'],\n",
    "    context=conten)\n",
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = zhipu_llm(None, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于向量匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size, overlap_size):\n",
    "    # 确保重叠大小不超过chunk大小\n",
    "    overlap_size = min(overlap_size, chunk_size)\n",
    "    \n",
    "    # 使用正则表达式分割文本，保持句子的完整性\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 如果当前chunk加上下一个句子小于chunk_size，则加入当前chunk\n",
    "        if len(current_chunk) + len(sentence) < chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            # 如果加上下一个句子超过chunk_size，则先保存当前chunk\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # 计算重叠部分\n",
    "            overlap = \" \" + \" \".join(sentences[sentences.index(sentence)-1].split()[-overlap_size:])\n",
    "            # 开始新的chunk，包含重叠部分\n",
    "            current_chunk = overlap + sentence + \" \"\n",
    "    \n",
    "    # 添加最后一个chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多模态语义理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "def zhipu_vllm(img_path, prompt):\n",
    "    with open(img_path, 'rb') as img_file:\n",
    "        img_base = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "    client = ZhipuAI(api_key=os.getenv(\"ZHIPU_API_KEY_1\")) # 填写您自己的APIKey\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4v\",  # \"glm-4v-plus\",  # 填写需要调用的模型名称\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": img_base\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }\n",
    "            ]\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "    return (response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"## TASK\n",
    "You are an academic scholar analyzing a image from a paper. \n",
    "Extract key information from the image that is relevant to the context.\n",
    "Try to answer the following questions: \n",
    "1. What is the image showing?\n",
    "2. What is the image related to?\n",
    "3. What is the image trying to convey?\n",
    "Be very concise and explicit in your answers. Try to show concrete results and numbers.\n",
    "\n",
    "## CONTEXT\n",
    "Here is background information of the paper for your guidance:\n",
    "<background>\n",
    "{background}\n",
    "</background>\n",
    "\n",
    "Here is short description of the image:\n",
    "<description>\n",
    "{description}\n",
    "</description>\n",
    "\n",
    "## OUTPUT\n",
    "The image reveals that: \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'output_new.png'\n",
    "prompt = prompt.format(\n",
    "    background=test_result,\n",
    "    description=result['output_text'])\n",
    "tmp_result = zhipu_vllm(img_path, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "for idx, item in enumerate(final_blocks):\n",
    "    full_text = \n",
    "    category_types = ['figure', 'figure_caption']\n",
    "    results = find_matches(final_blocks[5], category_types)\n",
    "\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['output_poly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
