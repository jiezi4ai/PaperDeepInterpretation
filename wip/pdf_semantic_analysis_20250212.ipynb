{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Training Large Language Models to Reason in a Continuous Latent Space\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前脚本所在目录的父目录 (即 my_project)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# 将父目录添加到 sys.path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.arxiv_tool import ArxivKit\n",
    "from apis.semanticscholar_tool import SemanticScholarKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:31:37,768 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Training+Large+Language+Models+to+Reason+in+a+Continuous+Latent+Space&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-13 09:31:42,240 - INFO - Got first page: 100 of 2656582 total results\n"
     ]
    }
   ],
   "source": [
    "arxiv = ArxivKit()\n",
    "\n",
    "# arxiv_metadata = []\n",
    "# for title in titles:\n",
    "#     candit_arxiv_metadata = arxiv.retrieve_metadata_by_paper(query_term=title, max_cnt=3)\n",
    "#     arxiv_metadata.append(candit_arxiv_metadata)\n",
    "#     time.sleep(5)\n",
    "\n",
    "arxiv_metadata = arxiv.retrieve_metadata_by_paper(query_term=title, max_cnt=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:31:43,715 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Training%20Large%20Language%20Models%20to%20Reason%20in%20a%20Continuous%20Latent%20Space&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "ss = SemanticScholarKit()\n",
    "\n",
    "# ss_metadata = []\n",
    "# for title in titles:\n",
    "#     candit_ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)\n",
    "#     ss_metadata.append(candit_ss_metadata)\n",
    "#     time.sleep(5)\n",
    "ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673fbdd957cada770d10dffca5e45b53da43a3c6\n"
     ]
    }
   ],
   "source": [
    "# paper_ss_id = ss_metadata[0][0].get('paperId')\n",
    "paper_ss_id = ss_metadata[0].get('paperId')\n",
    "print(paper_ss_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:31:48,942 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_metadata = ss.get_semanticscholar_references(paper_id=paper_ss_id, limit=100)\n",
    "len(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:31:52,430 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/citations?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citedby_metadata = ss.get_semanticscholar_citedby(paper_id=paper_ss_id, limit=100)\n",
    "len(citedby_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "processed_file_path = \"pdf_processed_wip_20250212.json\"\n",
    "\n",
    "with open(processed_file_path, 'r') as file:\n",
    "    processed_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def llm_gen(api_key, model_name, qa_prompt, sys_prompt=None, temperature=0.3):\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=sys_prompt,\n",
    "        temperature=temperature)\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name, \n",
    "        contents=qa_prompt,\n",
    "        config=config)\n",
    "    return response.text\n",
    "\n",
    "def llm_image_gen(api_key, model_name, qa_prompt, pil_images, sys_prompt=None, temperature=0.3):\n",
    "    \"\"\"q&a with images\n",
    "    Args:\n",
    "        pil_images:\n",
    "            import PIL.Image\n",
    "            image = PIL.Image.open('/path/to/image.png')\n",
    "    \"\"\"\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=sys_prompt,\n",
    "        temperature=temperature)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,  #　\"gemini-2.0-flash-exp\",\n",
    "        contents=[qa_prompt]+pil_images,\n",
    "        config=config)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def llm_gen_w_retry(api_key, model_name, qa_prompt, sys_prompt=None, temperature=0.3, max_retries=3, initial_delay=1):\n",
    "    \"\"\"\n",
    "    Wraps the llm_gen_w_images function to enable retries on RESOURCE_EXHAUSTED errors.\n",
    "\n",
    "    Args:\n",
    "        api_key: API key for the LLM service.\n",
    "        model_name: Name of the LLM model to use.\n",
    "        qa_prompt: Question and answer prompt for the LLM.\n",
    "        pil_images: List of PIL.Image objects.\n",
    "        temperature: Temperature for LLM response generation.\n",
    "        max_retries: Maximum number of retries in case of error.\n",
    "        initial_delay: Initial delay in seconds before the first retry.\n",
    "\n",
    "    Returns:\n",
    "        str: The text response from the LLM, or None if max retries are exceeded and still error.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm_gen(api_key, model_name, qa_prompt, sys_prompt, temperature)\n",
    "        except Exception as e:\n",
    "            if e.code == 429:\n",
    "                if retries < max_retries:\n",
    "                    retries += 1\n",
    "                    print(f\"Rate limit exceeded. Retrying in {delay} seconds (Retry {retries}/{max_retries})...\")\n",
    "                    time.sleep(delay)\n",
    "                    delay *= 2  # Exponential backoff for delay\n",
    "                else:\n",
    "                    print(f\"Max retries reached.  Raising the last exception.\")\n",
    "                    return None # raise  # Re-raise the last exception if max retries are exhausted\n",
    "            else:\n",
    "                print(f\"Error Code: {e.code} Error Message: {e.message}\")\n",
    "                return None\n",
    "                # raise  # Re-raise other ClientErrors (not related to resource exhaustion)\n",
    "\n",
    "    return None # Should not reach here in normal cases as exception is re-raised or value is returned in try block\n",
    "\n",
    "def llm_image_gen_w_retry(api_key, model_name, qa_prompt, pil_images, sys_prompt=None, temperature=0.3, max_retries=3, initial_delay=1):\n",
    "    \"\"\"\n",
    "    Wraps the llm_gen_w_images function to enable retries on RESOURCE_EXHAUSTED errors.\n",
    "\n",
    "    Args:\n",
    "        api_key: API key for the LLM service.\n",
    "        model_name: Name of the LLM model to use.\n",
    "        qa_prompt: Question and answer prompt for the LLM.\n",
    "        pil_images: List of PIL.Image objects.\n",
    "        sys_prompt: Optional system prompt for the LLM.\n",
    "        temperature: Temperature for LLM response generation.\n",
    "        max_retries: Maximum number of retries in case of error.\n",
    "        initial_delay: Initial delay in seconds before the first retry.\n",
    "\n",
    "    Returns:\n",
    "        str: The text response from the LLM, or None if max retries are exceeded and still error.\n",
    "    \"\"\"\n",
    "    retries = 0\n",
    "    delay = initial_delay\n",
    "\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            return llm_image_gen(api_key, model_name, qa_prompt, pil_images, sys_prompt, temperature)\n",
    "        except Exception as e:\n",
    "            if e.code == 429:\n",
    "                if retries < max_retries:\n",
    "                    retries += 1\n",
    "                    print(f\"Rate limit exceeded. Retrying in {delay} seconds (Retry {retries}/{max_retries})...\")\n",
    "                    time.sleep(delay)\n",
    "                    delay *= 2  # Exponential backoff for delay\n",
    "                else:\n",
    "                    print(f\"Max retries reached.  Raising the last exception.\")\n",
    "                    return None # raise  # Re-raise the last exception if max retries are exhausted\n",
    "            else:\n",
    "                print(f\"Error Code: {e.code} Error Message: {e.message}\")\n",
    "                return None\n",
    "                # raise  # Re-raise other ClientErrors (not related to resource exhaustion)\n",
    "\n",
    "    return None # Should not reach here in normal cases as exception is re-raised or value is returned in try block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_example_json = {\n",
    "  \"topics\": [\n",
    "    {\n",
    "      \"topic\": \"Performance Advantages of Transformer Networks over RNNs in Machine Translation Tasks\",\n",
    "      \"description\": \"This topic broadly concerns the comparison of Transformer networks and Recurrent Neural Networks (RNNs) in the context of machine translation, focusing on the superior performance characteristics of Transformers.\",\n",
    "      \"summary\": \"The provided text focuses on the significant performance advantages of Transformer networks over traditional Recurrent Neural Network (RNN) based models in machine translation tasks. It argues, based on presented empirical evidence, that Transformers achieve higher BLEU scores, indicating better translation quality, across multiple language pairs and datasets.  The authors specifically attribute this superior performance to the self-attention mechanism within Transformers, which allows for more effective capture of long-range dependencies in the input text compared to the sequential processing inherent in RNNs. The text cites experimental results demonstrating faster training times for Transformers due to their parallelizable architecture, contrasting this with the inherent sequential bottleneck of RNNs.  While acknowledging the potential computational cost of Transformers for extremely long sequences, the authors downplay this limitation in the context of typical machine translation scenarios. They further support their claims by comparing Transformers to convolutional models, arguing for the greater suitability of attention mechanisms for natural language processing. The paper concludes that the shift from recurrent to attention-based models, exemplified by Transformers, represents a major advancement in the field of machine translation. The authors mention, but do not extensively analyze, the limitations imposed by dataset size on the Transformer performance.\",\n",
    "      \"line_ids\": [1, 2, 3]\n",
    "    },\n",
    "    {\n",
    "      \"topic\": \"Role of Multi-Headed Scaled Dot-Product Self-Attention in Enhancing Contextual Understanding within Transformer Networks\",\n",
    "      \"description\": \"This topic encompasses the specific type of self-attention (scaled dot-product) and its multi-headed variant used in Transformer networks, and how these mechanisms contribute to the model's ability to understand context within input sequences.\",\n",
    "      \"summary\": \"The provided paragraphs delve into the critical role of multi-headed scaled dot-product self-attention in enhancing contextual understanding within Transformer networks. It explains that self-attention allows each word in a sentence to attend to all other words, including itself, to derive a context-aware representation. The scaled dot-product mechanism is presented as a computationally efficient way to calculate attention weights, preventing issues that can arise with large dot products. The text emphasizes the significance of the 'multi-headed' aspect, where multiple self-attention operations are performed in parallel, each learning different aspects of the relationships between words.  This allows the model to capture diverse contextual nuances, such as syntactic and semantic dependencies, simultaneously. The authors argue that this multi-headed approach is crucial for capturing the richness of human language. They contrast this with simpler attention mechanisms, highlighting the ability of multi-headed attention to learn multiple 'representation subspaces'.  The text provides a brief mathematical overview of the scaled dot-product calculation, reinforcing its efficiency and effectiveness. The authors posit that without multi-headed attention, the Transformer's ability to model complex language structures would be significantly diminished. They conclude by highlighting the importance for future works, such as model interpretability and analysis.\",\n",
    "      \"line_ids\": [7, 8, 9, 10]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "topics_prompt = \"\"\"You are a sophisticated academic scholar with expertise in {domain}. \n",
    "You are renowned for your ability to grasp the key topics and ideas of research papers which are significant and insightful.\n",
    "\n",
    "## TASK\n",
    "You are provided with a section of lines extracted from an academic paper.\n",
    "Analyze the provided information and identify key academic topics discussed.  \n",
    "For each topic, generate a JSON object containing the following:\n",
    "\n",
    "*   `topic`: A precise and information-rich name for the topic. This should be as specific as possible, potentially combining multiple concepts to accurately reflect the nuanced discussion in the text.  (e.g., 'Application of Transformer Networks to Machine Translation', 'Impact of Multi-Headed Self-Attention on Long-Range Dependency Capture in Transformers').\n",
    "*   `description`: A concise, general definition of the topic (1-2 sentences). Imagine you are explaining it to a colleague *unfamiliar* with the specific paper, but familiar with AI/NLP in general.  Keep the definition broad enough to encompass the general concept, even if the topic name is very specific.\n",
    "*   `summary`: A detailed summary of the topic's treatment *within the provided text*. This should include:\n",
    "    *   The specific arguments made about the topic.\n",
    "    *   Any evidence or examples the authors use related to the topic.\n",
    "    *   The authors' conclusions or claims regarding the topic.\n",
    "    *   Any limitations or critiques of the topic presented by the authors.\n",
    "    *   Any comparisons to other related concepts or methods.\n",
    "*   `line_ids`: Categorize section lines based on its closeness to topics. Put only line ids here. Make sure the line ids exist in input and do not fake. Ideally each line only correspond to one topic. \n",
    "Output your entire response as a single, valid JSON object. The highest level should be a list called 'topics'.\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "Example (using a hypothetical excerpt about Transformer Networks):\n",
    "\n",
    "```json\n",
    "{example_json}\n",
    "```\n",
    "\n",
    "## INPUT\n",
    "Here are the section of lines for the paper in markdown format:\n",
    "```markdown\n",
    "{markdown_text}\n",
    "```\n",
    "\n",
    "{further_information}\n",
    "\n",
    "## OUTPUT\n",
    "Now get started!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:29:05,849 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:29:28,850 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds (Retry 1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:29:31,406 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:29:52,404 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:30:08,788 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds (Retry 1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:30:11,750 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:30:34,086 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:31:03,679 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:31:30,718 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:31:45,718 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:32:40,865 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:33:01,437 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 11:33:16,255 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds (Retry 1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:33:18,427 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import PIL.Image\n",
    "\n",
    "api_key = os.getenv('GEMINI_API_KEY_1')\n",
    "temperature = 0.7\n",
    "domain = \"Artificial Intelligence and LLMs\"\n",
    "tmp_path = \"/home/jiezi/Code/Temp/tmp/2412.06769v2\"\n",
    "\n",
    "responses = []\n",
    "for section in processed_json:\n",
    "    title = \"#\" * section.get('level') + \" \" + section.get('title')\n",
    "    md_text = section.get('refined_text')\n",
    "    md_lines = \"\\n\".join([f\"<line_id> {x.get('id')} <\\line_id>  <line_text> {x.get('line')} <\\line_text>\" for x in section.get('lines')])\n",
    "    input_text = title + \"\\n\" + md_lines\n",
    "    \n",
    "    images = section.get('images')\n",
    "    if title not in [\"References\", \"Acknowledgments\"] and len(md_text) > 200:\n",
    "        imgs_prompt = \"\"\n",
    "        pil_images = []\n",
    "        if len(images) > 0:\n",
    "            img_info = \"\"\n",
    "            for img in images:\n",
    "                img_title = img.get('title')\n",
    "                img_url = os.path.join(tmp_path, img.get('img_path'))\n",
    "                pil_images.append(PIL.Image.open(img_url))\n",
    "                img_info += f\"- image title: {img_title}  attached image: {os.path.basename(img_url)} \\n\"\n",
    "            imgs_prompt = f\"Here are images mentioned in markdown text:\\n{img_info}\"\n",
    "        \n",
    "            qa_prompt = topics_prompt.format(\n",
    "                domain = domain,\n",
    "                example_json = json.dumps(topics_example_json, ensure_ascii=False), \n",
    "                markdown_text = input_text,\n",
    "                further_information = imgs_prompt)\n",
    "\n",
    "            res = llm_image_gen_w_retry(\n",
    "                api_key=api_key, model_name='gemini-2.0-flash-thinking-exp', \n",
    "                qa_prompt=qa_prompt, pil_images=pil_images, sys_prompt=None, temperature=0.6)\n",
    "\n",
    "        else:\n",
    "            qa_prompt = topics_prompt.format(\n",
    "                domain = domain,\n",
    "                example_json = json.dumps(topics_example_json, ensure_ascii=False), \n",
    "                markdown_text = input_text,\n",
    "                further_information = \"\")\n",
    "\n",
    "            res = llm_gen_w_retry(\n",
    "                api_key=api_key, model_name='gemini-2.0-flash-thinking-exp', \n",
    "                qa_prompt=qa_prompt, sys_prompt=None, temperature=0.6)\n",
    "        responses.append(res)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_wip_json_path = \"topic_analysis_wip_20250213.json\"\n",
    "\n",
    "with open(topic_wip_json_path, \"w\") as file:\n",
    "    json.dump(responses, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_json = {\n",
    "    \"field_of_study\": [\"Political Science\", \"Social Media Studies\", \"Communication Studies\", \"Sociology, Digital Culture\"],\n",
    "    \"keywords\": [\"social media usage\", \"political polarization\", \"mixed-methods approach\", \"semi-structured interviews\"],\n",
    "    \"tags\": [\"online behavior\", \"echo chambers\", \"survey methodology\", \"young adults\", \"political communication\", \"digital ethnography\", \"ideology\"],\n",
    "    \"section_type\": [\"abstract\", \"introduction\"]\n",
    "}\n",
    "\n",
    "additional_info_prompt = \"\"\"You are a sophisticated academic scholar with expertise in {domain}. \n",
    "You are renowned for your ability to quickly grasp the core concepts of research papers and expertly categorize and tag information for optimal organization and retrieval.\n",
    "\n",
    "## TASK\n",
    "When presented with some texts extracted from a research paper, you will meticulously analyze its content and provide the following:\n",
    "- field_of_study: Propose 2-4 detailed academic categories that this research paragraph would logically fall under. These categories should help situate the research within the fields of study. Consider the interdisciplinary nature of the paragraph as well.\n",
    "- keywords: Identify 3-5 key terms or phrases that accurately capture the specific subject matter and central ideas discussed within the paragraph. These keywords should be highly relevant and commonly used within the specific research area.\n",
    "- tags: Suggest 4-6 concise tags that could be used to further refine the indexing and searchability of the paragraph. These tags might include specific methodologies, theories, named entities, or emerging concepts mentioned within the text. They should be specific enough to differentiate the content from the broader categories.\n",
    "- section_class: Classify given excerpts based on their content and typical function within a research paper. **Remember to output ONLY the class names, ordered by descending closeness if more than one.** Candidate classes are:\n",
    "    * \"Abstract\"\n",
    "    * \"Introduction, Background, and Motivation\"\n",
    "    * \"Related Work, and Literature Review\"\n",
    "    * \"Methodology and Approach\"\n",
    "    * \"Experiment\"\n",
    "    * \"Analysis and Findings\"\n",
    "    * \"Conclusion\"\n",
    "    * \"References\"\n",
    "    * \"Acknowledgments\"\n",
    "    * \"FAQ\"\n",
    "    * \"Code and Examples\"\n",
    "\n",
    "Make sure you output in json with double quotes.\n",
    "\n",
    "## EXAMPLE\n",
    "Here is an example for demonstraction purpose only. Do not use this specific example in your response, it is solely illustrative.\n",
    "\n",
    "Input Paragraph:  \n",
    " ```\n",
    "\"This study employed a mixed-methods approach to investigate the impact of social media usage on political polarization among young adults in urban areas. \n",
    "Quantitative data was collected through a survey of 500 participants, while qualitative data was gathered via semi-structured interviews with a subset of 25 participants. \n",
    "The findings suggest a correlation between increased exposure to ideologically homogeneous content online and heightened political polarization.\"\n",
    " ```\n",
    "\n",
    "Hypothetical Output from this Example (Again, illustrative and not to be used in the actual response):\n",
    "```json\n",
    "{example_json}\n",
    "```\n",
    "\n",
    "## INSTRUCTIONS\n",
    "1. Your response should be clearly organized, using bullet points or numbered lists to separate the categories, keywords, and tags.\n",
    "2. Be precise and avoid overly broad or generic terms.\n",
    "3. Prioritize terms that are commonly used within the relevant academic field.\n",
    "4. Focus on accurate representation of the content provided.\n",
    "5. Ensure that categories, keywords, and tags are directly relevant to the specific area of expertise you are embodying.\n",
    "6. Please analyze the following paragraph and provide your expert recommendations:\n",
    "\n",
    "## INPUT\n",
    "Here is the section text in markdown format.\n",
    "```markdown\n",
    "{markdown_text}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:02:13,656 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:02:24,075 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds (Retry 1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:02:25,931 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:02:36,219 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:02:49,238 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:03:08,182 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:03:17,301 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:03:28,662 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds (Retry 1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 12:03:30,702 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:03:45,140 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:03:57,540 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:04:07,781 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-02-13 12:04:18,580 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "api_key = os.getenv('GEMINI_API_KEY_1')\n",
    "temperature = 0.7\n",
    "domain = \"Artificial Intelligence and LLMs\"\n",
    "tmp_path = \"/home/jiezi/Code/Temp/tmp/2412.06769v2\"\n",
    "\n",
    "addtional_infos = []\n",
    "for section in processed_json:\n",
    "    title = \"#\" * section.get('level') + \" \" + section.get('title')\n",
    "    md_text = section.get('refined_text')\n",
    "    input_text = title + \"\\n\" + md_text\n",
    "    \n",
    "    if title not in [\"References\", \"Acknowledgments\"] and len(md_text) > 200:\n",
    "        imgs_prompt = \"\"\n",
    "        qa_prompt = additional_info_prompt.format(\n",
    "            domain = domain,\n",
    "            example_json = json.dumps(example_json, ensure_ascii=False),\n",
    "            markdown_text = input_text)\n",
    "\n",
    "        res = llm_gen_w_retry(\n",
    "            api_key=api_key, model_name='gemini-2.0-flash-thinking-exp', \n",
    "            qa_prompt=qa_prompt, sys_prompt=None, temperature=0.6)\n",
    "        addtional_infos.append(res)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyinfo_wip_json_path = \"keyinfo_analysis_wip_20250213.json\"\n",
    "\n",
    "with open(keyinfo_wip_json_path, \"w\") as file:\n",
    "    json.dump(addtional_infos, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Research Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"You are a researcher in the field of '{subject}' who is good at summarizing papers using concise statements.\"\n",
    "\n",
    "summary_prompt = \"\"\" ## INSTRUCTION\n",
    "Given abstraction and introduction paragraph from the paper, you are asked to:                   \n",
    "1. identify the keywords of this article;\n",
    "2. summarize according to the following four points\n",
    "- (1): What is the research background of this article? What problem is this paper trying to solve? \n",
    "- (2): What are the relevant studies? What are the past methods? What are the issues with them? Is the approach well motivated?\n",
    "- (3): How does the paper solve this problem? What is the research methodology proposed in this paper?\n",
    "- (4): What experiments were done in the paper? On what task and what performance is achieved by the methods in this paper? Can the performance support their goals?\n",
    "- (5): Are there unsolved issues with the paper? What gaps can be explored further? Any suggestions?\n",
    "\n",
    "## CONTEXT\n",
    "Here are abstraction from the paper:\n",
    "<abstraction>\n",
    "{abstraction}\n",
    "</abstraction>\n",
    "\n",
    "Here are introduction from the paper:\n",
    "<introduction>\n",
    "{introduction}\n",
    "</introduction>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output that follows: \n",
    "```text                            \n",
    "1. Keywords: xxx\\n\\n     \n",
    "2. Summary: \\n\\n\n",
    "- (1):xxx;\\n \n",
    "- (2):xxx;\\n \n",
    "- (3):xxx;\\n  \n",
    "- (4):xxx.\\n\\n     \n",
    "- (5):xxx.\\n\\n  \n",
    "```\n",
    "\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not have too much repetitive information, numerical values using the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed.                 \n",
    "\"\"\"\n",
    "\n",
    "method_prompt = \"\"\"## INSTRUCTION\n",
    "Given method paragraph and a summary of a paper, you are asked to describe in detail the methodological idea of this article. \n",
    "- (1):...\n",
    "- (2):...\n",
    "- (3):...\n",
    "- .......\n",
    "\n",
    "## CONTEXT\n",
    "Here are method paragraph:\n",
    "<method>\n",
    "{method}\n",
    "</method>\n",
    "\n",
    "Here are summary of the paper fyi:\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output that follows: \n",
    "```text\n",
    "3. Methods: \\n\\n\n",
    "- (1):xxx;\\n \n",
    "- (2):xxx;\\n \n",
    "- (3):xxx;\\n  \n",
    "....... \\n\\n     \n",
    "```\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not repeat the content of the previous <summary>, the value of the use of the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed, ....... means fill in according to the actual requirements.                 \n",
    "\"\"\"\n",
    " \n",
    "conclusion_prompt = \"\"\"## INSTRUCTION\n",
    "Given conclusion paragraph and a summary of a paper, you are asked to: \n",
    "4. Make the following summary:\n",
    "- (1):What is the significance of this piece of work?\n",
    "- (2):Summarize the strengths and weaknesses of this article in three dimensions: innovation point, performance, and workload.                   \n",
    ".......\n",
    "\n",
    "    \"contribution\": \"What is the contribution of this paper?\",\n",
    "    \"novelty\": \"What is the novelty of this paper?\",\n",
    "    \"strength\": \"What are the strengths of this paper?\",\n",
    "    \"drawback\": \"What are the drawbacks of this paper?\",\n",
    "    \"improvement\": \"What might be the improvements of this paper?\",\n",
    "\n",
    "\n",
    "## CONTEXT\n",
    "Here are conclusion paragraph:\n",
    "<conclusion>\n",
    "{conclusion}\n",
    "</conclusion>\n",
    "\n",
    "Here are summary of the paper fyi:\n",
    "<summary>\n",
    "{summary}\n",
    "</summary>\n",
    "\n",
    "## OUTPUT\n",
    "Follow the format of the output later: \n",
    "```text\n",
    "4. Conclusion: \\n\\n\n",
    "- (1):xxx;\\n                     \n",
    "- (2):Innovation point: xxx; Performance: xxx; Workload: xxx;\\n    \n",
    "- (3):\n",
    "    contribution: What is the contribution of this paper?,\n",
    "    novelty: What is the novelty of this paper?,\n",
    "    strength\": What are the strengths of this paper?,\n",
    "    drawback: What are the drawbacks of this paper?,\n",
    "    improvement\": What might be the improvements of this paper?\n",
    "```\n",
    "\n",
    "Be sure to use {lang} answers (proper nouns need to be marked in English), statements as concise and academic as possible.\n",
    "Do not repeat the content of the previous <summary>, the value of the use of the original numbers.\n",
    "Be sure to strictly follow the format, the corresponding content output to xxx, in accordance with \\n line feed, ....... means fill in according to the actual requirements.                 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models (LLMs) are restricted to reason in the\"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed\"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_metadata[0].get('abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_repair import repair_json  # https://github.com/mangiucugna/json_repair/\n",
    "topics_infos = [repair_json(x) for x in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_text_lst = []\n",
    "for item in topics_infos:\n",
    "    item_json = json.loads(item)\n",
    "    item_topics = item_json.get('topics')\n",
    "    item_topics_text = \"\\n\".join([f\"## {x.get('topic')}  \\n{x.get('description')}  \\n\" for x in item_topics])\n",
    "    topics_text_lst.append(item_topics_text)\n",
    "\n",
    "topics_text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_md_text = ss_metadata[0].get('abstract')\n",
    "\n",
    "intro_md_text, met_text, con_md_text = \"\", \"\", \"\"\n",
    "for item in processed_json:\n",
    "    title = item.get('title').strip()\n",
    "    md_text = item.get('refined_text')\n",
    "    if title.lower() in ['introduction', 'overview']:\n",
    "        intro_md_text = md_text\n",
    "    elif title.lower() in ['method', 'methodology', 'approach', 'framework']:\n",
    "        met_text = md_text\n",
    "    elif title.lower() in ['conclusion', 'summary']:\n",
    "        con_md_text = md_text\n",
    "\n",
    "\n",
    "sum_md_text = \"\"\"# Key Information  \n",
    "{md_text}\n",
    "\"\"\".format(md_text=\"\\n\".join(topics_text_lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_info = [{\n",
    "    'abstract_text': abs_md_text,\n",
    "    'introduction_text': intro_md_text,\n",
    "    'method_text': met_text,\n",
    "    'conclusion_text': con_md_text,\n",
    "    'summary_text': sum_md_text\n",
    "             }]\n",
    "\n",
    "temptext_wip_json_path = \"temptext_analysis_wip_20250213.json\"\n",
    "\n",
    "with open(temptext_wip_json_path, \"w\") as file:\n",
    "    json.dump(tmp_info, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
