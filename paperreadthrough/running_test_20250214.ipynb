{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on ToC Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import toml\n",
    "import copy\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from pdf_process.pdf_meta_det import extract_meta, dump_toml\n",
    "from pdf_process.pdf_toc_det import gen_toc\n",
    "\n",
    "SECTION_TITLES = [\"Abstract\",\n",
    "                'Introduction', 'Related Work', 'Background',\n",
    "                \"Introduction and Motivation\", \"Computation Function\", \" Routing Function\",\n",
    "                \"Preliminary\", \"Problem Formulation\",\n",
    "                'Methods', 'Methodology', \"Method\", 'Approach', 'Approaches',\n",
    "                \"Materials and Methods\", \"Experiment Settings\",\n",
    "                'Experiment', \"Experimental Results\", \"Evaluation\", \"Experiments\",\n",
    "                \"Results\", 'Findings', 'Data Analysis',\n",
    "                \"Discussion\", \"Results and Discussion\", \"Conclusion\",\n",
    "                'References',\n",
    "                \"Acknowledgments\", \"Appendix\", \"FAQ\", \"Frequently Asked Questions\"]\n",
    "APPENDDIX_TITLES = [\"References\", \"Acknowledgments\", \"Appendix\", \"FAQ\", \"Frequently Asked Questions\"]     \n",
    "\n",
    "def count_by_keys(lst_dct, keys):\n",
    "    \"\"\"get item count within a list of dict by specified dict keys\n",
    "    Args:\n",
    "        lst_dct: list of dict\n",
    "        keys: specified dict keys like ['a', 'b', 'c']。\n",
    "    Returns:\n",
    "        dict: count of items based on keys combinations in descending order\n",
    "    \"\"\"\n",
    "    key_combinations = []\n",
    "    for dct in lst_dct:\n",
    "        combination = tuple(dct.get(key) for key in keys)\n",
    "        key_combinations.append(combination)\n",
    "    result_cnt = Counter(key_combinations)\n",
    "    sorted_result = sorted(result_cnt.items(), key=lambda item: item[0], reverse=True)\n",
    "    return sorted_result\n",
    "\n",
    "\n",
    "# OUtline Detection\n",
    "class PDFOutline:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.doc = self.open_pdf()\n",
    "\n",
    "    def open_pdf(self):\n",
    "        \"\"\"open pdf doc\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(self.pdf_path)\n",
    "            return doc\n",
    "        except Exception as e:\n",
    "            print(f\"处理 PDF 文件时出错: {self.pdf_path}, 错误信息: {e}\")\n",
    "            return None # 或者抛出异常，根据实际需求决定\n",
    "        \n",
    "    def toc_extraction(self, excpert_len:Optional[int]=300):\n",
    "        \"\"\"apply pymupdf to extract outline\n",
    "        Args:\n",
    "            pdf_path: path to pdf file\n",
    "            excpert_len: excerpt lenght of initial text\n",
    "        Return:\n",
    "            pdf_toc: pdf toc including level, title, page, position, nameddest, if_collapse, excerpt\n",
    "                     if_collapse: if contains next level title\n",
    "                     excerpt: initial text\n",
    "        \"\"\"\n",
    "        toc = self.doc.get_toc(simple=False) or []\n",
    "\n",
    "        pdf_toc = []\n",
    "        if len(toc) > 0:\n",
    "            for item in toc:\n",
    "                lvl = item[0] if len(item) > 0 else None\n",
    "                title = item[1] if len(item) > 1 else None\n",
    "                start_page = item[2] if len(item) > 2 else None\n",
    "                pos = item[3].get('to') if len(item) > 3 and item[3] else None\n",
    "                nameddest = item[3].get('nameddest') if len(item) > 3 and item[3] else None\n",
    "                if_collapse = item[3].get('collapse', False) if len(item) > 3 and item[3] else None\n",
    "\n",
    "                # get initial lines\n",
    "                lines = \"\"\n",
    "                if start_page is not None:\n",
    "                    page = self.doc[start_page-1]\n",
    "                    blocks = page.get_text(\"blocks\")\n",
    "                    for block in blocks:\n",
    "                        x0, y0, x1, y1, text, _, _ = block\n",
    "                        if len(lines) < excpert_len:\n",
    "                            if pos and x0 >= pos.x:\n",
    "                                lines += text\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    pdf_toc.append({\n",
    "                        \"level\": lvl,\n",
    "                        \"title\": title,\n",
    "                        \"page\": start_page,\n",
    "                        \"position\": pos,\n",
    "                        \"nameddest\": nameddest,\n",
    "                        'if_collapse': if_collapse,\n",
    "                        \"excerpt\": lines + \"...\"\n",
    "                    })\n",
    "        return pdf_toc\n",
    "    \n",
    "    def toc_detection(self, excpert_len:Optional[int]=300, titles=SECTION_TITLES):\n",
    "        \"\"\"identify toc based on title font, layout, etc\"\"\"\n",
    "        matched_meta_lst = []\n",
    "        pattern = '|'.join(re.escape(title) for title in titles)  \n",
    "        for i in range(len(self.doc)):\n",
    "            # extract_meta returns font size (size), font style (flags), font type (char_flags) \n",
    "            res = extract_meta(self.doc, pattern=pattern, page=i+1, ign_case=True)\n",
    "            matched_meta_lst.extend(res)\n",
    "\n",
    "        # get font size for titles\n",
    "        keys = ['size']\n",
    "        combinations = count_by_keys(matched_meta_lst, keys)  # get sorted count by keys in matched_meta_lst\n",
    "        for x in combinations:\n",
    "            if x[1] > 2:\n",
    "                font_size = x[0][0]\n",
    "                break\n",
    "\n",
    "        # return to sampled_metadata to match all potential combinations\n",
    "        title_meta_sample = [item for item in matched_meta_lst if item.get('size') == font_size]\n",
    "\n",
    "        auto_level = 1\n",
    "        addnl = False\n",
    "        title_meta_toml = [dump_toml(m, auto_level, addnl) for m in title_meta_sample]\n",
    "\n",
    "        # 直接使用 toml.loads 从字符串中加载 TOML 数据\n",
    "        recipe = toml.loads('\\n'.join(title_meta_toml))\n",
    "        toc = gen_toc(self.doc, recipe)\n",
    "\n",
    "        pdf_toc = []\n",
    "        if len(toc) > 0:\n",
    "            for item in toc:\n",
    "                start_page = item.pagenum\n",
    "                pos = item.pos\n",
    "                \n",
    "                # get initial lines\n",
    "                if start_page is not None:\n",
    "                    page = self.doc[start_page-1]\n",
    "                    blocks = page.get_text(\"blocks\")\n",
    "                    lines = \"\"\n",
    "                    for block in blocks:\n",
    "                        x0, y0, x1, y1, text, _, _ = block\n",
    "                        if len(lines) < excpert_len:\n",
    "                            if pos and x0 >= pos.x:\n",
    "                                lines += text\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    pdf_toc.append({\n",
    "                        \"level\": item.level,\n",
    "                        \"title\": item.title,\n",
    "                        \"page\": item.pagenum,\n",
    "                        \"position\": item.pos,\n",
    "                        \"nameddest\": \"section.\",\n",
    "                        'if_collapse': None,\n",
    "                        \"excerpt\": lines + \"...\"\n",
    "                    })\n",
    "        return pdf_toc\n",
    "    \n",
    "    def identify_toc_appendix(self, pdf_toc):\n",
    "        pdf_toc_rvsd = copy.deepcopy(pdf_toc)\n",
    "        pattern = '|'.join(re.escape(title) for title in APPENDDIX_TITLES) \n",
    "\n",
    "        for idx, item in enumerate(pdf_toc_rvsd):\n",
    "            mtch = re.search(pattern, item.get('title'), re.IGNORECASE)\n",
    "            if mtch:\n",
    "                item['if_appendix'] = True\n",
    "            elif 'appendix' in item.get('nameddest'):\n",
    "                item['if_appendix'] = True\n",
    "            elif idx > 0 and pdf_toc_rvsd[idx-1].get('if_appendix') == True:\n",
    "                item['if_appendix'] = True\n",
    "            else:\n",
    "                item['if_appendix'] = False\n",
    "        return pdf_toc_rvsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Training Large Language Models to Reason in a Continuous Latent Space\"\n",
    "pdf_path = \"/home/jiezi/Code/Temp/data/2412.06769v2.pdf\"\n",
    "\n",
    "outline = PDFOutline(pdf_path=pdf_path)\n",
    "toc_1 = outline.toc_extraction()\n",
    "toc_2 = outline.toc_detection()\n",
    "\n",
    "toc_1_rvsd = outline.identify_toc_appendix(toc_1)\n",
    "toc_2_rvsd = outline.identify_toc_appendix(toc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Markdown Titles with ToC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = \"/home/jiezi/Code/Temp/tmp/2412.06769v2/full.md\"\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_md_toc(pdf_md, pdf_toc):\n",
    "    \"\"\"Align markdown title with pdf table of content \n",
    "    Args:\n",
    "        md_file: Path to the markdown file.\n",
    "        pdf_toc: pdf toc from pdf_outline_detection function\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a section\n",
    "        with 'level', 'section_num', 'title', and 'text' keys.\n",
    "        Returns an empty list if the file doesn't exist.\n",
    "        Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    lines = pdf_md.splitlines()\n",
    "\n",
    "    modified_lines = [] \n",
    "    md_titles_info = []  # store title after modification\n",
    "    title_pattern = r\"^#{1,}\\s*.*$\"  # patttern of markdown title\n",
    "    \n",
    "    for line in lines: \n",
    "        if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"] and len(line) < 100:\n",
    "            ptrn_match = re.match(title_pattern, line)\n",
    "            if ptrn_match:  # find markdown title\n",
    "                flag = 0\n",
    "\n",
    "                for toc in pdf_toc:  # iterate pdf toc, refine markdown title based on toc title\n",
    "                    toc_title = toc['title'] \n",
    "                    toc_level = int(toc['level'])  \n",
    "                    if_appendix = toc['if_appendix']\n",
    "                    if re.search(re.escape(toc_title), line, re.IGNORECASE): # if toc_title in line: \n",
    "                        section_title = \"#\"*toc_level + \" \" + toc_title + \"  \"\n",
    "                        title_info = {'title': section_title, 'level': toc_level, 'if_appendix': if_appendix, 'if_modified': True}\n",
    "                        flag = 1\n",
    "                        break\n",
    "                \n",
    "                if flag == 0:  \n",
    "                    # for appendix\n",
    "                    pattern = '|'.join(re.escape(title) for title in APPENDDIX_TITLES) \n",
    "                    mtch = re.search(pattern, line, re.IGNORECASE)\n",
    "                    if mtch:\n",
    "                        section_title = title\n",
    "                        level = re.match('^#{1,}', line).group(0).count(\"#\")\n",
    "                        title_info = {'title': section_title, 'level': level, 'if_appendix': True, 'if_modified': False}\n",
    "                        flag = 1\n",
    "                \n",
    "                if flag == 0:\n",
    "                    # for others, downgrade one more level\n",
    "                    if len(md_titles_info) > 0:\n",
    "                        level = line.count(\"#\") + 1\n",
    "                        if_appendix = md_titles_info[-1].get('if_appendix')\n",
    "                        section_title = re.sub('^#{1,}', '#'*level, line)\n",
    "                        title_info = {'title': section_title, 'level': level, 'if_appendix': if_appendix, 'if_modified': True}\n",
    "                    else:\n",
    "                        section_title = line\n",
    "                        title_info = {'title': section_title, 'level': 1, 'if_appendix': False, 'if_modified': False}\n",
    "\n",
    "                modified_lines.append(section_title)\n",
    "                if title_info not in md_titles_info:\n",
    "                    md_titles_info.append(title_info)  # get markdown title\n",
    "                \n",
    "        else:\n",
    "            modified_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(modified_lines), md_titles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1, titles_1 = align_md_toc(markdown_content, toc_1_rvsd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Reference List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 19:03:00,252 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Training%20Large%20Language%20Models%20to%20Reason%20in%20a%20Continuous%20Latent%20Space&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673fbdd957cada770d10dffca5e45b53da43a3c6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 19:03:02,139 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apis.arxiv_tool import ArxivKit\n",
    "from apis.semanticscholar_tool import SemanticScholarKit\n",
    "\n",
    "title = \"Training Large Language Models to Reason in a Continuous Latent Space\"\n",
    "\n",
    "ss = SemanticScholarKit()\n",
    "ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)\n",
    "\n",
    "paper_ss_id = ss_metadata[0].get('paperId')\n",
    "print(paper_ss_id)\n",
    "\n",
    "reference_metadata = ss.get_semanticscholar_references(paper_id=paper_ss_id, limit=100)\n",
    "len(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = markdown_content.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_non_text_chars(text):\n",
    "    \"\"\"remove non text chars\n",
    "    \"\"\"\n",
    "    valid_chars = string.ascii_letters + string.digits  # 包含所有字母和数字的字符串\n",
    "    cleaned_text = ''\n",
    "    for char in text:\n",
    "        if char in valid_chars:\n",
    "            cleaned_text += char\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from thefuzz import fuzz # pip install thefuzz  https://github.com/seatgeek/thefuzz\n",
    "\n",
    "def modify_image_info(self, reference_metadata):\n",
    "    lines = self.pdf_md.splitlines()\n",
    "\n",
    "    ref_lst = copy.deepcopy(reference_metadata)\n",
    "    for ref in reference_metadata:\n",
    "        title = ref.get('citedPaper', {}).get('title')\n",
    "        if title:\n",
    "            for line in lines:\n",
    "                if len(line) < 500:\n",
    "                    if re.search(re.escape(title), line, re.IGNORECASE):\n",
    "                        ref['org_md_ref'] = line\n",
    "                    else:\n",
    "                        ratio = fuzz.partial_ratio(title, line)\n",
    "                        if ratio > 80:\n",
    "                            ref['org_md_ref'] = line\n",
    "                            break\n",
    "                        else:\n",
    "                            ptrn = remove_non_text_chars(title) \n",
    "                            line_rvsd =  remove_non_text_chars(line)\n",
    "                            if re.search(re.escape(ptrn), line_rvsd, re.IGNORECASE):\n",
    "                                ref['org_md_ref'] = line\n",
    "    return ref_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on PDF post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/jiezi/Code/Temp/tmp/2412.06769v2/content_list.json\") as json_data:\n",
    "    content_json = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_process.pdf_post_process import PDFProcess\n",
    "\n",
    "pdf = PDFProcess(pdf_path, toc_1_rvsd, markdown_content, content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.alighn_md_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_titles_info = pdf.align_md_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst, tbl_lst, formula_lst  = pdf.align_content_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst_rvsd = pdf.modify_image_info(img_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiezi/Code/Github/PaperDeepInterpretation/paperdeepinterpretation/pdf_process/pdf_post_process.py:380: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(line, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "tbl_lst_rvsd = pdf.modify_tables_info(tbl_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_lst = pdf.modify_reference_info(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
