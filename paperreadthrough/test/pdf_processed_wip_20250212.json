[
    {
        "level": 1,
        "num": 1,
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "text": "Shibo Hao $^{1,2,*}$ , Sainbayar Sukhbaatar1, DiJia $\\mathtt{s u}^{1}$ , Xian Li1, Zhiting ${\\mathsf{H}}{\\mathsf{u}}^{2}$ , Jason Weston1, Yuandong Tian1   \n1FAIR at Meta, $^2$ UC San Diego   \n\u2217Work done at Meta  \nLarge language models (LLMs) are restricted to reason in the \u201clanguage space\u201d, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \u201ccontinuous thought\u201d). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can efectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-frst search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These fndings demonstrate the promise of latent reasoning and ofer valuable insights for future research.  \nDate: December 12, 2024  \n$\\infty$ Meta",
        "lines": [
            {
                "id": 1,
                "line": "Shibo Hao $^{1,2,*}$ , Sainbayar Sukhbaatar1, DiJia $\\mathtt{s u}^{1}$ , Xian Li1, Zhiting ${\\mathsf{H}}{\\mathsf{u}}^{2}$ , Jason Weston1, Yuandong Tian1   "
            },
            {
                "id": 2,
                "line": "1FAIR at Meta, $^2$ UC San Diego   "
            },
            {
                "id": 3,
                "line": "\u2217Work done at Meta  "
            },
            {
                "id": 4,
                "line": "Large language models (LLMs) are restricted to reason in the \u201clanguage space\u201d, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \u201ccontinuous thought\u201d). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can efectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-frst search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These fndings demonstrate the promise of latent reasoning and ofer valuable insights for future research.  "
            },
            {
                "id": 5,
                "line": "Date: December 12, 2024  "
            },
            {
                "id": 6,
                "line": "$\\infty$ Meta  "
            }
        ],
        "refined_text": "Shibo Hao $^{1,2,*}$ , Sainbayar Sukhbaatar1, DiJia $\\mathtt{s u}^{1}$ , Xian Li1, Zhiting ${\\mathsf{H}}{\\mathsf{u}}^{2}$ , Jason Weston1, Yuandong Tian1   \n1FAIR at Meta, $^2$ UC San Diego   \n\u2217Work done at Meta  \nLarge language models (LLMs) are restricted to reason in the \u201clanguage space\u201d, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \u201ccontinuous thought\u201d). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can efectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-frst search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These fndings demonstrate the promise of latent reasoning and ofer valuable insights for future research.  \nDate: December 12, 2024  \n$\\infty$ Meta",
        "images": [],
        "tables": [],
        "references": []
    },
    {
        "level": 1,
        "num": 2,
        "title": "Introduction",
        "text": "Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an efective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach, known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to generate solutions step-by-step using natural language. However, this is in stark contrast to certain human cognition results. Neuroimaging studies have consistently shown that the language network \u2013 a set of brain regions responsible for language comprehension and production \u2013 remains largely inactive during various reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko et al., 2024).  \nA signifcant issue arises when LLMs use language for reasoning: the amount of reasoning required for each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fuency, contributing little to the actual reasoning process. On the contrary, some critical tokens require complex planning and pose huge challenges to LLMs. While previous work has attempted to fx these problems by prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain constrained within the language space and do not solve the fundamental problems. On the contrary, it would be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their fndings into language only when necessary.  \n![Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg 'Figure 1:')  \nIn this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut (Chain of Continuous Thought). It involves a simple modifcation to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token (Figure 1). This modifcation frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully diferentiable. To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024), which efectively utilizes language reasoning chains to guide the training process.  \nInterestingly, our proposed paradigm leads to an efcient reasoning pattern. Unlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-frst search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).  \nExperimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning (GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be benefcial to reasoning accuracy, mirroring the efects of language reasoning chains. This indicates the potential to scale and solve increasingly challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut and some of its variants even surpasses language-based CoT methods, while generating signifcantly fewer tokens during inference. We believe that these fndings underscore the potential of latent reasoning and could provide valuable insights for future research.",
        "lines": [
            {
                "id": 8,
                "line": "Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an efective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach, known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to generate solutions step-by-step using natural language. However, this is in stark contrast to certain human cognition results. Neuroimaging studies have consistently shown that the language network \u2013 a set of brain regions responsible for language comprehension and production \u2013 remains largely inactive during various reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko et al., 2024).  "
            },
            {
                "id": 9,
                "line": "A signifcant issue arises when LLMs use language for reasoning: the amount of reasoning required for each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fuency, contributing little to the actual reasoning process. On the contrary, some critical tokens require complex planning and pose huge challenges to LLMs. While previous work has attempted to fx these problems by prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain constrained within the language space and do not solve the fundamental problems. On the contrary, it would be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their fndings into language only when necessary.  "
            },
            {
                "id": 10,
                "line": "![Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg 'Figure 1:')  "
            },
            {
                "id": 12,
                "line": "In this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut (Chain of Continuous Thought). It involves a simple modifcation to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token (Figure 1). This modifcation frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully diferentiable. To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024), which efectively utilizes language reasoning chains to guide the training process.  "
            },
            {
                "id": 13,
                "line": "Interestingly, our proposed paradigm leads to an efcient reasoning pattern. Unlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-frst search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).  "
            },
            {
                "id": 14,
                "line": "Experimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning (GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be benefcial to reasoning accuracy, mirroring the efects of language reasoning chains. This indicates the potential to scale and solve increasingly challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut and some of its variants even surpasses language-based CoT methods, while generating signifcantly fewer tokens during inference. We believe that these fndings underscore the potential of latent reasoning and could provide valuable insights for future research.  "
            }
        ],
        "refined_text": "Large language models (LLMs) have demonstrated remarkable reasoning abilities, emerging from extensive pretraining on human languages (Dubey et al., 2024; Achiam et al., 2023). While next token prediction is an efective training objective, it imposes a fundamental constraint on the LLM as a reasoning machine: the explicit reasoning process of LLMs must be generated in word tokens. For example, a prevalent approach, known as chain-of-thought (CoT) reasoning (Wei et al., 2022), involves prompting or training LLMs to generate solutions step-by-step using natural language. However, this is in stark contrast to certain human cognition results. Neuroimaging studies have consistently shown that the language network \u2013 a set of brain regions responsible for language comprehension and production \u2013 remains largely inactive during various reasoning tasks (Amalric and Dehaene, 2019; Monti et al., 2012, 2007, 2009; Fedorenko et al., 2011). Further evidence indicates that human language is optimized for communication rather than reasoning (Fedorenko et al., 2024).  \nA signifcant issue arises when LLMs use language for reasoning: the amount of reasoning required for each particular reasoning token varies greatly, yet current LLM architectures allocate nearly the same computing budget for predicting every token. Most tokens in a reasoning chain are generated solely for fuency, contributing little to the actual reasoning process. On the contrary, some critical tokens require complex planning and pose huge challenges to LLMs. While previous work has attempted to fx these problems by prompting LLMs to generate succinct reasoning chains (Madaan and Yazdanbakhsh, 2022), or performing additional reasoning before generating some critical tokens (Zelikman et al., 2024), these solutions remain constrained within the language space and do not solve the fundamental problems. On the contrary, it would be ideal for LLMs to have the freedom to reason without any language constraints, and then translate their fndings into language only when necessary.  \n    \nIn this work we instead explore LLM reasoning in a latent space by introducing a novel paradigm, Coconut (Chain of Continuous Thought). It involves a simple modifcation to the traditional CoT process: instead of mapping between hidden states and language tokens using the language model head and embedding layer, Coconut directly feeds the last hidden state (a continuous thought) as the input embedding for the next token (Figure 1). This modifcation frees the reasoning from being within the language space, and the system can be optimized end-to-end by gradient descent, as continuous thoughts are fully diferentiable. To enhance the training of latent reasoning, we employ a multi-stage training strategy inspired by Deng et al. (2024), which efectively utilizes language reasoning chains to guide the training process.  \n  \nInterestingly, our proposed paradigm leads to an efcient reasoning pattern. Unlike language-based reasoning, continuous thoughts in Coconut can encode multiple potential next steps simultaneously, allowing for a reasoning process akin to breadth-frst search (BFS). While the model may not initially make the correct decision, it can maintain many possible options within the continuous thoughts and progressively eliminate incorrect paths through reasoning, guided by some implicit value functions. This advanced reasoning mechanism surpasses traditional CoT, even though the model is not explicitly trained or instructed to operate in this manner, as seen in previous works (Yao et al., 2023; Hao et al., 2023).  \nExperimentally, Coconut successfully enhances the reasoning capabilities of LLMs. For math reasoning (GSM8k, Cobbe et al., 2021), using continuous thoughts is shown to be benefcial to reasoning accuracy, mirroring the efects of language reasoning chains. This indicates the potential to scale and solve increasingly challenging problems by chaining more continuous thoughts. On logical reasoning including ProntoQA (Saparov and He, 2022), and our newly proposed ProsQA (Section 4.1) which requires stronger planning ability, Coconut and some of its variants even surpasses language-based CoT methods, while generating signifcantly fewer tokens during inference. We believe that these fndings underscore the potential of latent reasoning and could provide valuable insights for future research.",
        "images": [
            {
                "type": "image",
                "img_path": "images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg",
                "img_caption": [
                    "Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. "
                ],
                "img_footnote": [],
                "page_idx": 1,
                "id": "Figure 1",
                "related_ids": [],
                "title": "Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT).",
                "description": "Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. \n",
                "org_md_ref": "![](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg 'new')",
                "mod_md_ref": "![Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg 'Figure 1:')  "
            }
        ],
        "tables": [],
        "references": [
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "40e8af970329135ec95057d73e239dab805ad128",
                "externalIds": {
                    "ArXiv": "2407.21783",
                    "DBLP": "journals/corr/abs-2407-21783",
                    "DOI": "10.48550/arXiv.2407.21783",
                    "CorpusId": 271571434
                },
                "corpusId": 271571434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128",
                "title": "The Llama 3 Herd of Models",
                "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 1865,
                "influentialCitationCount": 394,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-31",
                "journal": {
                    "volume": "abs/2407.21783",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Dubey2024TheL3,\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aur\u00e9lien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cant\u00f3n Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gr\u00e9goire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasi\u0107 and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Doll\u00e1r and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Llama 3 Herd of Models},\n volume = {abs/2407.21783},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2479521",
                        "name": "Abhimanyu Dubey"
                    },
                    {
                        "authorId": "2369482",
                        "name": "Abhinav Jauhri"
                    },
                    {
                        "authorId": "2299944289",
                        "name": "Abhinav Pandey"
                    },
                    {
                        "authorId": "89942851",
                        "name": "Abhishek Kadian"
                    },
                    {
                        "authorId": "2313916217",
                        "name": "Ahmad Al-Dahle"
                    },
                    {
                        "authorId": "2313924937",
                        "name": "Aiesha Letman"
                    },
                    {
                        "authorId": "2313975817",
                        "name": "Akhil Mathur"
                    },
                    {
                        "authorId": "14279694",
                        "name": "Alan Schelten"
                    },
                    {
                        "authorId": "2329138320",
                        "name": "Amy Yang"
                    },
                    {
                        "authorId": "2247818297",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2313918197",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "2129047988",
                        "name": "Anthony S. Hartshorn"
                    },
                    {
                        "authorId": "2269467670",
                        "name": "Aobo Yang"
                    },
                    {
                        "authorId": "2313926281",
                        "name": "Archi Mitra"
                    },
                    {
                        "authorId": "2313918952",
                        "name": "Archie Sravankumar"
                    },
                    {
                        "authorId": "2294453195",
                        "name": "Artem Korenev"
                    },
                    {
                        "authorId": "2279336258",
                        "name": "Arthur Hinsvark"
                    },
                    {
                        "authorId": "2314521400",
                        "name": "Arun Rao"
                    },
                    {
                        "authorId": "2313922587",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "2166043087",
                        "name": "Aur\u00e9lien Rodriguez"
                    },
                    {
                        "authorId": "2313910288",
                        "name": "Austen Gregerson"
                    },
                    {
                        "authorId": "2295667288",
                        "name": "Ava Spataru"
                    },
                    {
                        "authorId": "2313953240",
                        "name": "Bap-tiste Roziere"
                    },
                    {
                        "authorId": "2313916233",
                        "name": "Bethany Biron"
                    },
                    {
                        "authorId": "2237987675",
                        "name": "Binh Tang"
                    },
                    {
                        "authorId": "2079950350",
                        "name": "Bobbie Chern"
                    },
                    {
                        "authorId": "83928755",
                        "name": "C. Caucheteux"
                    },
                    {
                        "authorId": "2313917653",
                        "name": "Chaya Nayak"
                    },
                    {
                        "authorId": "2313909658",
                        "name": "Chloe Bi"
                    },
                    {
                        "authorId": "2313913576",
                        "name": "Chris Marra"
                    },
                    {
                        "authorId": "2217959550",
                        "name": "Chris McConnell"
                    },
                    {
                        "authorId": "2313909741",
                        "name": "Christian Keller"
                    },
                    {
                        "authorId": "103277778",
                        "name": "Christophe Touret"
                    },
                    {
                        "authorId": "2268428822",
                        "name": "Chunyang Wu"
                    },
                    {
                        "authorId": "2273700455",
                        "name": "Corinne Wong"
                    },
                    {
                        "authorId": "66286536",
                        "name": "Cristian Cant\u00f3n Ferrer"
                    },
                    {
                        "authorId": "2273414632",
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "authorId": "51882206",
                        "name": "Damien Allonsius"
                    },
                    {
                        "authorId": "2273006690",
                        "name": "Daniel Song"
                    },
                    {
                        "authorId": "2313909437",
                        "name": "Danielle Pintz"
                    },
                    {
                        "authorId": "2313918299",
                        "name": "Danny Livshits"
                    },
                    {
                        "authorId": "71039937",
                        "name": "David Esiobu"
                    },
                    {
                        "authorId": "2303390957",
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "authorId": "2267338678",
                        "name": "Dhruv Mahajan"
                    },
                    {
                        "authorId": "2269456985",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "2306842160",
                        "name": "Diego Perino"
                    },
                    {
                        "authorId": "3449411",
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "authorId": "2343773325",
                        "name": "Egor Lakomkin"
                    },
                    {
                        "authorId": "1394834533",
                        "name": "Ehab A. AlBadawy"
                    },
                    {
                        "authorId": "2313918680",
                        "name": "Elina Lobanova"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "2268821751",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2708577",
                        "name": "Filip Radenovic"
                    },
                    {
                        "authorId": "2313967211",
                        "name": "Frank Zhang"
                    },
                    {
                        "authorId": "2282469774",
                        "name": "Gabriele Synnaeve"
                    },
                    {
                        "authorId": "2314074302",
                        "name": "Gabrielle Lee"
                    },
                    {
                        "authorId": "2313919767",
                        "name": "Georgia Lewis Anderson"
                    },
                    {
                        "authorId": "2268397654",
                        "name": "Graeme Nail"
                    },
                    {
                        "authorId": "51888120",
                        "name": "Gr\u00e9goire Mialon"
                    },
                    {
                        "authorId": "2264339927",
                        "name": "Guanglong Pang"
                    },
                    {
                        "authorId": "2313924900",
                        "name": "Guillem Cucurell"
                    },
                    {
                        "authorId": "2314075528",
                        "name": "Hailey Nguyen"
                    },
                    {
                        "authorId": "103405110",
                        "name": "Hannah Korevaar"
                    },
                    {
                        "authorId": "2314125186",
                        "name": "Hu Xu"
                    },
                    {
                        "authorId": "2290402489",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "121929334",
                        "name": "Iliyan Zarov"
                    },
                    {
                        "authorId": "34921162",
                        "name": "Imanol Arrieta Ibarra"
                    },
                    {
                        "authorId": "2207049",
                        "name": "Isabel M. Kloumann"
                    },
                    {
                        "authorId": "2267241285",
                        "name": "Ishan Misra"
                    },
                    {
                        "authorId": "2264288587",
                        "name": "Ivan Evtimov"
                    },
                    {
                        "authorId": "1805998294",
                        "name": "Jade Copet"
                    },
                    {
                        "authorId": "2314056661",
                        "name": "Jaewon Lee"
                    },
                    {
                        "authorId": "50825669",
                        "name": "J. Geffert"
                    },
                    {
                        "authorId": "2313917660",
                        "name": "Jana Vranes"
                    },
                    {
                        "authorId": "2314078634",
                        "name": "Jason Park"
                    },
                    {
                        "authorId": "3222225",
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "authorId": "2313919733",
                        "name": "Jeet Shah"
                    },
                    {
                        "authorId": "35721567",
                        "name": "J. V. D. Linde"
                    },
                    {
                        "authorId": "2313909388",
                        "name": "Jennifer Billock"
                    },
                    {
                        "authorId": "2287049560",
                        "name": "Jenny Hong"
                    },
                    {
                        "authorId": "2223749565",
                        "name": "Jenya Lee"
                    },
                    {
                        "authorId": "2223974989",
                        "name": "Jeremy Fu"
                    },
                    {
                        "authorId": "31357678",
                        "name": "Jianfeng Chi"
                    },
                    {
                        "authorId": "2314428565",
                        "name": "Jianyu Huang"
                    },
                    {
                        "authorId": "2314080357",
                        "name": "Jiawen Liu"
                    },
                    {
                        "authorId": "2314602001",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2314078877",
                        "name": "Jiecao Yu"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    },
                    {
                        "authorId": "90591458",
                        "name": "Joe Spisak"
                    },
                    {
                        "authorId": "2149161568",
                        "name": "Jongsoo Park"
                    },
                    {
                        "authorId": "2313925205",
                        "name": "Joseph Rocca"
                    },
                    {
                        "authorId": "2313912873",
                        "name": "Joshua Johnstun"
                    },
                    {
                        "authorId": "2273413914",
                        "name": "Joshua Saxe"
                    },
                    {
                        "authorId": "2211671694",
                        "name": "Ju-Qing Jia"
                    },
                    {
                        "authorId": "2313918589",
                        "name": "Kalyan Vasuden Alwala"
                    },
                    {
                        "authorId": "17097160",
                        "name": "K. Upasani"
                    },
                    {
                        "authorId": "2313918427",
                        "name": "Kate Plawiak"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2285859430",
                        "name": "K. Heafield"
                    },
                    {
                        "authorId": "2282542714",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1405642252",
                        "name": "Khalid El-Arini"
                    },
                    {
                        "authorId": "2273645788",
                        "name": "Krithika Iyer"
                    },
                    {
                        "authorId": "2279924280",
                        "name": "Kshitiz Malik"
                    },
                    {
                        "authorId": "2313925316",
                        "name": "Kuen-ley Chiu"
                    },
                    {
                        "authorId": "2313913532",
                        "name": "Kunal Bhalla"
                    },
                    {
                        "authorId": "2313915935",
                        "name": "Lauren Rantala-Yeary"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "2314080073",
                        "name": "Lawrence Chen"
                    },
                    {
                        "authorId": "2313924605",
                        "name": "Liang Tan"
                    },
                    {
                        "authorId": "2313918409",
                        "name": "Liz Jenkins"
                    },
                    {
                        "authorId": "2249724552",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "151093281",
                        "name": "Lovish Madaan"
                    },
                    {
                        "authorId": "2313912794",
                        "name": "Lubo Malo"
                    },
                    {
                        "authorId": "2040305955",
                        "name": "Lukas Blecher"
                    },
                    {
                        "authorId": "2313925619",
                        "name": "Lukas Landzaat"
                    },
                    {
                        "authorId": "2314194315",
                        "name": "Luke de Oliveira"
                    },
                    {
                        "authorId": "2000839712",
                        "name": "Madeline Muzzi"
                    },
                    {
                        "authorId": "2047114741",
                        "name": "M. Pasupuleti"
                    },
                    {
                        "authorId": "152964870",
                        "name": "Mannat Singh"
                    },
                    {
                        "authorId": "2210374",
                        "name": "Manohar Paluri"
                    },
                    {
                        "authorId": "2059886128",
                        "name": "Marcin Kardas"
                    },
                    {
                        "authorId": "2313909379",
                        "name": "Mathew Oldham"
                    },
                    {
                        "authorId": "2313912870",
                        "name": "Mathieu Rita"
                    },
                    {
                        "authorId": "2313905186",
                        "name": "Maya Pavlova"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "M. Kambadur"
                    },
                    {
                        "authorId": "2247796743",
                        "name": "Mike Lewis"
                    },
                    {
                        "authorId": "2310234768",
                        "name": "Min Si"
                    },
                    {
                        "authorId": "2247874378",
                        "name": "Mitesh Kumar Singh"
                    },
                    {
                        "authorId": "2314434867",
                        "name": "Mona Hassan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2911626",
                        "name": "Narjes Torabi"
                    },
                    {
                        "authorId": "2223756247",
                        "name": "Nikolay Bashlykov"
                    },
                    {
                        "authorId": "3444222",
                        "name": "Nikolay Bogoychev"
                    },
                    {
                        "authorId": "22193324",
                        "name": "Niladri S. Chatterji"
                    },
                    {
                        "authorId": "2096643450",
                        "name": "Olivier Duchenne"
                    },
                    {
                        "authorId": "2166310112",
                        "name": "Onur cCelebi"
                    },
                    {
                        "authorId": "2037772368",
                        "name": "Patrick Alrassy"
                    },
                    {
                        "authorId": "2257643167",
                        "name": "Pengchuan Zhang"
                    },
                    {
                        "authorId": "2273574279",
                        "name": "Pengwei Li"
                    },
                    {
                        "authorId": "2268221163",
                        "name": "Petar Vasi\u0107"
                    },
                    {
                        "authorId": "2313915931",
                        "name": "Peter Weng"
                    },
                    {
                        "authorId": "51229603",
                        "name": "Prajjwal Bhargava"
                    },
                    {
                        "authorId": "46175439",
                        "name": "Pratik Dubal"
                    },
                    {
                        "authorId": "2304450089",
                        "name": "Praveen Krishnan"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "2214843767",
                        "name": "Puxin Xu"
                    },
                    {
                        "authorId": "2314186827",
                        "name": "Qing He"
                    },
                    {
                        "authorId": "2313912601",
                        "name": "Qingxiao Dong"
                    },
                    {
                        "authorId": "2313910187",
                        "name": "Ragavan Srinivasan"
                    },
                    {
                        "authorId": "2313925467",
                        "name": "Raj Ganapathy"
                    },
                    {
                        "authorId": "98804036",
                        "name": "Ramon Calderer"
                    },
                    {
                        "authorId": "2313909428",
                        "name": "Ricardo Silveira Cabral"
                    },
                    {
                        "authorId": "1962768",
                        "name": "Robert Stojnic"
                    },
                    {
                        "authorId": "48647153",
                        "name": "Roberta Raileanu"
                    },
                    {
                        "authorId": "3102850",
                        "name": "Rohit Girdhar"
                    },
                    {
                        "authorId": "2313913363",
                        "name": "Rohit Patel"
                    },
                    {
                        "authorId": "2007285239",
                        "name": "R. Sauvestre"
                    },
                    {
                        "authorId": "2313925210",
                        "name": "Ronnie Polidoro"
                    },
                    {
                        "authorId": "1722889",
                        "name": "Roshan Sumbaly"
                    },
                    {
                        "authorId": "2110697298",
                        "name": "Ross Taylor"
                    },
                    {
                        "authorId": "2214818043",
                        "name": "Ruan Silva"
                    },
                    {
                        "authorId": "2266467782",
                        "name": "Rui Hou"
                    },
                    {
                        "authorId": "2248766592",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2268759462",
                        "name": "S. Hosseini"
                    },
                    {
                        "authorId": "2273416143",
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "authorId": "2313985129",
                        "name": "Sanjay Singh"
                    },
                    {
                        "authorId": "2277511475",
                        "name": "Sean Bell"
                    },
                    {
                        "authorId": "2281792543",
                        "name": "Seohyun Sonia Kim"
                    },
                    {
                        "authorId": "2068070",
                        "name": "Sergey Edunov"
                    },
                    {
                        "authorId": "35557488",
                        "name": "Shaoliang Nie"
                    },
                    {
                        "authorId": "46617804",
                        "name": "Sharan Narang"
                    },
                    {
                        "authorId": "1498636613",
                        "name": "S. Raparthy"
                    },
                    {
                        "authorId": "2191455",
                        "name": "Sheng Shen"
                    },
                    {
                        "authorId": "2272846244",
                        "name": "Shengye Wan"
                    },
                    {
                        "authorId": "2116473",
                        "name": "Shruti Bhosale"
                    },
                    {
                        "authorId": "2314071119",
                        "name": "Shun Zhang"
                    },
                    {
                        "authorId": "83754395",
                        "name": "Simon Vandenhende"
                    },
                    {
                        "authorId": "47505161",
                        "name": "Soumya Batra"
                    },
                    {
                        "authorId": "2273415395",
                        "name": "Spencer Whitman"
                    },
                    {
                        "authorId": "31460313",
                        "name": "Sten Sootla"
                    },
                    {
                        "authorId": "2313909594",
                        "name": "Stephane Collot"
                    },
                    {
                        "authorId": "40895369",
                        "name": "Suchin Gururangan"
                    },
                    {
                        "authorId": "148016419",
                        "name": "S. Borodinsky"
                    },
                    {
                        "authorId": "2313925454",
                        "name": "Tamar Herman"
                    },
                    {
                        "authorId": "2313918585",
                        "name": "Tara Fowler"
                    },
                    {
                        "authorId": "2313917558",
                        "name": "Tarek Sheasha"
                    },
                    {
                        "authorId": "2313910328",
                        "name": "Thomas Georgiou"
                    },
                    {
                        "authorId": "2073456043",
                        "name": "Thomas Scialom"
                    },
                    {
                        "authorId": "2313915815",
                        "name": "Tobias Speckbacher"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "2313914277",
                        "name": "Tong Xiao"
                    },
                    {
                        "authorId": "46907106",
                        "name": "Ujjwal Karn"
                    },
                    {
                        "authorId": "28554843",
                        "name": "Vedanuj Goswami"
                    },
                    {
                        "authorId": "2314332514",
                        "name": "Vibhor Gupta"
                    },
                    {
                        "authorId": "34066479",
                        "name": "Vignesh Ramanathan"
                    },
                    {
                        "authorId": "2190957318",
                        "name": "Viktor Kerkez"
                    },
                    {
                        "authorId": "2313913380",
                        "name": "Vincent Gonguet"
                    },
                    {
                        "authorId": "2313918349",
                        "name": "Virginie Do"
                    },
                    {
                        "authorId": "2232955561",
                        "name": "Vish Vogeti"
                    },
                    {
                        "authorId": "2162195471",
                        "name": "Vladan Petrovic"
                    },
                    {
                        "authorId": "2266751414",
                        "name": "Weiwei Chu"
                    },
                    {
                        "authorId": "2290750668",
                        "name": "Wenhan Xiong"
                    },
                    {
                        "authorId": "2223742000",
                        "name": "Wenyin Fu"
                    },
                    {
                        "authorId": "2313913371",
                        "name": "Whit-ney Meers"
                    },
                    {
                        "authorId": "1490887583",
                        "name": "Xavier Martinet"
                    },
                    {
                        "authorId": "2297930724",
                        "name": "Xiaodong Wang"
                    },
                    {
                        "authorId": "2249851858",
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "authorId": "2285798957",
                        "name": "Xinfeng Xie"
                    },
                    {
                        "authorId": "2313910170",
                        "name": "Xuchao Jia"
                    },
                    {
                        "authorId": "2314067352",
                        "name": "Xuewei Wang"
                    },
                    {
                        "authorId": "1404341450",
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "authorId": "2286511206",
                        "name": "Yashesh Gaur"
                    },
                    {
                        "authorId": "2223764353",
                        "name": "Yasmine Babaei"
                    },
                    {
                        "authorId": "148416622",
                        "name": "Yiqian Wen"
                    },
                    {
                        "authorId": "2314381758",
                        "name": "Yiwen Song"
                    },
                    {
                        "authorId": "2108473229",
                        "name": "Yuchen Zhang"
                    },
                    {
                        "authorId": "2297839847",
                        "name": "Yue Li"
                    },
                    {
                        "authorId": "2272672481",
                        "name": "Yuning Mao"
                    },
                    {
                        "authorId": "2297187212",
                        "name": "Zacharie Delpierre Coudert"
                    },
                    {
                        "authorId": "2293992938",
                        "name": "Zhengxu Yan"
                    },
                    {
                        "authorId": "2266490735",
                        "name": "Zhengxing Chen"
                    },
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "2306863572",
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "authorId": "2233294011",
                        "name": "Aaron Grattafiori"
                    },
                    {
                        "authorId": "2312019070",
                        "name": "Abha Jain"
                    },
                    {
                        "authorId": "2313915967",
                        "name": "Adam Kelsey"
                    },
                    {
                        "authorId": "116814432",
                        "name": "Adam Shajnfeld"
                    },
                    {
                        "authorId": "2077604116",
                        "name": "Adi Gangidi"
                    },
                    {
                        "authorId": "2313910256",
                        "name": "Adolfo Victoria"
                    },
                    {
                        "authorId": "2313913221",
                        "name": "Ahuva Goldstand"
                    },
                    {
                        "authorId": "2313925780",
                        "name": "Ajay Menon"
                    },
                    {
                        "authorId": "2314067298",
                        "name": "Ajay Sharma"
                    },
                    {
                        "authorId": "2313915843",
                        "name": "Alex Boesenberg"
                    },
                    {
                        "authorId": "2313910116",
                        "name": "Alex Vaughan"
                    },
                    {
                        "authorId": "14667698",
                        "name": "Alexei Baevski"
                    },
                    {
                        "authorId": "2313918472",
                        "name": "Allie Feinstein"
                    },
                    {
                        "authorId": "122882087",
                        "name": "A. Kallet"
                    },
                    {
                        "authorId": "2313918666",
                        "name": "Amit Sangani"
                    },
                    {
                        "authorId": "2313925473",
                        "name": "Anam Yunus"
                    },
                    {
                        "authorId": "2266838640",
                        "name": "Andrei Lupu"
                    },
                    {
                        "authorId": "2243192949",
                        "name": "Andres Alvarado"
                    },
                    {
                        "authorId": "2313925570",
                        "name": "Andrew Caples"
                    },
                    {
                        "authorId": "2313913152",
                        "name": "Andrew Gu"
                    },
                    {
                        "authorId": "2313919243",
                        "name": "Andrew Ho"
                    },
                    {
                        "authorId": "2282542314",
                        "name": "Andrew Poulton"
                    },
                    {
                        "authorId": "2313909933",
                        "name": "Andrew Ryan"
                    },
                    {
                        "authorId": "1453469113",
                        "name": "Ankit Ramchandani"
                    },
                    {
                        "authorId": "2313918528",
                        "name": "Annie Franco"
                    },
                    {
                        "authorId": "51912276",
                        "name": "Aparajita Saraf"
                    },
                    {
                        "authorId": "2313917455",
                        "name": "Arkabandhu Chowdhury"
                    },
                    {
                        "authorId": "2313925699",
                        "name": "Ashley Gabriel"
                    },
                    {
                        "authorId": "2313909987",
                        "name": "Ashwin Bharambe"
                    },
                    {
                        "authorId": "35198582",
                        "name": "Assaf Eisenman"
                    },
                    {
                        "authorId": "2313915928",
                        "name": "Azadeh Yazdan"
                    },
                    {
                        "authorId": "2313918606",
                        "name": "Beau James"
                    },
                    {
                        "authorId": "2313913272",
                        "name": "Ben Maurer"
                    },
                    {
                        "authorId": "2897362",
                        "name": "Ben Leonhardi"
                    },
                    {
                        "authorId": "2319973",
                        "name": "Po-Yao (Bernie) Huang"
                    },
                    {
                        "authorId": "2313918673",
                        "name": "Beth Loyd"
                    },
                    {
                        "authorId": "2313909983",
                        "name": "Beto De Paola"
                    },
                    {
                        "authorId": "8005713",
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "authorId": "2314014642",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "2314069142",
                        "name": "Bo Wu"
                    },
                    {
                        "authorId": "2313909094",
                        "name": "Boyu Ni"
                    },
                    {
                        "authorId": "2313916282",
                        "name": "Braden Hancock"
                    },
                    {
                        "authorId": "46240090",
                        "name": "Bram Wasti"
                    },
                    {
                        "authorId": "2313918213",
                        "name": "Brandon Spence"
                    },
                    {
                        "authorId": "2313918219",
                        "name": "Brani Stojkovic"
                    },
                    {
                        "authorId": "2313925572",
                        "name": "Brian Gamido"
                    },
                    {
                        "authorId": "2313917587",
                        "name": "Britt Montalvo"
                    },
                    {
                        "authorId": "2313919256",
                        "name": "Carl Parker"
                    },
                    {
                        "authorId": "2313913658",
                        "name": "Carly Burton"
                    },
                    {
                        "authorId": "2313917281",
                        "name": "Catalina Mejia"
                    },
                    {
                        "authorId": "2313925537",
                        "name": "Changhan Wang"
                    },
                    {
                        "authorId": "2314071777",
                        "name": "Changkyu Kim"
                    },
                    {
                        "authorId": "2314072280",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2313982652",
                        "name": "Chester Hu"
                    },
                    {
                        "authorId": "2290129157",
                        "name": "Ching-Hsiang Chu"
                    },
                    {
                        "authorId": "2263867885",
                        "name": "Chris Cai"
                    },
                    {
                        "authorId": "2313925773",
                        "name": "Chris Tindal"
                    },
                    {
                        "authorId": "2322150",
                        "name": "Christoph Feichtenhofer"
                    },
                    {
                        "authorId": "50986776",
                        "name": "Damon Civin"
                    },
                    {
                        "authorId": "2313913237",
                        "name": "Dana Beaty"
                    },
                    {
                        "authorId": "3046707",
                        "name": "Daniel Kreymer"
                    },
                    {
                        "authorId": "2530311",
                        "name": "Shang-Wen Li"
                    },
                    {
                        "authorId": "2313916159",
                        "name": "Danny Wyatt"
                    },
                    {
                        "authorId": "2161835643",
                        "name": "David Adkins"
                    },
                    {
                        "authorId": "2313915190",
                        "name": "David Xu"
                    },
                    {
                        "authorId": "2273657478",
                        "name": "Davide Testuggine"
                    },
                    {
                        "authorId": "2311498203",
                        "name": "Delia David"
                    },
                    {
                        "authorId": "2248278031",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "2145259939",
                        "name": "Diana Liskovich"
                    },
                    {
                        "authorId": "2313925798",
                        "name": "Didem Foss"
                    },
                    {
                        "authorId": "2283843884",
                        "name": "Dingkang Wang"
                    },
                    {
                        "authorId": "145267619",
                        "name": "Duc Le"
                    },
                    {
                        "authorId": "2313913567",
                        "name": "Dustin Holland"
                    },
                    {
                        "authorId": "2313916034",
                        "name": "Edward Dowling"
                    },
                    {
                        "authorId": "2313916009",
                        "name": "Eissa Jamil"
                    },
                    {
                        "authorId": "2313925401",
                        "name": "Elaine Montgomery"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2313914699",
                        "name": "Emily Hahn"
                    },
                    {
                        "authorId": "2313913986",
                        "name": "Emily Wood"
                    },
                    {
                        "authorId": "2313913160",
                        "name": "Erik Brinkman"
                    },
                    {
                        "authorId": "2064373270",
                        "name": "Esteban Arcaute"
                    },
                    {
                        "authorId": "2313915853",
                        "name": "Evan Dunbar"
                    },
                    {
                        "authorId": "2313918562",
                        "name": "Evan Smothers"
                    },
                    {
                        "authorId": "2314197755",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "32653170",
                        "name": "Felix Kreuk"
                    },
                    {
                        "authorId": "2313907929",
                        "name": "Feng Tian"
                    },
                    {
                        "authorId": "2160885118",
                        "name": "Firat Ozgenel"
                    },
                    {
                        "authorId": "31292058",
                        "name": "Francesco Caggioni"
                    },
                    {
                        "authorId": "2061585840",
                        "name": "Francisco Guzm'an"
                    },
                    {
                        "authorId": "3360115",
                        "name": "Frank J. Kanayet"
                    },
                    {
                        "authorId": "2243280567",
                        "name": "Frank Seide"
                    },
                    {
                        "authorId": "2313913137",
                        "name": "Gabriela Medina Florez"
                    },
                    {
                        "authorId": "2313925846",
                        "name": "Gabriella Schwarz"
                    },
                    {
                        "authorId": "2313918570",
                        "name": "Gada Badeer"
                    },
                    {
                        "authorId": "2313916006",
                        "name": "Georgia Swee"
                    },
                    {
                        "authorId": "2313925677",
                        "name": "Gil Halpern"
                    },
                    {
                        "authorId": "2028300167",
                        "name": "G. Thattai"
                    },
                    {
                        "authorId": "2313918648",
                        "name": "Grant Herman"
                    },
                    {
                        "authorId": "2266304177",
                        "name": "G. Sizov"
                    },
                    {
                        "authorId": "47776500",
                        "name": "Guangyi Zhang"
                    },
                    {
                        "authorId": "2289832027",
                        "name": "Guna Lakshminarayanan"
                    },
                    {
                        "authorId": "2343773236",
                        "name": "Hamid Shojanazeri"
                    },
                    {
                        "authorId": "2313908554",
                        "name": "Han Zou"
                    },
                    {
                        "authorId": "2314725059",
                        "name": "Hannah Wang"
                    },
                    {
                        "authorId": "2261827291",
                        "name": "Han Zha"
                    },
                    {
                        "authorId": "30279076",
                        "name": "Haroun Habeeb"
                    },
                    {
                        "authorId": "2313913215",
                        "name": "Harrison Rudolph"
                    },
                    {
                        "authorId": "2297942583",
                        "name": "Helen Suk"
                    },
                    {
                        "authorId": "87085411",
                        "name": "Henry Aspegren"
                    },
                    {
                        "authorId": "2313910892",
                        "name": "Hunter Goldman"
                    },
                    {
                        "authorId": "2322981055",
                        "name": "Igor Molybog"
                    },
                    {
                        "authorId": "2032201719",
                        "name": "Igor Tufanov"
                    },
                    {
                        "authorId": "2127473751",
                        "name": "Irina-Elena Veliche"
                    },
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "2313913125",
                        "name": "Jake Weissman"
                    },
                    {
                        "authorId": "2313913133",
                        "name": "James Geboski"
                    },
                    {
                        "authorId": "2313917982",
                        "name": "James Kohli"
                    },
                    {
                        "authorId": "2313909751",
                        "name": "Japhet Asher"
                    },
                    {
                        "authorId": "2131867883",
                        "name": "Jean-Baptiste Gaya"
                    },
                    {
                        "authorId": "2313917933",
                        "name": "Jeff Marcus"
                    },
                    {
                        "authorId": "2314079720",
                        "name": "Jeff Tang"
                    },
                    {
                        "authorId": "2313924089",
                        "name": "Jennifer Chan"
                    },
                    {
                        "authorId": "2313906372",
                        "name": "Jenny Zhen"
                    },
                    {
                        "authorId": "39906022",
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "authorId": "2313925697",
                        "name": "Jeremy Teboul"
                    },
                    {
                        "authorId": "2314712137",
                        "name": "Jessica Zhong"
                    },
                    {
                        "authorId": "2314696266",
                        "name": "Jian Jin"
                    },
                    {
                        "authorId": "2314170063",
                        "name": "Jingyi Yang"
                    },
                    {
                        "authorId": "2313921009",
                        "name": "Joe Cummings"
                    },
                    {
                        "authorId": "2313916920",
                        "name": "Jon Carvill"
                    },
                    {
                        "authorId": "2313918017",
                        "name": "Jon Shepard"
                    },
                    {
                        "authorId": "2313925667",
                        "name": "Jonathan McPhie"
                    },
                    {
                        "authorId": "2314554743",
                        "name": "Jonathan Torres"
                    },
                    {
                        "authorId": "2313925786",
                        "name": "Josh Ginsburg"
                    },
                    {
                        "authorId": "2314072216",
                        "name": "Junjie Wang"
                    },
                    {
                        "authorId": "2115598555",
                        "name": "Kaixing(Kai) Wu"
                    },
                    {
                        "authorId": "2313913073",
                        "name": "U. KamHou"
                    },
                    {
                        "authorId": "2313917036",
                        "name": "Karan Saxena"
                    },
                    {
                        "authorId": "2313913208",
                        "name": "Karthik Prasad"
                    },
                    {
                        "authorId": "40267343",
                        "name": "Kartikay Khandelwal"
                    },
                    {
                        "authorId": "2158995926",
                        "name": "Katayoun Zand"
                    },
                    {
                        "authorId": "2313926373",
                        "name": "Kathy Matosich"
                    },
                    {
                        "authorId": "2262920209",
                        "name": "K. Veeraraghavan"
                    },
                    {
                        "authorId": "2313925800",
                        "name": "Kelly Michelena"
                    },
                    {
                        "authorId": "2313920868",
                        "name": "Keqian Li"
                    },
                    {
                        "authorId": "2314883034",
                        "name": "Kun Huang"
                    },
                    {
                        "authorId": "2313918835",
                        "name": "Kunal Chawla"
                    },
                    {
                        "authorId": "1410624139",
                        "name": "Kushal Lakhotia"
                    },
                    {
                        "authorId": "2314883036",
                        "name": "Kyle Huang"
                    },
                    {
                        "authorId": "2197533966",
                        "name": "Lailin Chen"
                    },
                    {
                        "authorId": "2313913092",
                        "name": "Lakshya Garg"
                    },
                    {
                        "authorId": "2313926370",
                        "name": "A. Lavender"
                    },
                    {
                        "authorId": "2314073913",
                        "name": "Leandro Silva"
                    },
                    {
                        "authorId": "2313926731",
                        "name": "Lee Bell"
                    },
                    {
                        "authorId": "2313951052",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2314460078",
                        "name": "Liangpeng Guo"
                    },
                    {
                        "authorId": "2269696579",
                        "name": "Licheng Yu"
                    },
                    {
                        "authorId": "2313918301",
                        "name": "Liron Moshkovich"
                    },
                    {
                        "authorId": "2331511165",
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "authorId": "2072010",
                        "name": "Madian Khabsa"
                    },
                    {
                        "authorId": "2313918577",
                        "name": "Manav Avalani"
                    },
                    {
                        "authorId": "2273002871",
                        "name": "Manish Bhatt"
                    },
                    {
                        "authorId": "2010057",
                        "name": "M. Tsimpoukelli"
                    },
                    {
                        "authorId": "98800979",
                        "name": "Martynas Mankus"
                    },
                    {
                        "authorId": "2093466943",
                        "name": "Matan Hasson"
                    },
                    {
                        "authorId": "83174287",
                        "name": "M. Lennie"
                    },
                    {
                        "authorId": "2248340971",
                        "name": "Matthias Reso"
                    },
                    {
                        "authorId": "2313918566",
                        "name": "Maxim Groshev"
                    },
                    {
                        "authorId": "2290016941",
                        "name": "Maxim Naumov"
                    },
                    {
                        "authorId": "52097509",
                        "name": "Maya Lathi"
                    },
                    {
                        "authorId": "2313926376",
                        "name": "Meghan Keneally"
                    },
                    {
                        "authorId": "1727524",
                        "name": "M. Seltzer"
                    },
                    {
                        "authorId": "2259912893",
                        "name": "Michal Valko"
                    },
                    {
                        "authorId": "2313917797",
                        "name": "Michelle Restrepo"
                    },
                    {
                        "authorId": "2314105463",
                        "name": "Mihir Patel"
                    },
                    {
                        "authorId": "2313909990",
                        "name": "Mik Vyatskov"
                    },
                    {
                        "authorId": "49089678",
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "authorId": "2314111844",
                        "name": "Mike Clark"
                    },
                    {
                        "authorId": "2313910746",
                        "name": "Mike Macey"
                    },
                    {
                        "authorId": "2314078208",
                        "name": "Mike Wang"
                    },
                    {
                        "authorId": "147487949",
                        "name": "Miquel Jubert Hermoso"
                    },
                    {
                        "authorId": "2313913313",
                        "name": "Mo Metanat"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "authorId": "2313910172",
                        "name": "Munish Bansal"
                    },
                    {
                        "authorId": "2265554054",
                        "name": "Nandhini Santhanam"
                    },
                    {
                        "authorId": "2313916856",
                        "name": "Natascha Parks"
                    },
                    {
                        "authorId": "2313910535",
                        "name": "Natasha White"
                    },
                    {
                        "authorId": "2313918859",
                        "name": "Navyata Bawa"
                    },
                    {
                        "authorId": "40943290",
                        "name": "Nayan Singhal"
                    },
                    {
                        "authorId": "2313909893",
                        "name": "Nick Egebo"
                    },
                    {
                        "authorId": "1746841",
                        "name": "Nicolas Usunier"
                    },
                    {
                        "authorId": "2285597551",
                        "name": "Nikolay Pavlovich Laptev"
                    },
                    {
                        "authorId": "2313910396",
                        "name": "Ning Dong"
                    },
                    {
                        "authorId": "2314687456",
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2313916655",
                        "name": "Norman Cheng"
                    },
                    {
                        "authorId": "1405690366",
                        "name": "Oleg Chernoguz"
                    },
                    {
                        "authorId": "2313918980",
                        "name": "Olivia Hart"
                    },
                    {
                        "authorId": "1778909324",
                        "name": "Omkar Salpekar"
                    },
                    {
                        "authorId": "1729960",
                        "name": "Ozlem Kalinli"
                    },
                    {
                        "authorId": "2313916098",
                        "name": "Parkin Kent"
                    },
                    {
                        "authorId": "2313909999",
                        "name": "Parth Parekh"
                    },
                    {
                        "authorId": "2313915995",
                        "name": "Paul Saab"
                    },
                    {
                        "authorId": "2307470796",
                        "name": "Pavan Balaji"
                    },
                    {
                        "authorId": "31915793",
                        "name": "Pedro Rittner"
                    },
                    {
                        "authorId": "14171685",
                        "name": "Philip Bontrager"
                    },
                    {
                        "authorId": "2313919367",
                        "name": "Pierre Roux"
                    },
                    {
                        "authorId": "2257254817",
                        "name": "Piotr Doll\u00e1r"
                    },
                    {
                        "authorId": "2163709899",
                        "name": "Polina Zvyagina"
                    },
                    {
                        "authorId": "2459737",
                        "name": "Prashant Ratanchandani"
                    },
                    {
                        "authorId": "41020300",
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "authorId": "2313916229",
                        "name": "Qian Liang"
                    },
                    {
                        "authorId": "2313909804",
                        "name": "Rachad Alao"
                    },
                    {
                        "authorId": "2313934481",
                        "name": "Rachel Rodriguez"
                    },
                    {
                        "authorId": "14714641",
                        "name": "Rafi Ayub"
                    },
                    {
                        "authorId": "2313909891",
                        "name": "Raghotham Murthy"
                    },
                    {
                        "authorId": "2313918294",
                        "name": "Raghu Nayani"
                    },
                    {
                        "authorId": "2264459586",
                        "name": "Rahul Mitra"
                    },
                    {
                        "authorId": "2313919671",
                        "name": "Raymond Li"
                    },
                    {
                        "authorId": "2313918488",
                        "name": "Rebekkah Hogan"
                    },
                    {
                        "authorId": "2313913081",
                        "name": "Robin Battey"
                    },
                    {
                        "authorId": "46886013",
                        "name": "Rocky Wang"
                    },
                    {
                        "authorId": "2313918943",
                        "name": "Rohan Maheswari"
                    },
                    {
                        "authorId": "1410913697",
                        "name": "Russ Howes"
                    },
                    {
                        "authorId": "1905713",
                        "name": "Ruty Rinott"
                    },
                    {
                        "authorId": "2313916859",
                        "name": "Sai Jayesh Bondu"
                    },
                    {
                        "authorId": "19200118",
                        "name": "Samyak Datta"
                    },
                    {
                        "authorId": "2313913104",
                        "name": "Sara Chugh"
                    },
                    {
                        "authorId": "2313916525",
                        "name": "Sara Hunt"
                    },
                    {
                        "authorId": "2313919311",
                        "name": "Sargun Dhillon"
                    },
                    {
                        "authorId": "2313918976",
                        "name": "Sasha Sidorov"
                    },
                    {
                        "authorId": "2314108295",
                        "name": "Satadru Pan"
                    },
                    {
                        "authorId": "2314005184",
                        "name": "Saurabh Verma"
                    },
                    {
                        "authorId": "2314406059",
                        "name": "Seiji Yamamoto"
                    },
                    {
                        "authorId": "48347720",
                        "name": "Sharadh Ramaswamy"
                    },
                    {
                        "authorId": "2313926214",
                        "name": "Shaun Lindsay"
                    },
                    {
                        "authorId": "2314693028",
                        "name": "Sheng Feng"
                    },
                    {
                        "authorId": "2311565327",
                        "name": "Shenghao Lin"
                    },
                    {
                        "authorId": "2268757277",
                        "name": "S. Zha"
                    },
                    {
                        "authorId": "2313916766",
                        "name": "Shiva Shankar"
                    },
                    {
                        "authorId": "2237076180",
                        "name": "Shuqiang Zhang"
                    },
                    {
                        "authorId": "2237101143",
                        "name": "Sinong Wang"
                    },
                    {
                        "authorId": "50230355",
                        "name": "Sneha Agarwal"
                    },
                    {
                        "authorId": "2423558",
                        "name": "S. Sajuyigbe"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "2313919129",
                        "name": "Stephanie Max"
                    },
                    {
                        "authorId": "2125208891",
                        "name": "Stephen Chen"
                    },
                    {
                        "authorId": "2313926357",
                        "name": "Steve Kehoe"
                    },
                    {
                        "authorId": "2313909787",
                        "name": "Steve Satterfield"
                    },
                    {
                        "authorId": "2313918283",
                        "name": "Sudarshan Govindaprasad"
                    },
                    {
                        "authorId": "2157683980",
                        "name": "Sumit Gupta"
                    },
                    {
                        "authorId": "2286537482",
                        "name": "Sung-Bae Cho"
                    },
                    {
                        "authorId": "2313919117",
                        "name": "Sunny Virk"
                    },
                    {
                        "authorId": "2313914940",
                        "name": "Suraj Subramanian"
                    },
                    {
                        "authorId": "89754631",
                        "name": "Sy Choudhury"
                    },
                    {
                        "authorId": "2313908725",
                        "name": "Sydney Goldman"
                    },
                    {
                        "authorId": "2211633",
                        "name": "T. Remez"
                    },
                    {
                        "authorId": "2071335303",
                        "name": "Tamar Glaser"
                    },
                    {
                        "authorId": "2313910221",
                        "name": "Tamara Best"
                    },
                    {
                        "authorId": "2189305275",
                        "name": "Thilo Kohler"
                    },
                    {
                        "authorId": "2313919760",
                        "name": "Thomas Robinson"
                    },
                    {
                        "authorId": "2314332791",
                        "name": "Tianhe Li"
                    },
                    {
                        "authorId": "1993655237",
                        "name": "Tianjun Zhang"
                    },
                    {
                        "authorId": "2313919132",
                        "name": "Tim Matthews"
                    },
                    {
                        "authorId": "2313919289",
                        "name": "Timothy Chou"
                    },
                    {
                        "authorId": "2313919174",
                        "name": "Tzook Shaked"
                    },
                    {
                        "authorId": "2273415095",
                        "name": "Varun Vontimitta"
                    },
                    {
                        "authorId": "2313918541",
                        "name": "Victoria Ajayi"
                    },
                    {
                        "authorId": "2313910202",
                        "name": "Victoria Montanez"
                    },
                    {
                        "authorId": "2313916470",
                        "name": "Vijai Mohan"
                    },
                    {
                        "authorId": "2314056846",
                        "name": "Vinay Satish Kumar"
                    },
                    {
                        "authorId": "71203676",
                        "name": "Vishal Mangla"
                    },
                    {
                        "authorId": "2313685593",
                        "name": "Vlad Ionescu"
                    },
                    {
                        "authorId": "144386035",
                        "name": "V. Poenaru"
                    },
                    {
                        "authorId": "2051654054",
                        "name": "Vlad T. Mihailescu"
                    },
                    {
                        "authorId": "2313920446",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2293767405",
                        "name": "Wei Li"
                    },
                    {
                        "authorId": "2314069334",
                        "name": "Wenchen Wang"
                    }
                ]
            },
            {
                "paperId": "75cce3867943084f128a50efabbfbf7cffd731f6",
                "externalIds": {
                    "DOI": "10.1038/s41586-024-07522-w",
                    "CorpusId": 270617306,
                    "PubMed": "38898296"
                },
                "corpusId": 270617306,
                "publicationVenue": {
                    "id": "6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                    "name": "Nature",
                    "type": "journal",
                    "issn": "0028-0836",
                    "url": "https://www.nature.com/",
                    "alternate_urls": [
                        "http://www.nature.com/nature/",
                        "https://www.nature.com/nature/",
                        "http://www.nature.com/nature/archive/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75cce3867943084f128a50efabbfbf7cffd731f6",
                "title": "Language is primarily a tool for communication rather than thought.",
                "abstract": null,
                "venue": "Nature",
                "year": 2024,
                "referenceCount": 209,
                "citationCount": 30,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "Review",
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-01",
                "journal": {
                    "volume": "630 8017",
                    "pages": "\n          575-586\n        ",
                    "name": "Nature"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2024LanguageIP,\n author = {Evelina Fedorenko and Steven T Piantadosi and Edward Gibson},\n booktitle = {Nature},\n journal = {Nature},\n pages = {\n          575-586\n        },\n title = {Language is primarily a tool for communication rather than thought.},\n volume = {630 8017},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "2238331992",
                        "name": "Steven T Piantadosi"
                    },
                    {
                        "authorId": "2288316723",
                        "name": "Edward Gibson"
                    }
                ]
            },
            {
                "paperId": "163b4d6a79a5b19af88b8585456363340d9efd04",
                "externalIds": {
                    "ArXiv": "2303.08774",
                    "CorpusId": 257532815
                },
                "corpusId": 257532815,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/163b4d6a79a5b19af88b8585456363340d9efd04",
                "title": "GPT-4 Technical Report",
                "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
                "venue": "",
                "year": 2023,
                "referenceCount": 0,
                "citationCount": 9748,
                "influentialCitationCount": 1454,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": "2023-03-15",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Inproceedings{Achiam2023GPT4TR,\n author = {OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and S. Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and J. Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and B. Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and C. Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and L. Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and C. Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and S. Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and B. Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and I. Kanitscheider and N. Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and J. Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and A. Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and J. Leike and Jade Leung and Daniel Levy and C. Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and A. Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and S. McKinney and C. McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and O. Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and J. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond\u00e9 de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and M. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and F. Such and Natalie Summers and I. Sutskever and Jie Tang and N. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and P. Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},\n title = {GPT-4 Technical Report},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2275249853",
                        "name": "OpenAI Josh Achiam"
                    },
                    {
                        "authorId": "2275250875",
                        "name": "Steven Adler"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2274773568",
                        "name": "Lama Ahmad"
                    },
                    {
                        "authorId": "2258629",
                        "name": "Ilge Akkaya"
                    },
                    {
                        "authorId": "2275244794",
                        "name": "Florencia Leoni Aleman"
                    },
                    {
                        "authorId": "2275252021",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2275252424",
                        "name": "Janko Altenschmidt"
                    },
                    {
                        "authorId": "2275245579",
                        "name": "Sam Altman"
                    },
                    {
                        "authorId": "2275246437",
                        "name": "Shyamal Anadkat"
                    },
                    {
                        "authorId": "2275139370",
                        "name": "Red Avila"
                    },
                    {
                        "authorId": "2256699302",
                        "name": "Igor Babuschkin"
                    },
                    {
                        "authorId": "2054519183",
                        "name": "S. Balaji"
                    },
                    {
                        "authorId": "2275251659",
                        "name": "Valerie Balcom"
                    },
                    {
                        "authorId": "47626612",
                        "name": "Paul Baltescu"
                    },
                    {
                        "authorId": "2275198557",
                        "name": "Haim-ing Bao"
                    },
                    {
                        "authorId": "2275251620",
                        "name": "Mo Bavarian"
                    },
                    {
                        "authorId": "2275245092",
                        "name": "J. Belgum"
                    },
                    {
                        "authorId": "4689792",
                        "name": "Irwan Bello"
                    },
                    {
                        "authorId": "2275245414",
                        "name": "Jake Berdine"
                    },
                    {
                        "authorId": "2275245581",
                        "name": "Gabriel Bernadett-Shapiro"
                    },
                    {
                        "authorId": "133740015",
                        "name": "Christopher Berner"
                    },
                    {
                        "authorId": "2275251674",
                        "name": "Lenny Bogdonoff"
                    },
                    {
                        "authorId": "2275246071",
                        "name": "Oleg Boiko"
                    },
                    {
                        "authorId": "2275248137",
                        "name": "Made-laine Boyd"
                    },
                    {
                        "authorId": "2275245419",
                        "name": "Anna-Luisa Brakman"
                    },
                    {
                        "authorId": "2065151121",
                        "name": "Greg Brockman"
                    },
                    {
                        "authorId": "2275219628",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "35167962",
                        "name": "Miles Brundage"
                    },
                    {
                        "authorId": "2146257251",
                        "name": "Kevin Button"
                    },
                    {
                        "authorId": "2275157286",
                        "name": "Trevor Cai"
                    },
                    {
                        "authorId": "2274782053",
                        "name": "Rosie Campbell"
                    },
                    {
                        "authorId": "2275245404",
                        "name": "Andrew Cann"
                    },
                    {
                        "authorId": "2275246368",
                        "name": "Brittany Carey"
                    },
                    {
                        "authorId": "2275120298",
                        "name": "Chelsea Carlson"
                    },
                    {
                        "authorId": "144114446",
                        "name": "Rory Carmichael"
                    },
                    {
                        "authorId": "1466431052",
                        "name": "Brooke Chan"
                    },
                    {
                        "authorId": "2275545855",
                        "name": "Che Chang"
                    },
                    {
                        "authorId": "2057091285",
                        "name": "Fotis Chantzis"
                    },
                    {
                        "authorId": "2253841704",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2275188918",
                        "name": "Sully Chen"
                    },
                    {
                        "authorId": "2275179180",
                        "name": "Ruby Chen"
                    },
                    {
                        "authorId": "2275289833",
                        "name": "Jason Chen"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "1490681878",
                        "name": "B. Chess"
                    },
                    {
                        "authorId": "2275251158",
                        "name": "Chester Cho"
                    },
                    {
                        "authorId": "2276186593",
                        "name": "Casey Chu"
                    },
                    {
                        "authorId": "2275839391",
                        "name": "Hyung Won Chung"
                    },
                    {
                        "authorId": "2275231534",
                        "name": "Dave Cummings"
                    },
                    {
                        "authorId": "49645091",
                        "name": "Jeremiah Currier"
                    },
                    {
                        "authorId": "2276187456",
                        "name": "Yunxing Dai"
                    },
                    {
                        "authorId": "2275251205",
                        "name": "C. Decareaux"
                    },
                    {
                        "authorId": "2275244920",
                        "name": "Thomas Degry"
                    },
                    {
                        "authorId": "2275247090",
                        "name": "Noah Deutsch"
                    },
                    {
                        "authorId": "2275251200",
                        "name": "Damien Deville"
                    },
                    {
                        "authorId": "2275244298",
                        "name": "Arka Dhar"
                    },
                    {
                        "authorId": "35363891",
                        "name": "David Dohan"
                    },
                    {
                        "authorId": "2275252295",
                        "name": "Steve Dowling"
                    },
                    {
                        "authorId": "2275245491",
                        "name": "Sheila Dunning"
                    },
                    {
                        "authorId": "66821245",
                        "name": "Adrien Ecoffet"
                    },
                    {
                        "authorId": "2275245457",
                        "name": "Atty Eleti"
                    },
                    {
                        "authorId": "2146257131",
                        "name": "Tyna Eloundou"
                    },
                    {
                        "authorId": "2065430571",
                        "name": "David Farhi"
                    },
                    {
                        "authorId": "2096916416",
                        "name": "L. Fedus"
                    },
                    {
                        "authorId": "2275249996",
                        "name": "Niko Felix"
                    },
                    {
                        "authorId": "2275245820",
                        "name": "Sim'on Posada Fishman"
                    },
                    {
                        "authorId": "2275244914",
                        "name": "Juston Forte"
                    },
                    {
                        "authorId": "2275251173",
                        "name": "Is-abella Fulford"
                    },
                    {
                        "authorId": "2027599537",
                        "name": "Leo Gao"
                    },
                    {
                        "authorId": "2275200811",
                        "name": "Elie Georges"
                    },
                    {
                        "authorId": "2275254804",
                        "name": "C. Gibson"
                    },
                    {
                        "authorId": "2275144649",
                        "name": "Vik Goel"
                    },
                    {
                        "authorId": "2325028819",
                        "name": "Tarun Gogineni"
                    },
                    {
                        "authorId": "2261041177",
                        "name": "Gabriel Goh"
                    },
                    {
                        "authorId": "2158366935",
                        "name": "Raphael Gontijo-Lopes"
                    },
                    {
                        "authorId": "2265066144",
                        "name": "Jonathan Gordon"
                    },
                    {
                        "authorId": "2275250003",
                        "name": "Morgan Grafstein"
                    },
                    {
                        "authorId": "145565184",
                        "name": "Scott Gray"
                    },
                    {
                        "authorId": "2275247307",
                        "name": "Ryan Greene"
                    },
                    {
                        "authorId": "2275137274",
                        "name": "Joshua Gross"
                    },
                    {
                        "authorId": "2253699903",
                        "name": "S. Gu"
                    },
                    {
                        "authorId": "2276101257",
                        "name": "Yufei Guo"
                    },
                    {
                        "authorId": "2004021329",
                        "name": "Chris Hallacy"
                    },
                    {
                        "authorId": "2275540338",
                        "name": "Jesse Han"
                    },
                    {
                        "authorId": "2275295848",
                        "name": "Jeff Harris"
                    },
                    {
                        "authorId": "2275226809",
                        "name": "Yuchen He"
                    },
                    {
                        "authorId": "2275245527",
                        "name": "Mike Heaton"
                    },
                    {
                        "authorId": "2151087994",
                        "name": "Jo-hannes Heidecke"
                    },
                    {
                        "authorId": "2242286342",
                        "name": "Chris Hesse"
                    },
                    {
                        "authorId": "2226452668",
                        "name": "Alan Hickey"
                    },
                    {
                        "authorId": "2275246148",
                        "name": "Wade Hickey"
                    },
                    {
                        "authorId": "2275245339",
                        "name": "Peter Hoeschele"
                    },
                    {
                        "authorId": "103681415",
                        "name": "Brandon Houghton"
                    },
                    {
                        "authorId": "2275214107",
                        "name": "Kenny Hsu"
                    },
                    {
                        "authorId": "2275210604",
                        "name": "Shengli Hu"
                    },
                    {
                        "authorId": "2275777049",
                        "name": "Xin Hu"
                    },
                    {
                        "authorId": "39378983",
                        "name": "Joost Huizinga"
                    },
                    {
                        "authorId": "2276187117",
                        "name": "Shantanu Jain"
                    },
                    {
                        "authorId": "2171110177",
                        "name": "Shawn Jain"
                    },
                    {
                        "authorId": "2151094350",
                        "name": "Joanne Jang"
                    },
                    {
                        "authorId": "2253471334",
                        "name": "Angela Jiang"
                    },
                    {
                        "authorId": "2275172062",
                        "name": "Roger Jiang"
                    },
                    {
                        "authorId": "2275752035",
                        "name": "Haozhun Jin"
                    },
                    {
                        "authorId": "2275203081",
                        "name": "Denny Jin"
                    },
                    {
                        "authorId": "2275250083",
                        "name": "Shino Jomoto"
                    },
                    {
                        "authorId": "2275247096",
                        "name": "B. Jonn"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "2403754",
                        "name": "Tomer Kaftan"
                    },
                    {
                        "authorId": "2275230678",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "2275169038",
                        "name": "Ali Kamali"
                    },
                    {
                        "authorId": "3151440",
                        "name": "I. Kanitscheider"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2152264064",
                        "name": "Tabarak Khan"
                    },
                    {
                        "authorId": "2275246102",
                        "name": "Logan Kilpatrick"
                    },
                    {
                        "authorId": "2260346092",
                        "name": "Jong Wook Kim"
                    },
                    {
                        "authorId": "2149054292",
                        "name": "Christina Kim"
                    },
                    {
                        "authorId": "2275296777",
                        "name": "Yongjik Kim"
                    },
                    {
                        "authorId": "2275112980",
                        "name": "Hendrik Kirchner"
                    },
                    {
                        "authorId": "51131802",
                        "name": "J. Kiros"
                    },
                    {
                        "authorId": "2146257375",
                        "name": "Matthew Knight"
                    },
                    {
                        "authorId": "1485556711",
                        "name": "Daniel Kokotajlo"
                    },
                    {
                        "authorId": "2275246094",
                        "name": "Lukasz Kondraciuk"
                    },
                    {
                        "authorId": "1666171360",
                        "name": "A. Kondrich"
                    },
                    {
                        "authorId": "2275252322",
                        "name": "Aris Konstantinidis"
                    },
                    {
                        "authorId": "2275245594",
                        "name": "Kyle Kosic"
                    },
                    {
                        "authorId": "2064404342",
                        "name": "Gretchen Krueger"
                    },
                    {
                        "authorId": "2275229877",
                        "name": "Vishal Kuo"
                    },
                    {
                        "authorId": "2275247085",
                        "name": "Michael Lampe"
                    },
                    {
                        "authorId": "2275246287",
                        "name": "Ikai Lan"
                    },
                    {
                        "authorId": "2274915115",
                        "name": "Teddy Lee"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "52152632",
                        "name": "Jade Leung"
                    },
                    {
                        "authorId": "2275256930",
                        "name": "Daniel Levy"
                    },
                    {
                        "authorId": "2275285124",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2275176375",
                        "name": "Rachel Lim"
                    },
                    {
                        "authorId": "2275759230",
                        "name": "Molly Lin"
                    },
                    {
                        "authorId": "2253840098",
                        "name": "Stephanie Lin"
                    },
                    {
                        "authorId": "1380985420",
                        "name": "Ma-teusz Litwin"
                    },
                    {
                        "authorId": "2275248327",
                        "name": "Theresa Lopez"
                    },
                    {
                        "authorId": "2257272397",
                        "name": "Ryan Lowe"
                    },
                    {
                        "authorId": "2275245628",
                        "name": "Patricia Lue"
                    },
                    {
                        "authorId": "119341078",
                        "name": "A. Makanju"
                    },
                    {
                        "authorId": "2275245649",
                        "name": "Kim Malfacini"
                    },
                    {
                        "authorId": "46430291",
                        "name": "Sam Manning"
                    },
                    {
                        "authorId": "14113256",
                        "name": "Todor Markov"
                    },
                    {
                        "authorId": "2275245336",
                        "name": "Yaniv Markovski"
                    },
                    {
                        "authorId": "2114362965",
                        "name": "Bianca Martin"
                    },
                    {
                        "authorId": "2275231822",
                        "name": "Katie Mayer"
                    },
                    {
                        "authorId": "2275247045",
                        "name": "Andrew Mayne"
                    },
                    {
                        "authorId": "39593364",
                        "name": "Bob McGrew"
                    },
                    {
                        "authorId": "2047820455",
                        "name": "S. McKinney"
                    },
                    {
                        "authorId": "3028785",
                        "name": "C. McLeavey"
                    },
                    {
                        "authorId": "2274772421",
                        "name": "Paul McMillan"
                    },
                    {
                        "authorId": "2275234856",
                        "name": "Jake McNeil"
                    },
                    {
                        "authorId": "2275210659",
                        "name": "David Medina"
                    },
                    {
                        "authorId": "2275132306",
                        "name": "Aalok Mehta"
                    },
                    {
                        "authorId": "10698483",
                        "name": "Jacob Menick"
                    },
                    {
                        "authorId": "2275246330",
                        "name": "Luke Metz"
                    },
                    {
                        "authorId": "2275252694",
                        "name": "Andrey Mishchenko"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2275245453",
                        "name": "Vinnie Monaco"
                    },
                    {
                        "authorId": "1404556973",
                        "name": "Evan Morikawa"
                    },
                    {
                        "authorId": "3407880",
                        "name": "Daniel P. Mossing"
                    },
                    {
                        "authorId": "2275154456",
                        "name": "Tong Mu"
                    },
                    {
                        "authorId": "2117715631",
                        "name": "Mira Murati"
                    },
                    {
                        "authorId": "147746767",
                        "name": "O. Murk"
                    },
                    {
                        "authorId": "2275246116",
                        "name": "David M'ely"
                    },
                    {
                        "authorId": "3422774",
                        "name": "Ashvin Nair"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "2057426488",
                        "name": "Rajeev Nayak"
                    },
                    {
                        "authorId": "2072676",
                        "name": "Arvind Neelakantan"
                    },
                    {
                        "authorId": "2273886618",
                        "name": "Richard Ngo"
                    },
                    {
                        "authorId": "2275115983",
                        "name": "Hyeonwoo Noh"
                    },
                    {
                        "authorId": "2228518120",
                        "name": "Ouyang Long"
                    },
                    {
                        "authorId": "1435765036",
                        "name": "Cullen O'Keefe"
                    },
                    {
                        "authorId": "2713380",
                        "name": "J. Pachocki"
                    },
                    {
                        "authorId": "34800652",
                        "name": "Alex Paino"
                    },
                    {
                        "authorId": "2275244652",
                        "name": "Joe Palermo"
                    },
                    {
                        "authorId": "2275246178",
                        "name": "Ashley Pantuliano"
                    },
                    {
                        "authorId": "50213542",
                        "name": "Giambattista Parascandolo"
                    },
                    {
                        "authorId": "2275245818",
                        "name": "Joel Parish"
                    },
                    {
                        "authorId": "2275245435",
                        "name": "Emy Parparita"
                    },
                    {
                        "authorId": "2274774915",
                        "name": "Alexandre Passos"
                    },
                    {
                        "authorId": "2068123790",
                        "name": "Mikhail Pavlov"
                    },
                    {
                        "authorId": "2275125663",
                        "name": "Andrew Peng"
                    },
                    {
                        "authorId": "2275245529",
                        "name": "Adam Perelman"
                    },
                    {
                        "authorId": "2275250075",
                        "name": "Filipe de Avila Belbute Peres"
                    },
                    {
                        "authorId": "2136008481",
                        "name": "Michael Petrov"
                    },
                    {
                        "authorId": "1463773776",
                        "name": "Henrique Pond\u00e9 de Oliveira Pinto"
                    },
                    {
                        "authorId": "2275246346",
                        "name": "Michael Pokorny"
                    },
                    {
                        "authorId": "2275246814",
                        "name": "Michelle Pokrass"
                    },
                    {
                        "authorId": "144401061",
                        "name": "Vitchyr H. Pong"
                    },
                    {
                        "authorId": "2275150061",
                        "name": "Tolly Powell"
                    },
                    {
                        "authorId": "146162186",
                        "name": "Alethea Power"
                    },
                    {
                        "authorId": "2151088845",
                        "name": "Boris Power"
                    },
                    {
                        "authorId": "2275243930",
                        "name": "Elizabeth Proehl"
                    },
                    {
                        "authorId": "2285654208",
                        "name": "Raul Puri"
                    },
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "2275178294",
                        "name": "Jack W. Rae"
                    },
                    {
                        "authorId": "2261024614",
                        "name": "Aditya Ramesh"
                    },
                    {
                        "authorId": "2275225165",
                        "name": "Cameron Raymond"
                    },
                    {
                        "authorId": "2275252438",
                        "name": "Francis Real"
                    },
                    {
                        "authorId": "2275252095",
                        "name": "Kendra Rimbach"
                    },
                    {
                        "authorId": "2275207240",
                        "name": "Carl Ross"
                    },
                    {
                        "authorId": "11150265",
                        "name": "Bob Rotsted"
                    },
                    {
                        "authorId": "2275250007",
                        "name": "Henri Roussez"
                    },
                    {
                        "authorId": "2260406867",
                        "name": "Nick Ryder"
                    },
                    {
                        "authorId": "47204843",
                        "name": "M. Saltarelli"
                    },
                    {
                        "authorId": "2275246803",
                        "name": "Ted Sanders"
                    },
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "144864359",
                        "name": "Girish Sastry"
                    },
                    {
                        "authorId": "2275265666",
                        "name": "Heather Schmidt"
                    },
                    {
                        "authorId": "2252874293",
                        "name": "David Schnurr"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    },
                    {
                        "authorId": "2196579",
                        "name": "Daniel Selsam"
                    },
                    {
                        "authorId": "2275244711",
                        "name": "Kyla Sheppard"
                    },
                    {
                        "authorId": "102475503",
                        "name": "Toki Sherbakov"
                    },
                    {
                        "authorId": "2275246834",
                        "name": "Jessica Shieh"
                    },
                    {
                        "authorId": "118335789",
                        "name": "Sarah Shoker"
                    },
                    {
                        "authorId": "67311962",
                        "name": "Pranav Shyam"
                    },
                    {
                        "authorId": "2700360",
                        "name": "Szymon Sidor"
                    },
                    {
                        "authorId": "2064673055",
                        "name": "Eric Sigler"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "2275252299",
                        "name": "Jordan Sitkin"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "103422608",
                        "name": "Ian Sohl"
                    },
                    {
                        "authorId": "2901424",
                        "name": "Benjamin Sokolowsky"
                    },
                    {
                        "authorId": "2307592658",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2275245668",
                        "name": "Natalie Staudacher"
                    },
                    {
                        "authorId": "9927844",
                        "name": "F. Such"
                    },
                    {
                        "authorId": "2275252251",
                        "name": "Natalie Summers"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    },
                    {
                        "authorId": "2275750817",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "145950540",
                        "name": "N. Tezak"
                    },
                    {
                        "authorId": "2151289331",
                        "name": "Madeleine Thompson"
                    },
                    {
                        "authorId": "2275252092",
                        "name": "Phil Tillet"
                    },
                    {
                        "authorId": "2267339677",
                        "name": "Amin Tootoonchian"
                    },
                    {
                        "authorId": "2275249879",
                        "name": "Elizabeth Tseng"
                    },
                    {
                        "authorId": "2275249709",
                        "name": "Preston Tuggle"
                    },
                    {
                        "authorId": "2275244171",
                        "name": "Nick Turley"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2275203310",
                        "name": "Juan Felipe Cer'on Uribe"
                    },
                    {
                        "authorId": "2275244586",
                        "name": "Andrea Vallone"
                    },
                    {
                        "authorId": "2275245661",
                        "name": "Arun Vijayvergiya"
                    },
                    {
                        "authorId": "153387869",
                        "name": "Chelsea Voss"
                    },
                    {
                        "authorId": "2275245962",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2275528432",
                        "name": "Justin Jay Wang"
                    },
                    {
                        "authorId": "2275540420",
                        "name": "Alvin Wang"
                    },
                    {
                        "authorId": "2275189326",
                        "name": "Ben Wang"
                    },
                    {
                        "authorId": "2170081200",
                        "name": "Jonathan Ward"
                    },
                    {
                        "authorId": "2253952872",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2275244218",
                        "name": "CJ Weinmann"
                    },
                    {
                        "authorId": "2275245663",
                        "name": "Akila Welihinda"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "2275139180",
                        "name": "Jiayi Weng"
                    },
                    {
                        "authorId": "2065741038",
                        "name": "Lilian Weng"
                    },
                    {
                        "authorId": "2275252154",
                        "name": "Matt Wiethoff"
                    },
                    {
                        "authorId": "2275249733",
                        "name": "Dave Willner"
                    },
                    {
                        "authorId": "2059411355",
                        "name": "Clemens Winter"
                    },
                    {
                        "authorId": "2275244177",
                        "name": "Samuel Wolrich"
                    },
                    {
                        "authorId": "2275225207",
                        "name": "Hannah Wong"
                    },
                    {
                        "authorId": "2275245771",
                        "name": "Lauren Workman"
                    },
                    {
                        "authorId": "2275299848",
                        "name": "Sherwin Wu"
                    },
                    {
                        "authorId": "2274911253",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2307456650",
                        "name": "Michael Wu"
                    },
                    {
                        "authorId": "2275190169",
                        "name": "Kai Xiao"
                    },
                    {
                        "authorId": "2275452480",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "2275310096",
                        "name": "Sarah Yoo"
                    },
                    {
                        "authorId": "2275593618",
                        "name": "Kevin Yu"
                    },
                    {
                        "authorId": "2275194186",
                        "name": "Qim-ing Yuan"
                    },
                    {
                        "authorId": "2563432",
                        "name": "Wojciech Zaremba"
                    },
                    {
                        "authorId": "49629836",
                        "name": "Rowan Zellers"
                    },
                    {
                        "authorId": "2262080679",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "2275288889",
                        "name": "Marvin Zhang"
                    },
                    {
                        "authorId": "2275545682",
                        "name": "Shengjia Zhao"
                    },
                    {
                        "authorId": "2275257857",
                        "name": "Tianhao Zheng"
                    },
                    {
                        "authorId": "2275201537",
                        "name": "Juntang Zhuang"
                    },
                    {
                        "authorId": "2275245715",
                        "name": "William Zhuk"
                    },
                    {
                        "authorId": "2368067",
                        "name": "Barret Zoph"
                    }
                ]
            },
            {
                "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "externalIds": {
                    "ArXiv": "2201.11903",
                    "DBLP": "journals/corr/abs-2201-11903",
                    "CorpusId": 246411621
                },
                "corpusId": 246411621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
                "venue": "Neural Information Processing Systems",
                "year": 2022,
                "referenceCount": 118,
                "citationCount": 6630,
                "influentialCitationCount": 681,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-01-28",
                "journal": {
                    "volume": "abs/2201.11903",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wei2022ChainOT,\n author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},\n volume = {abs/2201.11903},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1524732527",
                        "name": "Xuezhi Wang"
                    },
                    {
                        "authorId": "1714772",
                        "name": "Dale Schuurmans"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2226805",
                        "name": "Ed H. Chi"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            },
            {
                "paperId": "bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "externalIds": {
                    "MAG": "2907706060",
                    "DBLP": "journals/neuroimage/AmalricD19",
                    "DOI": "10.1016/j.neuroimage.2019.01.001",
                    "CorpusId": 58647234,
                    "PubMed": "30611876"
                },
                "corpusId": 58647234,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd06b63ab14c5673be3cfc84865cc5b4f68f4300",
                "title": "A distinct cortical network for mathematical knowledge in the human brain",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2019,
                "referenceCount": 50,
                "citationCount": 84,
                "influentialCitationCount": 4,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://manuscript.elsevier.com/S1053811919300011/pdf/S1053811919300011.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2019-04-01",
                "journal": {
                    "volume": "189",
                    "pages": "19-31",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Amalric2019ADC,\n author = {Marie Amalric and S. Dehaene},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {19-31},\n title = {A distinct cortical network for mathematical knowledge in the human brain},\n volume = {189},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "29877658",
                        "name": "Marie Amalric"
                    },
                    {
                        "authorId": "1787332",
                        "name": "S. Dehaene"
                    }
                ]
            },
            {
                "paperId": "b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "externalIds": {
                    "MAG": "2096462008",
                    "DOI": "10.1073/pnas.1112937108",
                    "CorpusId": 15092714,
                    "PubMed": "21885736"
                },
                "corpusId": 15092714,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b62b60be4fe51188d90a27dd9fc695c924a55b0b",
                "title": "Functional specificity for high-level linguistic processing in the human brain",
                "abstract": "Neuroscientists have debated for centuries whether some regions of the human brain are selectively engaged in specific high-level mental functions or whether, instead, cognition is implemented in multifunctional brain regions. For the critical case of language, conflicting answers arise from the neuropsychological literature, which features striking dissociations between deficits in linguistic and nonlinguistic abilities, vs. the neuroimaging literature, which has argued for overlap between activations for linguistic and nonlinguistic processes, including arithmetic, domain general abilities like cognitive control, and music. Here, we use functional MRI to define classic language regions functionally in each subject individually and then examine the response of these regions to the nonlinguistic functions most commonly argued to engage these regions: arithmetic, working memory, cognitive control, and music. We find little or no response in language regions to these nonlinguistic functions. These data support a clear distinction between language and other cognitive processes, resolving the prior conflict between the neuropsychological and neuroimaging literatures.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2011,
                "referenceCount": 49,
                "citationCount": 472,
                "influentialCitationCount": 28,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://europepmc.org/articles/pmc3182706?pdf=render",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Medicine",
                    "Psychology"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2011-09-01",
                "journal": {
                    "volume": "108",
                    "pages": "16428 - 16433",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Fedorenko2011FunctionalSF,\n author = {Evelina Fedorenko and Michael K. Behr and N. Kanwisher},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {16428 - 16433},\n title = {Functional specificity for high-level linguistic processing in the human brain},\n volume = {108},\n year = {2011}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144733430",
                        "name": "Evelina Fedorenko"
                    },
                    {
                        "authorId": "47971482",
                        "name": "Michael K. Behr"
                    },
                    {
                        "authorId": "1931482",
                        "name": "N. Kanwisher"
                    }
                ]
            },
            {
                "paperId": "2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "externalIds": {
                    "MAG": "2110980037",
                    "DOI": "10.1073/pnas.0902422106",
                    "CorpusId": 5624174,
                    "PubMed": "19617569"
                },
                "corpusId": 5624174,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2dc9e03e59287d7bdde556558dcac380f4b384dd",
                "title": "The boundaries of language and thought in deductive inference",
                "abstract": "Is human thought fully embedded in language, or do some forms of thought operate independently? To directly address this issue, we focus on inference-making, a central feature of human cognition. In a 3T fMRI study we compare logical inferences relying on sentential connectives (e.g., not, or, if \u2026 then) to linguistic inferences based on syntactic transformation of sentences involving ditransitive verbs (e.g., give, say, take). When contrasted with matched grammaticality judgments, logic inference alone recruited \u201ccore\u201d regions of deduction [Brodmann area (BA) 10p and 8m], whereas linguistic inference alone recruited perisylvian regions of linguistic competence, among others (BA 21, 22, 37, 39, 44, and 45 and caudate). In addition, the two inferences commonly recruited a set of general \u201csupport\u201d areas in frontoparietal cortex (BA 6, 7, 8, 40, and 47). The results indicate that logical inference is not embedded in natural language and confirm the relative modularity of linguistic processes.",
                "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                "year": 2009,
                "referenceCount": 64,
                "citationCount": 144,
                "influentialCitationCount": 10,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://www.pnas.org/content/pnas/106/30/12554.full.pdf",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Psychology",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Psychology",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "ClinicalTrial"
                ],
                "publicationDate": "2009-07-28",
                "journal": {
                    "volume": "106",
                    "pages": "12554 - 12559",
                    "name": "Proceedings of the National Academy of Sciences"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2009TheBO,\n author = {M. Monti and L. Parsons and D. Osherson},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {12554 - 12559},\n title = {The boundaries of language and thought in deductive inference},\n volume = {106},\n year = {2009}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    }
                ]
            },
            {
                "paperId": "d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "externalIds": {
                    "MAG": "2097907696",
                    "DBLP": "journals/neuroimage/MontiOMP07",
                    "DOI": "10.1016/j.neuroimage.2007.04.069",
                    "CorpusId": 3361884,
                    "PubMed": "17627851"
                },
                "corpusId": 3361884,
                "publicationVenue": {
                    "id": "fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                    "name": "NeuroImage",
                    "type": "journal",
                    "issn": "1053-8119",
                    "url": "http://www.elsevier.com/locate/ynimg",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622925/description#description",
                        "https://www.journals.elsevier.com/neuroimage",
                        "http://www.sciencedirect.com/science/journal/10538119",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d77ec4deaf9a0b384c30398eb4311bb137a3322a",
                "title": "Functional neuroanatomy of deductive inference: A language-independent distributed network",
                "abstract": null,
                "venue": "NeuroImage",
                "year": 2007,
                "referenceCount": 83,
                "citationCount": 135,
                "influentialCitationCount": 16,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Medicine"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Medicine",
                        "source": "external"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Psychology",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2007-09-01",
                "journal": {
                    "volume": "37",
                    "pages": "1005-1016",
                    "name": "NeuroImage"
                },
                "citationStyles": {
                    "bibtex": "@Article{Monti2007FunctionalNO,\n author = {M. Monti and D. Osherson and Michael J. Martinez and L. Parsons},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {1005-1016},\n title = {Functional neuroanatomy of deductive inference: A language-independent distributed network},\n volume = {37},\n year = {2007}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "1873669",
                        "name": "M. Monti"
                    },
                    {
                        "authorId": "2312012",
                        "name": "D. Osherson"
                    },
                    {
                        "authorId": "39546838",
                        "name": "Michael J. Martinez"
                    },
                    {
                        "authorId": "35053395",
                        "name": "L. Parsons"
                    }
                ]
            },
            {
                "paperId": null,
                "externalIds": null,
                "corpusId": null,
                "publicationVenue": null,
                "url": null,
                "title": "Thought beyond language: neural dissociation of algebra and natural language",
                "abstract": null,
                "venue": "Psychological science",
                "year": 2012,
                "referenceCount": null,
                "citationCount": null,
                "influentialCitationCount": null,
                "isOpenAccess": null,
                "openAccessPdf": null,
                "fieldsOfStudy": null,
                "s2FieldsOfStudy": null,
                "publicationTypes": null,
                "publicationDate": null,
                "journal": null,
                "citationStyles": null,
                "authors": []
            },
            {
                "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "externalIds": {
                    "DBLP": "conf/emnlp/HaoGMHWWH23",
                    "ArXiv": "2305.14992",
                    "DOI": "10.48550/arXiv.2305.14992",
                    "CorpusId": 258865812
                },
                "corpusId": 258865812,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
                "title": "Reasoning with Language Model is Planning with World Model",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2023,
                "referenceCount": 94,
                "citationCount": 362,
                "influentialCitationCount": 45,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.14992",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.14992",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2023ReasoningWL,\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Reasoning with Language Model is Planning with World Model},\n volume = {abs/2305.14992},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2110816708",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2218162745",
                        "name": "Joshua Jiahua Hong"
                    },
                    {
                        "authorId": "47197370",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2111220343",
                        "name": "D. Wang"
                    },
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            }
        ]
    },
    {
        "level": 1,
        "num": 3,
        "title": "Related Work",
        "text": "Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate an intermediate reasoning process in language before outputting the fnal answer. This includes prompting LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains, either with supervised fnetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classifed the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By employing CoT, the efective depth of the transformer increases because the generated outputs are looped back to the input (Feng et al., 2023). These analyses, combined with the established efectiveness of CoT, motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While CoT has proven efective for certain tasks, its autoregressive generation nature makes it challenging to mimic human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al., 2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi et al., 2024; Su et al., 2024). In our analysis, we fnd that after removing the constraint of a language space, a new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.  \nLatent reasoning in LLMs. Previous works mostly defne latent reasoning in LLMs as the hidden computation in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) further proposed to intervene the latent reasoning by \u201cback-patching\u201d the hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a diferent latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable <pause> tokens to the training corpus. This improves LLM\u2019s performance on a variety of tasks, especially when followed by supervised fnetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further explored the usage of fller tokens, e.g., \u201c...\u201d, and concluded that they work well for highly parallelizable problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently, it has also been found that one can \u201cinternalize\u201d the CoT reasoning into latent reasoning in the transformer with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms. These training methods can be combined to our framework, and specifcally, we fnd that breaking down the learning of continuous thoughts into multiple stages, inspired by iCoT (Deng et al., 2024), is very benefcial for the training. Recently, looped transformers (Giannou et al., 2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating latent reasoning in comparison to language space.",
        "lines": [
            {
                "id": 16,
                "line": "Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate an intermediate reasoning process in language before outputting the fnal answer. This includes prompting LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains, either with supervised fnetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classifed the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By employing CoT, the efective depth of the transformer increases because the generated outputs are looped back to the input (Feng et al., 2023). These analyses, combined with the established efectiveness of CoT, motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While CoT has proven efective for certain tasks, its autoregressive generation nature makes it challenging to mimic human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al., 2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi et al., 2024; Su et al., 2024). In our analysis, we fnd that after removing the constraint of a language space, a new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.  "
            },
            {
                "id": 17,
                "line": "Latent reasoning in LLMs. Previous works mostly defne latent reasoning in LLMs as the hidden computation in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) further proposed to intervene the latent reasoning by \u201cback-patching\u201d the hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a diferent latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable <pause> tokens to the training corpus. This improves LLM\u2019s performance on a variety of tasks, especially when followed by supervised fnetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further explored the usage of fller tokens, e.g., \u201c...\u201d, and concluded that they work well for highly parallelizable problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently, it has also been found that one can \u201cinternalize\u201d the CoT reasoning into latent reasoning in the transformer with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms. These training methods can be combined to our framework, and specifcally, we fnd that breaking down the learning of continuous thoughts into multiple stages, inspired by iCoT (Deng et al., 2024), is very benefcial for the training. Recently, looped transformers (Giannou et al., 2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating latent reasoning in comparison to language space.  "
            }
        ],
        "refined_text": "Chain-of-thought (CoT) reasoning. We use the term chain-of-thought broadly to refer to methods that generate an intermediate reasoning process in language before outputting the fnal answer. This includes prompting LLMs (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022), or training LLMs to generate reasoning chains, either with supervised fnetuning (Yue et al., 2023; Yu et al., 2023) or reinforcement learning (Wang et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). Madaan and Yazdanbakhsh (2022) classifed the tokens in CoT into symbols, patterns, and text, and proposed to guide the LLM to generate concise CoT based on analysis of their roles. Recent theoretical analyses have demonstrated the usefulness of CoT from the perspective of model expressivity (Feng et al., 2023; Merrill and Sabharwal, 2023; Li et al., 2024). By employing CoT, the efective depth of the transformer increases because the generated outputs are looped back to the input (Feng et al., 2023). These analyses, combined with the established efectiveness of CoT, motivated our design that feeds the continuous thoughts back to the LLM as the next input embedding. While CoT has proven efective for certain tasks, its autoregressive generation nature makes it challenging to mimic human reasoning on more complex problems (LeCun, 2022; Hao et al., 2023), which typically require planning and search. There are works that equip LLMs with explicit tree search algorithms (Xie et al., 2023; Yao et al., 2023; Hao et al., 2024), or train the LLM on search dynamics and trajectories (Lehnert et al., 2024; Gandhi et al., 2024; Su et al., 2024). In our analysis, we fnd that after removing the constraint of a language space, a new reasoning pattern similar to BFS emerges, even though the model is not explicitly trained in this way.  \nLatent reasoning in LLMs. Previous works mostly defne latent reasoning in LLMs as the hidden computation in transformers (Yang et al., 2024; Biran et al., 2024). Yang et al. (2024) constructed a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) further proposed to intervene the latent reasoning by \u201cback-patching\u201d the hidden representation. Shalev et al. (2024) discovered parallel latent reasoning paths in LLMs. Another line of work has discovered that, even if the model generates a CoT to reason, the model may actually utilize a diferent latent reasoning process. This phenomenon is known as the unfaithfulness of CoT reasoning (Wang et al., 2022; Turpin et al., 2024). To enhance the latent reasoning of LLM, previous research proposed to augment it with additional tokens. Goyal et al. (2023) pretrained the model by randomly inserting a learnable <pause> tokens to the training corpus. This improves LLM\u2019s performance on a variety of tasks, especially when followed by supervised fnetuning with <pause> tokens. On the other hand, Pfau et al. (2024) further explored the usage of fller tokens, e.g., \u201c...\u201d, and concluded that they work well for highly parallelizable problems. However, Pfau et al. (2024) mentioned these methods do not extend the expressivity of the LLM like CoT; hence, they may not scale to more general and complex reasoning problems. Wang et al. (2023) proposed to predict a planning token as a discrete latent variable before generating the next reasoning step. Recently, it has also been found that one can \u201cinternalize\u201d the CoT reasoning into latent reasoning in the transformer with knowledge distillation (Deng et al., 2023) or a special training curriculum which gradually shortens CoT (Deng et al., 2024). Yu et al. (2024b) also proposed to distill a model that can reason latently from data generated with complex reasoning algorithms. These training methods can be combined to our framework, and specifcally, we fnd that breaking down the learning of continuous thoughts into multiple stages, inspired by iCoT (Deng et al., 2024), is very benefcial for the training. Recently, looped transformers (Giannou et al., 2023; Fan et al., 2024) have been proposed to solve algorithmic tasks, which have some similarities to the computing process of continuous thoughts, but we focus on common reasoning tasks and aim at investigating latent reasoning in comparison to language space.",
        "images": [],
        "tables": [],
        "references": [
            {
                "paperId": "48472a217174053a146b0ebd3821e71b53f05a36",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2410-09918",
                    "ArXiv": "2410.09918",
                    "DOI": "10.48550/arXiv.2410.09918",
                    "CorpusId": 273346442
                },
                "corpusId": 273346442,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/48472a217174053a146b0ebd3821e71b53f05a36",
                "title": "Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces",
                "abstract": "In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 x 30 maze navigation tasks 97.6% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3%, while only using 45.5% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30%. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 82,
                "citationCount": 2,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-10-13",
                "journal": {
                    "volume": "abs/2410.09918",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Su2024DualformerCF,\n author = {DiJia Su and Sainbayar Sukhbaatar and Michael Rabbat and Yuandong Tian and Qinqing Zheng},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces},\n volume = {abs/2410.09918},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2325888815",
                        "name": "DiJia Su"
                    },
                    {
                        "authorId": "2265067",
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "authorId": "2284991448",
                        "name": "Michael Rabbat"
                    },
                    {
                        "authorId": "2285362895",
                        "name": "Yuandong Tian"
                    },
                    {
                        "authorId": "2326106870",
                        "name": "Qinqing Zheng"
                    }
                ]
            },
            {
                "paperId": "b4f5d23f139e65a90c5806391e5d22a1cc3df248",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2404-05221",
                    "ArXiv": "2404.05221",
                    "DOI": "10.48550/arXiv.2404.05221",
                    "CorpusId": 269004939
                },
                "corpusId": 269004939,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b4f5d23f139e65a90c5806391e5d22a1cc3df248",
                "title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "abstract": "Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 8,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-04-08",
                "journal": {
                    "volume": "abs/2404.05221",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2024LLMRN,\n author = {Shibo Hao and Yi Gu and Haotian Luo and Tianyang Liu and Xiyan Shao and Xinyuan Wang and Shuhua Xie and Haodi Ma and Adithya Samavedhi and Qiyue Gao and Zhen Wang and Zhiting Hu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models},\n volume = {abs/2404.05221},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2261698849",
                        "name": "Haotian Luo"
                    },
                    {
                        "authorId": "2115347044",
                        "name": "Tianyang Liu"
                    },
                    {
                        "authorId": "2218793118",
                        "name": "Xiyan Shao"
                    },
                    {
                        "authorId": "2261680539",
                        "name": "Xinyuan Wang"
                    },
                    {
                        "authorId": "2295679772",
                        "name": "Shuhua Xie"
                    },
                    {
                        "authorId": "2295874218",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2124016792",
                        "name": "Adithya Samavedhi"
                    },
                    {
                        "authorId": "2295678976",
                        "name": "Qiyue Gao"
                    },
                    {
                        "authorId": "2261683280",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2295863002",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "b2688b3a0ddf190cd99b11b6bf589a6e071c5369",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2404-03683",
                    "ArXiv": "2404.03683",
                    "DOI": "10.48550/arXiv.2404.03683",
                    "CorpusId": 268987503
                },
                "corpusId": 268987503,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2688b3a0ddf190cd99b11b6bf589a6e071c5369",
                "title": "Stream of Search (SoS): Learning to Search in Language",
                "abstract": "Language models are rarely shown fruitful mistakes while training. They then struggle to look beyond the next token, suffering from a snowballing of errors and struggling to predict the consequence of their actions several steps ahead. In this paper, we show how language models can be taught to search by representing the process of search in language, as a flattened string -- a stream of search (SoS). We propose a unified language for search that captures an array of different symbolic search strategies. We demonstrate our approach using the simple yet difficult game of Countdown, where the goal is to combine input numbers with arithmetic operations to reach a target number. We pretrain a transformer-based language model from scratch on a dataset of streams of search generated by heuristic solvers. We find that SoS pretraining increases search accuracy by 25% over models trained to predict only the optimal search trajectory. We further finetune this model with two policy improvement methods: Advantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The finetuned SoS models solve 36% of previously unsolved problems, including problems that cannot be solved by any of the heuristic solvers. Our results indicate that language models can learn to solve problems via search, self-improve to flexibly use different search strategies, and potentially discover new ones.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 28,
                "citationCount": 13,
                "influentialCitationCount": 5,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-04-01",
                "journal": {
                    "volume": "abs/2404.03683",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Gandhi2024StreamOS,\n author = {Kanishk Gandhi and Denise Lee and Gabriel Grand and Muxin Liu and Winson Cheng and Archit Sharma and Noah D. Goodman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Stream of Search (SoS): Learning to Search in Language},\n volume = {abs/2404.03683},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144841994",
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "authorId": "2295586561",
                        "name": "Denise Lee"
                    },
                    {
                        "authorId": "2295510687",
                        "name": "Gabriel Grand"
                    },
                    {
                        "authorId": "2264511729",
                        "name": "Muxin Liu"
                    },
                    {
                        "authorId": "2295592705",
                        "name": "Winson Cheng"
                    },
                    {
                        "authorId": "2295593182",
                        "name": "Archit Sharma"
                    },
                    {
                        "authorId": "2265069313",
                        "name": "Noah D. Goodman"
                    }
                ]
            },
            {
                "paperId": "df5bb57e03c38a439d664d3c609a1c03a9a64009",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2402-14083",
                    "ArXiv": "2402.14083",
                    "DOI": "10.48550/arXiv.2402.14083",
                    "CorpusId": 267782588
                },
                "corpusId": 267782588,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df5bb57e03c38a439d664d3c609a1c03a9a64009",
                "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
                "abstract": "While Transformers have enabled tremendous progress in various application settings, such architectures still trail behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks. This is accomplished by training an encoder-decoder Transformer model to predict the search dynamics of the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than the $A^*$ implementation that was used for training initially. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\times$ smaller model size and a 10$\\times$ smaller training dataset. Lastly, we demonstrate how Searchformer scales to larger and more complex decision making tasks with improved percentage of solved tasks and shortened search dynamics.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 52,
                "citationCount": 37,
                "influentialCitationCount": 2,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-02-21",
                "journal": {
                    "volume": "abs/2402.14083",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Lehnert2024BeyondAB,\n author = {Lucas Lehnert and Sainbayar Sukhbaatar and Paul Mcvay and Michael Rabbat and Yuandong Tian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping},\n volume = {abs/2402.14083},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2284988047",
                        "name": "Lucas Lehnert"
                    },
                    {
                        "authorId": "2265067",
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "authorId": "2284990505",
                        "name": "Paul Mcvay"
                    },
                    {
                        "authorId": "2284991448",
                        "name": "Michael Rabbat"
                    },
                    {
                        "authorId": "2285362895",
                        "name": "Yuandong Tian"
                    }
                ]
            },
            {
                "paperId": "5a20aa49b81b4e14fdb36814e557b3da60259ce9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2402-12875",
                    "ArXiv": "2402.12875",
                    "DOI": "10.48550/arXiv.2402.12875",
                    "CorpusId": 267760184
                },
                "corpusId": 267760184,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5a20aa49b81b4e14fdb36814e557b3da60259ce9",
                "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
                "abstract": "Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",
                "venue": "International Conference on Learning Representations",
                "year": 2024,
                "referenceCount": 74,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Mathematics"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-02-20",
                "journal": {
                    "volume": "abs/2402.12875",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Li2024ChainOT,\n author = {Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},\n volume = {abs/2402.12875},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2284829110",
                        "name": "Zhiyuan Li"
                    },
                    {
                        "authorId": "2284940181",
                        "name": "Hong Liu"
                    },
                    {
                        "authorId": "2284824080",
                        "name": "Denny Zhou"
                    },
                    {
                        "authorId": "2284835869",
                        "name": "Tengyu Ma"
                    }
                ]
            },
            {
                "paperId": "c2260403fd5cb2de73491323433e48b6ec36872c",
                "externalIds": {
                    "ArXiv": "2305.15408",
                    "DBLP": "journals/corr/abs-2305-15408",
                    "DOI": "10.48550/arXiv.2305.15408",
                    "CorpusId": 258865989
                },
                "corpusId": 258865989,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c2260403fd5cb2de73491323433e48b6ec36872c",
                "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective",
                "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 69,
                "citationCount": 153,
                "influentialCitationCount": 26,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.15408",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Mathematics"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.15408",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Feng2023TowardsRT,\n author = {Guhao Feng and Yuntian Gu and Bohang Zhang and Haotian Ye and Di He and Liwei Wang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},\n volume = {abs/2305.15408},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2205796820",
                        "name": "Guhao Feng"
                    },
                    {
                        "authorId": "2171999927",
                        "name": "Yuntian Gu"
                    },
                    {
                        "authorId": "1988294358",
                        "name": "Bohang Zhang"
                    },
                    {
                        "authorId": "2284300618",
                        "name": "Haotian Ye"
                    },
                    {
                        "authorId": "1391126980",
                        "name": "Di He"
                    },
                    {
                        "authorId": "39060743",
                        "name": "Liwei Wang"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "ef018d9fad6167cfddb7d6654c5422df1e953730",
                "externalIds": {
                    "ArXiv": "2305.00633",
                    "DBLP": "conf/nips/XieKZZKHX23",
                    "CorpusId": 258426922
                },
                "corpusId": 258426922,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ef018d9fad6167cfddb7d6654c5422df1e953730",
                "title": "Self-Evaluation Guided Beam Search for Reasoning",
                "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 79,
                "influentialCitationCount": 7,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-01",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Article{Xie2023SelfEvaluationGB,\n author = {Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and MingSung Kan and Junxian He and Qizhe Xie},\n booktitle = {Neural Information Processing Systems},\n title = {Self-Evaluation Guided Beam Search for Reasoning},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "46268944",
                        "name": "Yuxi Xie"
                    },
                    {
                        "authorId": "2324666",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "2249827256",
                        "name": "Yiran Zhao"
                    },
                    {
                        "authorId": "1410132011",
                        "name": "Xu Zhao"
                    },
                    {
                        "authorId": "1576535050",
                        "name": "MingSung Kan"
                    },
                    {
                        "authorId": "6215698",
                        "name": "Junxian He"
                    },
                    {
                        "authorId": "1912046",
                        "name": "Qizhe Xie"
                    }
                ]
            },
            {
                "paperId": "75c19f3249f644f5cb2182282fc117c089fd3f65",
                "externalIds": {
                    "DBLP": "conf/iclr/MerrillS24",
                    "DOI": "10.48550/arXiv.2310.07923",
                    "CorpusId": 263909434
                },
                "corpusId": 263909434,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75c19f3249f644f5cb2182282fc117c089fd3f65",
                "title": "The Expressive Power of Transformers with Chain of Thought",
                "abstract": "Recent theoretical work has identi\ufb01ed surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating \ufb01nite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers\u2019 reasoning can be improved by allowing them to use a \u201cchain of thought\u201d or \u201cscratchpad\u201d, i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this",
                "venue": "International Conference on Learning Representations",
                "year": 2024,
                "referenceCount": 29,
                "citationCount": 75,
                "influentialCitationCount": 5,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.07923",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": null,
                "journal": {
                    "volume": "abs/2310.07923",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Merrill2024TheEP,\n author = {William Merrill and Ashish Sabharwal},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {The Expressive Power of Transformers with Chain of Thought},\n volume = {abs/2310.07923},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2257349351",
                        "name": "William Merrill"
                    },
                    {
                        "authorId": "2257349398",
                        "name": "Ashish Sabharwal"
                    }
                ]
            },
            {
                "paperId": "f15a1a87c345a0b0b2b989064ede053d1411bb03",
                "externalIds": {
                    "ArXiv": "2407.06023",
                    "DBLP": "journals/corr/abs-2407-06023",
                    "DOI": "10.48550/arXiv.2407.06023",
                    "CorpusId": 271050364
                },
                "corpusId": 271050364,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f15a1a87c345a0b0b2b989064ede053d1411bb03",
                "title": "Distilling System 2 into System 1",
                "abstract": "Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2 Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al., 2023). In this work we investigate self-supervised methods to ``compile'' (distill) higher quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1. We show that several such techniques can be successfully distilled, resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2. We posit that such System 2 distillation will be an important feature of future continually learning AI systems, enabling them to focus System 2 capabilities on the reasoning tasks that they cannot yet do well.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 50,
                "citationCount": 22,
                "influentialCitationCount": 2,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-08",
                "journal": {
                    "volume": "abs/2407.06023",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yu2024DistillingS2,\n author = {Ping Yu and Jing Xu and Jason Weston and Ilia Kulikov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distilling System 2 into System 1},\n volume = {abs/2407.06023},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2308246369",
                        "name": "Ping Yu"
                    },
                    {
                        "authorId": "2276766957",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "2267341626",
                        "name": "Jason Weston"
                    },
                    {
                        "authorId": "2308102420",
                        "name": "Ilia Kulikov"
                    }
                ]
            },
            {
                "paperId": "3046cb3ace7baa64f40c46b43c5cb412a975ab3b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2406-13858",
                    "ArXiv": "2406.13858",
                    "DOI": "10.48550/arXiv.2406.13858",
                    "CorpusId": 270620132
                },
                "corpusId": 270620132,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3046cb3ace7baa64f40c46b43c5cb412a975ab3b",
                "title": "Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning",
                "abstract": "Large language models (LLMs) have shown an impressive ability to perform tasks believed to require thought processes. When the model does not document an explicit thought process, it becomes difficult to understand the processes occurring within its hidden layers and to determine if these processes can be referred to as reasoning. We introduce a novel and interpretable analysis of internal multi-hop reasoning processes in LLMs. We demonstrate that the prediction process for compositional reasoning questions can be modeled using a simple linear transformation between two semantic category spaces. We show that during inference, the middle layers of the network generate highly interpretable embeddings that represent a set of potential intermediate answers for the multi-hop question. We use statistical analyses to show that a corresponding subset of tokens is activated in the model's output, implying the existence of parallel reasoning paths. These observations hold true even when the model lacks the necessary knowledge to solve the task. Our findings can help uncover the strategies that LLMs use to solve reasoning tasks, offering insights into the types of thought processes that can emerge from artificial intelligence. Finally, we also discuss the implication of cognitive modeling of these results.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 32,
                "citationCount": 1,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-19",
                "journal": {
                    "volume": "abs/2406.13858",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Shalev2024DistributionalRI,\n author = {Yuval Shalev and Amir Feder and Ariel Goldstein},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning},\n volume = {abs/2406.13858},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "151264946",
                        "name": "Yuval Shalev"
                    },
                    {
                        "authorId": "46609506",
                        "name": "Amir Feder"
                    },
                    {
                        "authorId": "2289609556",
                        "name": "Ariel Goldstein"
                    }
                ]
            },
            {
                "paperId": "d610e066e5c3cc7eda13313554e25a340ce18f71",
                "externalIds": {
                    "ArXiv": "2406.12775",
                    "DBLP": "journals/corr/abs-2406-12775",
                    "ACL": "2024.emnlp-main.781",
                    "DOI": "10.48550/arXiv.2406.12775",
                    "CorpusId": 270562421
                },
                "corpusId": 270562421,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/d610e066e5c3cc7eda13313554e25a340ce18f71",
                "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries",
                "abstract": "Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as \u201cThe spouse of the performer of Imagine is\u201d. These queries require two information extraction steps: a latent one for resolving the first hop (\u201cthe performer of Imagine\u201d) into the bridge entity (John Lennon), and another for resolving the second hop (\u201cthe spouse of John Lennon\u201d) into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel \u201cback-patching\u201d analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2024,
                "referenceCount": 43,
                "citationCount": 10,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2024-06-18",
                "journal": {
                    "pages": "14113-14130"
                },
                "citationStyles": {
                    "bibtex": "@Article{Biran2024HoppingTL,\n author = {Eden Biran and Daniela Gottesman and Sohee Yang and Mor Geva and Amir Globerson},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {14113-14130},\n title = {Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2226283125",
                        "name": "Eden Biran"
                    },
                    {
                        "authorId": "2307080108",
                        "name": "Daniela Gottesman"
                    },
                    {
                        "authorId": "16110760",
                        "name": "Sohee Yang"
                    },
                    {
                        "authorId": "22245981",
                        "name": "Mor Geva"
                    },
                    {
                        "authorId": "2261392157",
                        "name": "Amir Globerson"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "397e5015f761bc4a0d4e81daff7d6462ae7c5a98",
                "externalIds": {
                    "ArXiv": "2404.15758",
                    "DBLP": "journals/corr/abs-2404-15758",
                    "DOI": "10.48550/arXiv.2404.15758",
                    "CorpusId": 269362669
                },
                "corpusId": 269362669,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/397e5015f761bc4a0d4e81daff7d6462ae7c5a98",
                "title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language Models",
                "abstract": "Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 17,
                "citationCount": 37,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-04-24",
                "journal": {
                    "volume": "abs/2404.15758",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Pfau2024LetsTD,\n author = {Jacob Pfau and William Merrill and Samuel R. Bowman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Let's Think Dot by Dot: Hidden Computation in Transformer Language Models},\n volume = {abs/2404.15758},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2298272496",
                        "name": "Jacob Pfau"
                    },
                    {
                        "authorId": "2282138468",
                        "name": "William Merrill"
                    },
                    {
                        "authorId": "2298270956",
                        "name": "Samuel R. Bowman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "b29134737a0c81c13d31fc0263b3c4d4f05ccb78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2310-05707",
                    "ArXiv": "2310.05707",
                    "DOI": "10.48550/arXiv.2310.05707",
                    "CorpusId": 270973281
                },
                "corpusId": 270973281,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b29134737a0c81c13d31fc0263b3c4d4f05ccb78",
                "title": "Guiding Language Model Reasoning with Planning Tokens",
                "abstract": "Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought (CoT) reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. To encourage a more structural generation of CoT steps, we propose a hierarchical generation scheme: we let the LM generate a planning token at the start of each reasoning step, intuitively serving as a high-level plan of the current step, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets and one multihop QA dataset with respect to standard fine-tuning baselines.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 78,
                "citationCount": 9,
                "influentialCitationCount": 2,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.05707",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-09",
                "journal": {
                    "volume": "abs/2310.05707",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wang2023GuidingLM,\n author = {Xinyi Wang and Lucas Caccia and O. Ostapenko and Xingdi Yuan and Alessandro Sordoni},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Guiding Language Model Reasoning with Planning Tokens},\n volume = {abs/2310.05707},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2309866809",
                        "name": "Xinyi Wang"
                    },
                    {
                        "authorId": "51889580",
                        "name": "Lucas Caccia"
                    },
                    {
                        "authorId": "145191120",
                        "name": "O. Ostapenko"
                    },
                    {
                        "authorId": "2258299929",
                        "name": "Xingdi Yuan"
                    },
                    {
                        "authorId": "2041695",
                        "name": "Alessandro Sordoni"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
                "externalIds": {
                    "DBLP": "conf/nips/TurpinMPB23",
                    "ArXiv": "2305.04388",
                    "DOI": "10.48550/arXiv.2305.04388",
                    "CorpusId": 258556812
                },
                "corpusId": 258556812,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
                "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
                "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 75,
                "citationCount": 303,
                "influentialCitationCount": 15,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.04388",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-07",
                "journal": {
                    "volume": "abs/2305.04388",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Turpin2023LanguageMD,\n author = {Miles Turpin and Julian Michael and Ethan Perez and Sam Bowman},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},\n volume = {abs/2305.04388},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144196816",
                        "name": "Miles Turpin"
                    },
                    {
                        "authorId": "38614754",
                        "name": "Julian Michael"
                    },
                    {
                        "authorId": "3439053",
                        "name": "Ethan Perez"
                    },
                    {
                        "authorId": "1799822",
                        "name": "Sam Bowman"
                    }
                ]
            },
            {
                "paperId": "35922cd0d6b17e45320917338e9f98cb5c1a4f6f",
                "externalIds": {
                    "ArXiv": "2212.10001",
                    "DBLP": "conf/acl/WangM0S0Z023",
                    "ACL": "2023.acl-long.153",
                    "DOI": "10.48550/arXiv.2212.10001",
                    "CorpusId": 254877569
                },
                "corpusId": 254877569,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f",
                "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
                "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs\u2019 capability to learn to reason in context.",
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "year": 2022,
                "referenceCount": 40,
                "citationCount": 184,
                "influentialCitationCount": 13,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2212.10001",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2022-12-20",
                "journal": {
                    "pages": "2717-2739"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wang2022TowardsUC,\n author = {Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2717-2739},\n title = {Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "7425689",
                        "name": "Boshi Wang"
                    },
                    {
                        "authorId": "48872685",
                        "name": "Sewon Min"
                    },
                    {
                        "authorId": "145924070",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "3363642",
                        "name": "Jiaming Shen"
                    },
                    {
                        "authorId": "1557391861",
                        "name": "You Wu"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "authorId": "1515546612",
                        "name": "Huan Sun"
                    }
                ]
            },
            {
                "paperId": "48472a217174053a146b0ebd3821e71b53f05a36",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2410-09918",
                    "ArXiv": "2410.09918",
                    "DOI": "10.48550/arXiv.2410.09918",
                    "CorpusId": 273346442
                },
                "corpusId": 273346442,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/48472a217174053a146b0ebd3821e71b53f05a36",
                "title": "Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces",
                "abstract": "In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities. Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present Dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. Dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (fast mode) or both the reasoning chain and the final solution (slow mode), or automatically decide which mode to engage (auto mode). In all cases, Dualformer outperforms the corresponding baseline models in both performance and computational efficiency: (1) in slow mode, Dualformer optimally solves unseen 30 x 30 maze navigation tasks 97.6% of the time, surpassing the Searchformer (trained on data with complete reasoning traces) baseline performance of 93.3%, while only using 45.5% fewer reasoning steps; (2) in fast mode, Dualformer completes those tasks with an 80% optimal rate, significantly outperforming the Solution-Only model (trained on solution-only data), which has an optimal rate of only 30%. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 82,
                "citationCount": 2,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-10-13",
                "journal": {
                    "volume": "abs/2410.09918",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Su2024DualformerCF,\n author = {DiJia Su and Sainbayar Sukhbaatar and Michael Rabbat and Yuandong Tian and Qinqing Zheng},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces},\n volume = {abs/2410.09918},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2325888815",
                        "name": "DiJia Su"
                    },
                    {
                        "authorId": "2265067",
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "authorId": "2284991448",
                        "name": "Michael Rabbat"
                    },
                    {
                        "authorId": "2285362895",
                        "name": "Yuandong Tian"
                    },
                    {
                        "authorId": "2326106870",
                        "name": "Qinqing Zheng"
                    }
                ]
            },
            {
                "paperId": "b4f5d23f139e65a90c5806391e5d22a1cc3df248",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2404-05221",
                    "ArXiv": "2404.05221",
                    "DOI": "10.48550/arXiv.2404.05221",
                    "CorpusId": 269004939
                },
                "corpusId": 269004939,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b4f5d23f139e65a90c5806391e5d22a1cc3df248",
                "title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
                "abstract": "Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 0,
                "citationCount": 8,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-04-08",
                "journal": {
                    "volume": "abs/2404.05221",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Hao2024LLMRN,\n author = {Shibo Hao and Yi Gu and Haotian Luo and Tianyang Liu and Xiyan Shao and Xinyuan Wang and Shuhua Xie and Haodi Ma and Adithya Samavedhi and Qiyue Gao and Zhen Wang and Zhiting Hu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models},\n volume = {abs/2404.05221},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2128965713",
                        "name": "Shibo Hao"
                    },
                    {
                        "authorId": "2112578816",
                        "name": "Yi Gu"
                    },
                    {
                        "authorId": "2261698849",
                        "name": "Haotian Luo"
                    },
                    {
                        "authorId": "2115347044",
                        "name": "Tianyang Liu"
                    },
                    {
                        "authorId": "2218793118",
                        "name": "Xiyan Shao"
                    },
                    {
                        "authorId": "2261680539",
                        "name": "Xinyuan Wang"
                    },
                    {
                        "authorId": "2295679772",
                        "name": "Shuhua Xie"
                    },
                    {
                        "authorId": "2295874218",
                        "name": "Haodi Ma"
                    },
                    {
                        "authorId": "2124016792",
                        "name": "Adithya Samavedhi"
                    },
                    {
                        "authorId": "2295678976",
                        "name": "Qiyue Gao"
                    },
                    {
                        "authorId": "2261683280",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2295863002",
                        "name": "Zhiting Hu"
                    }
                ]
            },
            {
                "paperId": "b2688b3a0ddf190cd99b11b6bf589a6e071c5369",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2404-03683",
                    "ArXiv": "2404.03683",
                    "DOI": "10.48550/arXiv.2404.03683",
                    "CorpusId": 268987503
                },
                "corpusId": 268987503,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2688b3a0ddf190cd99b11b6bf589a6e071c5369",
                "title": "Stream of Search (SoS): Learning to Search in Language",
                "abstract": "Language models are rarely shown fruitful mistakes while training. They then struggle to look beyond the next token, suffering from a snowballing of errors and struggling to predict the consequence of their actions several steps ahead. In this paper, we show how language models can be taught to search by representing the process of search in language, as a flattened string -- a stream of search (SoS). We propose a unified language for search that captures an array of different symbolic search strategies. We demonstrate our approach using the simple yet difficult game of Countdown, where the goal is to combine input numbers with arithmetic operations to reach a target number. We pretrain a transformer-based language model from scratch on a dataset of streams of search generated by heuristic solvers. We find that SoS pretraining increases search accuracy by 25% over models trained to predict only the optimal search trajectory. We further finetune this model with two policy improvement methods: Advantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The finetuned SoS models solve 36% of previously unsolved problems, including problems that cannot be solved by any of the heuristic solvers. Our results indicate that language models can learn to solve problems via search, self-improve to flexibly use different search strategies, and potentially discover new ones.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 28,
                "citationCount": 13,
                "influentialCitationCount": 5,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-04-01",
                "journal": {
                    "volume": "abs/2404.03683",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Gandhi2024StreamOS,\n author = {Kanishk Gandhi and Denise Lee and Gabriel Grand and Muxin Liu and Winson Cheng and Archit Sharma and Noah D. Goodman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Stream of Search (SoS): Learning to Search in Language},\n volume = {abs/2404.03683},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144841994",
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "authorId": "2295586561",
                        "name": "Denise Lee"
                    },
                    {
                        "authorId": "2295510687",
                        "name": "Gabriel Grand"
                    },
                    {
                        "authorId": "2264511729",
                        "name": "Muxin Liu"
                    },
                    {
                        "authorId": "2295592705",
                        "name": "Winson Cheng"
                    },
                    {
                        "authorId": "2295593182",
                        "name": "Archit Sharma"
                    },
                    {
                        "authorId": "2265069313",
                        "name": "Noah D. Goodman"
                    }
                ]
            },
            {
                "paperId": "df5bb57e03c38a439d664d3c609a1c03a9a64009",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2402-14083",
                    "ArXiv": "2402.14083",
                    "DOI": "10.48550/arXiv.2402.14083",
                    "CorpusId": 267782588
                },
                "corpusId": 267782588,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df5bb57e03c38a439d664d3c609a1c03a9a64009",
                "title": "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping",
                "abstract": "While Transformers have enabled tremendous progress in various application settings, such architectures still trail behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks. This is accomplished by training an encoder-decoder Transformer model to predict the search dynamics of the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than the $A^*$ implementation that was used for training initially. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\times$ smaller model size and a 10$\\times$ smaller training dataset. Lastly, we demonstrate how Searchformer scales to larger and more complex decision making tasks with improved percentage of solved tasks and shortened search dynamics.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 52,
                "citationCount": 37,
                "influentialCitationCount": 2,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-02-21",
                "journal": {
                    "volume": "abs/2402.14083",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Lehnert2024BeyondAB,\n author = {Lucas Lehnert and Sainbayar Sukhbaatar and Paul Mcvay and Michael Rabbat and Yuandong Tian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping},\n volume = {abs/2402.14083},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2284988047",
                        "name": "Lucas Lehnert"
                    },
                    {
                        "authorId": "2265067",
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "authorId": "2284990505",
                        "name": "Paul Mcvay"
                    },
                    {
                        "authorId": "2284991448",
                        "name": "Michael Rabbat"
                    },
                    {
                        "authorId": "2285362895",
                        "name": "Yuandong Tian"
                    }
                ]
            },
            {
                "paperId": "5a20aa49b81b4e14fdb36814e557b3da60259ce9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2402-12875",
                    "ArXiv": "2402.12875",
                    "DOI": "10.48550/arXiv.2402.12875",
                    "CorpusId": 267760184
                },
                "corpusId": 267760184,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5a20aa49b81b4e14fdb36814e557b3da60259ce9",
                "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
                "abstract": "Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",
                "venue": "International Conference on Learning Representations",
                "year": 2024,
                "referenceCount": 74,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science",
                    "Mathematics"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-02-20",
                "journal": {
                    "volume": "abs/2402.12875",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Li2024ChainOT,\n author = {Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},\n volume = {abs/2402.12875},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2284829110",
                        "name": "Zhiyuan Li"
                    },
                    {
                        "authorId": "2284940181",
                        "name": "Hong Liu"
                    },
                    {
                        "authorId": "2284824080",
                        "name": "Denny Zhou"
                    },
                    {
                        "authorId": "2284835869",
                        "name": "Tengyu Ma"
                    }
                ]
            },
            {
                "paperId": "c2260403fd5cb2de73491323433e48b6ec36872c",
                "externalIds": {
                    "ArXiv": "2305.15408",
                    "DBLP": "journals/corr/abs-2305-15408",
                    "DOI": "10.48550/arXiv.2305.15408",
                    "CorpusId": 258865989
                },
                "corpusId": 258865989,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c2260403fd5cb2de73491323433e48b6ec36872c",
                "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective",
                "abstract": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 69,
                "citationCount": 153,
                "influentialCitationCount": 26,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.15408",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science",
                    "Mathematics"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-24",
                "journal": {
                    "volume": "abs/2305.15408",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Feng2023TowardsRT,\n author = {Guhao Feng and Yuntian Gu and Bohang Zhang and Haotian Ye and Di He and Liwei Wang},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},\n volume = {abs/2305.15408},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2205796820",
                        "name": "Guhao Feng"
                    },
                    {
                        "authorId": "2171999927",
                        "name": "Yuntian Gu"
                    },
                    {
                        "authorId": "1988294358",
                        "name": "Bohang Zhang"
                    },
                    {
                        "authorId": "2284300618",
                        "name": "Haotian Ye"
                    },
                    {
                        "authorId": "1391126980",
                        "name": "Di He"
                    },
                    {
                        "authorId": "39060743",
                        "name": "Liwei Wang"
                    }
                ]
            },
            {
                "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "externalIds": {
                    "ArXiv": "2305.10601",
                    "DBLP": "conf/nips/YaoYZS00N23",
                    "DOI": "10.48550/arXiv.2305.10601",
                    "CorpusId": 258762525
                },
                "corpusId": 258762525,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
                "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 1283,
                "influentialCitationCount": 111,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.10601",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-17",
                "journal": {
                    "volume": "abs/2305.10601",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yao2023TreeOT,\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n volume = {abs/2305.10601},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2093302161",
                        "name": "Shunyu Yao"
                    },
                    {
                        "authorId": "150978762",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "2144551262",
                        "name": "Jeffrey Zhao"
                    },
                    {
                        "authorId": "1697494",
                        "name": "Izhak Shafran"
                    },
                    {
                        "authorId": "1799860",
                        "name": "T. Griffiths"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "144958935",
                        "name": "Karthik Narasimhan"
                    }
                ]
            },
            {
                "paperId": "ef018d9fad6167cfddb7d6654c5422df1e953730",
                "externalIds": {
                    "ArXiv": "2305.00633",
                    "DBLP": "conf/nips/XieKZZKHX23",
                    "CorpusId": 258426922
                },
                "corpusId": 258426922,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ef018d9fad6167cfddb7d6654c5422df1e953730",
                "title": "Self-Evaluation Guided Beam Search for Reasoning",
                "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 52,
                "citationCount": 79,
                "influentialCitationCount": 7,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-01",
                "journal": null,
                "citationStyles": {
                    "bibtex": "@Article{Xie2023SelfEvaluationGB,\n author = {Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and MingSung Kan and Junxian He and Qizhe Xie},\n booktitle = {Neural Information Processing Systems},\n title = {Self-Evaluation Guided Beam Search for Reasoning},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "46268944",
                        "name": "Yuxi Xie"
                    },
                    {
                        "authorId": "2324666",
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "authorId": "2249827256",
                        "name": "Yiran Zhao"
                    },
                    {
                        "authorId": "1410132011",
                        "name": "Xu Zhao"
                    },
                    {
                        "authorId": "1576535050",
                        "name": "MingSung Kan"
                    },
                    {
                        "authorId": "6215698",
                        "name": "Junxian He"
                    },
                    {
                        "authorId": "1912046",
                        "name": "Qizhe Xie"
                    }
                ]
            },
            {
                "paperId": "75c19f3249f644f5cb2182282fc117c089fd3f65",
                "externalIds": {
                    "DBLP": "conf/iclr/MerrillS24",
                    "DOI": "10.48550/arXiv.2310.07923",
                    "CorpusId": 263909434
                },
                "corpusId": 263909434,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/75c19f3249f644f5cb2182282fc117c089fd3f65",
                "title": "The Expressive Power of Transformers with Chain of Thought",
                "abstract": "Recent theoretical work has identi\ufb01ed surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating \ufb01nite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers\u2019 reasoning can be improved by allowing them to use a \u201cchain of thought\u201d or \u201cscratchpad\u201d, i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this",
                "venue": "International Conference on Learning Representations",
                "year": 2024,
                "referenceCount": 29,
                "citationCount": 75,
                "influentialCitationCount": 5,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.07923",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": null,
                "journal": {
                    "volume": "abs/2310.07923",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Merrill2024TheEP,\n author = {William Merrill and Ashish Sabharwal},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {The Expressive Power of Transformers with Chain of Thought},\n volume = {abs/2310.07923},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2257349351",
                        "name": "William Merrill"
                    },
                    {
                        "authorId": "2257349398",
                        "name": "Ashish Sabharwal"
                    }
                ]
            },
            {
                "paperId": "f15a1a87c345a0b0b2b989064ede053d1411bb03",
                "externalIds": {
                    "ArXiv": "2407.06023",
                    "DBLP": "journals/corr/abs-2407-06023",
                    "DOI": "10.48550/arXiv.2407.06023",
                    "CorpusId": 271050364
                },
                "corpusId": 271050364,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f15a1a87c345a0b0b2b989064ede053d1411bb03",
                "title": "Distilling System 2 into System 1",
                "abstract": "Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2 Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al., 2023). In this work we investigate self-supervised methods to ``compile'' (distill) higher quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1. We show that several such techniques can be successfully distilled, resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2. We posit that such System 2 distillation will be an important feature of future continually learning AI systems, enabling them to focus System 2 capabilities on the reasoning tasks that they cannot yet do well.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 50,
                "citationCount": 22,
                "influentialCitationCount": 2,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-07-08",
                "journal": {
                    "volume": "abs/2407.06023",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Yu2024DistillingS2,\n author = {Ping Yu and Jing Xu and Jason Weston and Ilia Kulikov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distilling System 2 into System 1},\n volume = {abs/2407.06023},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2308246369",
                        "name": "Ping Yu"
                    },
                    {
                        "authorId": "2276766957",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "2267341626",
                        "name": "Jason Weston"
                    },
                    {
                        "authorId": "2308102420",
                        "name": "Ilia Kulikov"
                    }
                ]
            },
            {
                "paperId": "3046cb3ace7baa64f40c46b43c5cb412a975ab3b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2406-13858",
                    "ArXiv": "2406.13858",
                    "DOI": "10.48550/arXiv.2406.13858",
                    "CorpusId": 270620132
                },
                "corpusId": 270620132,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3046cb3ace7baa64f40c46b43c5cb412a975ab3b",
                "title": "Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning",
                "abstract": "Large language models (LLMs) have shown an impressive ability to perform tasks believed to require thought processes. When the model does not document an explicit thought process, it becomes difficult to understand the processes occurring within its hidden layers and to determine if these processes can be referred to as reasoning. We introduce a novel and interpretable analysis of internal multi-hop reasoning processes in LLMs. We demonstrate that the prediction process for compositional reasoning questions can be modeled using a simple linear transformation between two semantic category spaces. We show that during inference, the middle layers of the network generate highly interpretable embeddings that represent a set of potential intermediate answers for the multi-hop question. We use statistical analyses to show that a corresponding subset of tokens is activated in the model's output, implying the existence of parallel reasoning paths. These observations hold true even when the model lacks the necessary knowledge to solve the task. Our findings can help uncover the strategies that LLMs use to solve reasoning tasks, offering insights into the types of thought processes that can emerge from artificial intelligence. Finally, we also discuss the implication of cognitive modeling of these results.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 32,
                "citationCount": 1,
                "influentialCitationCount": 0,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-06-19",
                "journal": {
                    "volume": "abs/2406.13858",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Shalev2024DistributionalRI,\n author = {Yuval Shalev and Amir Feder and Ariel Goldstein},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning},\n volume = {abs/2406.13858},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "151264946",
                        "name": "Yuval Shalev"
                    },
                    {
                        "authorId": "46609506",
                        "name": "Amir Feder"
                    },
                    {
                        "authorId": "2289609556",
                        "name": "Ariel Goldstein"
                    }
                ]
            },
            {
                "paperId": "d610e066e5c3cc7eda13313554e25a340ce18f71",
                "externalIds": {
                    "ArXiv": "2406.12775",
                    "DBLP": "journals/corr/abs-2406-12775",
                    "ACL": "2024.emnlp-main.781",
                    "DOI": "10.48550/arXiv.2406.12775",
                    "CorpusId": 270562421
                },
                "corpusId": 270562421,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/d610e066e5c3cc7eda13313554e25a340ce18f71",
                "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries",
                "abstract": "Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as \u201cThe spouse of the performer of Imagine is\u201d. These queries require two information extraction steps: a latent one for resolving the first hop (\u201cthe performer of Imagine\u201d) into the bridge entity (John Lennon), and another for resolving the second hop (\u201cthe spouse of John Lennon\u201d) into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel \u201cback-patching\u201d analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.",
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "year": 2024,
                "referenceCount": 43,
                "citationCount": 10,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2024-06-18",
                "journal": {
                    "pages": "14113-14130"
                },
                "citationStyles": {
                    "bibtex": "@Article{Biran2024HoppingTL,\n author = {Eden Biran and Daniela Gottesman and Sohee Yang and Mor Geva and Amir Globerson},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {14113-14130},\n title = {Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2226283125",
                        "name": "Eden Biran"
                    },
                    {
                        "authorId": "2307080108",
                        "name": "Daniela Gottesman"
                    },
                    {
                        "authorId": "16110760",
                        "name": "Sohee Yang"
                    },
                    {
                        "authorId": "22245981",
                        "name": "Mor Geva"
                    },
                    {
                        "authorId": "2261392157",
                        "name": "Amir Globerson"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "397e5015f761bc4a0d4e81daff7d6462ae7c5a98",
                "externalIds": {
                    "ArXiv": "2404.15758",
                    "DBLP": "journals/corr/abs-2404-15758",
                    "DOI": "10.48550/arXiv.2404.15758",
                    "CorpusId": 269362669
                },
                "corpusId": 269362669,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/397e5015f761bc4a0d4e81daff7d6462ae7c5a98",
                "title": "Let's Think Dot by Dot: Hidden Computation in Transformer Language Models",
                "abstract": "Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 17,
                "citationCount": 37,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-04-24",
                "journal": {
                    "volume": "abs/2404.15758",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Pfau2024LetsTD,\n author = {Jacob Pfau and William Merrill and Samuel R. Bowman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Let's Think Dot by Dot: Hidden Computation in Transformer Language Models},\n volume = {abs/2404.15758},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2298272496",
                        "name": "Jacob Pfau"
                    },
                    {
                        "authorId": "2282138468",
                        "name": "William Merrill"
                    },
                    {
                        "authorId": "2298270956",
                        "name": "Samuel R. Bowman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "b29134737a0c81c13d31fc0263b3c4d4f05ccb78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2310-05707",
                    "ArXiv": "2310.05707",
                    "DOI": "10.48550/arXiv.2310.05707",
                    "CorpusId": 270973281
                },
                "corpusId": 270973281,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b29134737a0c81c13d31fc0263b3c4d4f05ccb78",
                "title": "Guiding Language Model Reasoning with Planning Tokens",
                "abstract": "Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought (CoT) reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. To encourage a more structural generation of CoT steps, we propose a hierarchical generation scheme: we let the LM generate a planning token at the start of each reasoning step, intuitively serving as a high-level plan of the current step, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets and one multihop QA dataset with respect to standard fine-tuning baselines.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 78,
                "citationCount": 9,
                "influentialCitationCount": 2,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.05707",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-09",
                "journal": {
                    "volume": "abs/2310.05707",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wang2023GuidingLM,\n author = {Xinyi Wang and Lucas Caccia and O. Ostapenko and Xingdi Yuan and Alessandro Sordoni},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Guiding Language Model Reasoning with Planning Tokens},\n volume = {abs/2310.05707},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2309866809",
                        "name": "Xinyi Wang"
                    },
                    {
                        "authorId": "51889580",
                        "name": "Lucas Caccia"
                    },
                    {
                        "authorId": "145191120",
                        "name": "O. Ostapenko"
                    },
                    {
                        "authorId": "2258299929",
                        "name": "Xingdi Yuan"
                    },
                    {
                        "authorId": "2041695",
                        "name": "Alessandro Sordoni"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
                "externalIds": {
                    "DBLP": "conf/nips/TurpinMPB23",
                    "ArXiv": "2305.04388",
                    "DOI": "10.48550/arXiv.2305.04388",
                    "CorpusId": 258556812
                },
                "corpusId": 258556812,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
                "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
                "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
                "venue": "Neural Information Processing Systems",
                "year": 2023,
                "referenceCount": 75,
                "citationCount": 303,
                "influentialCitationCount": 15,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2305.04388",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-05-07",
                "journal": {
                    "volume": "abs/2305.04388",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Turpin2023LanguageMD,\n author = {Miles Turpin and Julian Michael and Ethan Perez and Sam Bowman},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},\n volume = {abs/2305.04388},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "144196816",
                        "name": "Miles Turpin"
                    },
                    {
                        "authorId": "38614754",
                        "name": "Julian Michael"
                    },
                    {
                        "authorId": "3439053",
                        "name": "Ethan Perez"
                    },
                    {
                        "authorId": "1799822",
                        "name": "Sam Bowman"
                    }
                ]
            },
            {
                "paperId": "35922cd0d6b17e45320917338e9f98cb5c1a4f6f",
                "externalIds": {
                    "ArXiv": "2212.10001",
                    "DBLP": "conf/acl/WangM0S0Z023",
                    "ACL": "2023.acl-long.153",
                    "DOI": "10.48550/arXiv.2212.10001",
                    "CorpusId": 254877569
                },
                "corpusId": 254877569,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f",
                "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
                "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs\u2019 capability to learn to reason in context.",
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "year": 2022,
                "referenceCount": 40,
                "citationCount": 184,
                "influentialCitationCount": 13,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2212.10001",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle",
                    "Conference"
                ],
                "publicationDate": "2022-12-20",
                "journal": {
                    "pages": "2717-2739"
                },
                "citationStyles": {
                    "bibtex": "@Article{Wang2022TowardsUC,\n author = {Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2717-2739},\n title = {Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "7425689",
                        "name": "Boshi Wang"
                    },
                    {
                        "authorId": "48872685",
                        "name": "Sewon Min"
                    },
                    {
                        "authorId": "145924070",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "3363642",
                        "name": "Jiaming Shen"
                    },
                    {
                        "authorId": "1557391861",
                        "name": "You Wu"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "authorId": "1515546612",
                        "name": "Huan Sun"
                    }
                ]
            }
        ]
    },
    {
        "level": 1,
        "num": 4,
        "title": "Coconut: Chain of Continuous Thought",
        "text": "In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in an unconstrained latent space. We begin by introducing the background and notation we use for language models. For an input sequence $\\boldsymbol{x}=(x_{1},...,x_{T})$ , the standard large language model $\\mathcal{M}$ can be described as:  \n```latext formula_1\n$$\n\\begin{array}{c}{H_{t}=\\operatorname{Transformer}(E_{t})}\\\\ {\\mathcal{M}(x_{t+1}\\mid x_{\\le t})=\\operatorname{softmax}(W h_{t})}\\end{array}\n$$  \n```\nwhere $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{t})]$ is the sequence of token embeddings up to position $t$ ; $H_{t}\\in\\mathbb{R}^{t\\times d}$ is the matrix of the last hidden states for all tokens up to position $t$ ; $h_{t}$ is the last hidden state of position $t$ , i.e., $h_{t}=H_{t}[t,:];\\,e(\\cdot)$ is the token embedding function; $W$ is the parameter of the language model head.  \nMethod Overview. In the proposed Coconut method, the LLM switches between the \u201clanguage mode\u201d and \u201clatent mode\u201d (Figure 1). In language mode, the model operates as a standard language model, autoregressively generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding. This last hidden state represents the current reasoning state, termed as a \u201ccontinuous thought\u201d.  \nSpecial tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought mode, respectively. As an example, we assume latent reasoning occurs between positions $i$ and $j$ , i.e., $x_{i}=$ $<b\\cot>$ and $x_{j}=<\\tt e o t>$ . When the model is in the latent mode $(i<t<j$ ), we use the last hidden state from the previous token to replace the input embedding, i.e., $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{t-1}]$  \n![Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg 'Figure 2: Yes!')  \nAfter the latent mode fnishes $(t\\ \\geq\\ j)$ , the input reverts to using the token embedding, i.e., $E_{t}~=$ $[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{j-1},e(x_{j}),...,e(x_{t})]$ . It is worth noting that the last hidden states have been processed by the fnal normalization layer, so they are not too large in magnitude. $\\mathcal{M}(x_{t+1}\\mid x_{\\leq t})$ is not defned when $i<t<j$ , since the latent thought is not intended to be mapped back to language space. However, softmax $\\left(W h_{t}\\right)$ can still be calculated for probing purposes (see Section 4).  \nTraining Procedure. In this work, we focus on a problem-solving setting where the model receives a question as input and is expected to generate an answer through a reasoning process. We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al. (2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the subsequent stages, at the $k$ -th stage, the frst $k$ reasoning steps in the CoT are replaced with $k\\times c$ continuous thoughts1, where $c$ is a hyperparameter controlling the number of latent thoughts replacing a single language reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We insert <bot> and <eot> tokens (which are not counted towards $c$ ) to encapsulate the continuous thoughts.  \nDuring the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions and latent thoughts. It is important to note that the objective does not encourage the continuous thought to compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore, it\u2019s possible for the LLM to learn more efective representations of reasoning steps compared to human language.  \nTraining Details. Our proposed continuous thoughts are fully diferentiable and allow for back-propagation. We perform $n+1$ forward passes when $n$ latent thoughts are scheduled in the current training stage, computing a new latent thought with each pass and fnally conducting an additional forward pass to obtain a loss on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the training efciency of Coconut remains an important direction for future research.  \nInference Process. The inference process for Coconut is analogous to standard language model decoding, except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge lies in determining when to switch between latent and language modes. As we focus on the problem-solving setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two potential strategies: a) train a binary classifer on latent thoughts to enable the model to autonomously decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We found that both approaches work comparably well. Therefore, we use the second option in our experiment for simplicity, unless specifed otherwise.",
        "lines": [
            {
                "id": 19,
                "line": "In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in an unconstrained latent space. We begin by introducing the background and notation we use for language models. For an input sequence $\\boldsymbol{x}=(x_{1},...,x_{T})$ , the standard large language model $\\mathcal{M}$ can be described as:  "
            },
            {
                "id": 20,
                "line": "```latext formula_1"
            },
            {
                "id": 21,
                "line": "$$"
            },
            {
                "id": 22,
                "line": "\\begin{array}{c}{H_{t}=\\operatorname{Transformer}(E_{t})}\\\\ {\\mathcal{M}(x_{t+1}\\mid x_{\\le t})=\\operatorname{softmax}(W h_{t})}\\end{array}"
            },
            {
                "id": 23,
                "line": "$$  "
            },
            {
                "id": 24,
                "line": "```"
            },
            {
                "id": 25,
                "line": "where $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{t})]$ is the sequence of token embeddings up to position $t$ ; $H_{t}\\in\\mathbb{R}^{t\\times d}$ is the matrix of the last hidden states for all tokens up to position $t$ ; $h_{t}$ is the last hidden state of position $t$ , i.e., $h_{t}=H_{t}[t,:];\\,e(\\cdot)$ is the token embedding function; $W$ is the parameter of the language model head.  "
            },
            {
                "id": 26,
                "line": "Method Overview. In the proposed Coconut method, the LLM switches between the \u201clanguage mode\u201d and \u201clatent mode\u201d (Figure 1). In language mode, the model operates as a standard language model, autoregressively generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding. This last hidden state represents the current reasoning state, termed as a \u201ccontinuous thought\u201d.  "
            },
            {
                "id": 27,
                "line": "Special tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought mode, respectively. As an example, we assume latent reasoning occurs between positions $i$ and $j$ , i.e., $x_{i}=$ $<b\\cot>$ and $x_{j}=<\\tt e o t>$ . When the model is in the latent mode $(i<t<j$ ), we use the last hidden state from the previous token to replace the input embedding, i.e., $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{t-1}]$  "
            },
            {
                "id": 28,
                "line": "![Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg 'Figure 2: Yes!')  "
            },
            {
                "id": 30,
                "line": "After the latent mode fnishes $(t\\ \\geq\\ j)$ , the input reverts to using the token embedding, i.e., $E_{t}~=$ $[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{j-1},e(x_{j}),...,e(x_{t})]$ . It is worth noting that the last hidden states have been processed by the fnal normalization layer, so they are not too large in magnitude. $\\mathcal{M}(x_{t+1}\\mid x_{\\leq t})$ is not defned when $i<t<j$ , since the latent thought is not intended to be mapped back to language space. However, softmax $\\left(W h_{t}\\right)$ can still be calculated for probing purposes (see Section 4).  "
            },
            {
                "id": 31,
                "line": "Training Procedure. In this work, we focus on a problem-solving setting where the model receives a question as input and is expected to generate an answer through a reasoning process. We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al. (2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the subsequent stages, at the $k$ -th stage, the frst $k$ reasoning steps in the CoT are replaced with $k\\times c$ continuous thoughts1, where $c$ is a hyperparameter controlling the number of latent thoughts replacing a single language reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We insert <bot> and <eot> tokens (which are not counted towards $c$ ) to encapsulate the continuous thoughts.  "
            },
            {
                "id": 32,
                "line": "During the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions and latent thoughts. It is important to note that the objective does not encourage the continuous thought to compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore, it\u2019s possible for the LLM to learn more efective representations of reasoning steps compared to human language.  "
            },
            {
                "id": 33,
                "line": "Training Details. Our proposed continuous thoughts are fully diferentiable and allow for back-propagation. We perform $n+1$ forward passes when $n$ latent thoughts are scheduled in the current training stage, computing a new latent thought with each pass and fnally conducting an additional forward pass to obtain a loss on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the training efciency of Coconut remains an important direction for future research.  "
            },
            {
                "id": 34,
                "line": "Inference Process. The inference process for Coconut is analogous to standard language model decoding, except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge lies in determining when to switch between latent and language modes. As we focus on the problem-solving setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two potential strategies: a) train a binary classifer on latent thoughts to enable the model to autonomously decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We found that both approaches work comparably well. Therefore, we use the second option in our experiment for simplicity, unless specifed otherwise.  "
            }
        ],
        "refined_text": "In this section, we introduce our new paradigm Coconut (Chain of Continuous Thought) for reasoning in an unconstrained latent space. We begin by introducing the background and notation we use for language models. For an input sequence $\\boldsymbol{x}=(x_{1},...,x_{T})$ , the standard large language model $\\mathcal{M}$ can be described as:  \n```latext formula_1\n$$\n\\begin{array}{c}{H_{t}=\\operatorname{Transformer}(E_{t})}\\\\ {\\mathcal{M}(x_{t+1}\\mid x_{\\le t})=\\operatorname{softmax}(W h_{t})}\\end{array}\n$$  \n```\nwhere $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{t})]$ is the sequence of token embeddings up to position $t$ ; $H_{t}\\in\\mathbb{R}^{t\\times d}$ is the matrix of the last hidden states for all tokens up to position $t$ ; $h_{t}$ is the last hidden state of position $t$ , i.e., $h_{t}=H_{t}[t,:];\\,e(\\cdot)$ is the token embedding function; $W$ is the parameter of the language model head.  \nMethod Overview. In the proposed Coconut method, the LLM switches between the \u201clanguage mode\u201d and \u201clatent mode\u201d (Figure 1). In language mode, the model operates as a standard language model, autoregressively generating the next token. In latent mode, it directly utilizes the last hidden state as the next input embedding. This last hidden state represents the current reasoning state, termed as a \u201ccontinuous thought\u201d.  \n  \nSpecial tokens <bot> and <eot> are employed to mark the beginning and end of the latent thought mode, respectively. As an example, we assume latent reasoning occurs between positions $i$ and $j$ , i.e., $x_{i}=$ $<b\\cot>$ and $x_{j}=<\\tt e o t>$ . When the model is in the latent mode $(i<t<j$ ), we use the last hidden state from the previous token to replace the input embedding, i.e., $E_{t}=[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{t-1}]$  \n    \nAfter the latent mode fnishes $(t\\ \\geq\\ j)$ , the input reverts to using the token embedding, i.e., $E_{t}~=$ $[e(x_{1}),e(x_{2}),...,e(x_{i}),h_{i},h_{i+1},...,h_{j-1},e(x_{j}),...,e(x_{t})]$ . It is worth noting that the last hidden states have been processed by the fnal normalization layer, so they are not too large in magnitude. $\\mathcal{M}(x_{t+1}\\mid x_{\\leq t})$ is not defned when $i<t<j$ , since the latent thought is not intended to be mapped back to language space. However, softmax $\\left(W h_{t}\\right)$ can still be calculated for probing purposes (see Section 4).  \nTraining Procedure. In this work, we focus on a problem-solving setting where the model receives a question as input and is expected to generate an answer through a reasoning process. We leverage language CoT data to supervise continuous thought by implementing a multi-stage training curriculum inspired by Deng et al. (2024). As shown in Figure 2, in the initial stage, the model is trained on regular CoT instances. In the subsequent stages, at the $k$ -th stage, the frst $k$ reasoning steps in the CoT are replaced with $k\\times c$ continuous thoughts1, where $c$ is a hyperparameter controlling the number of latent thoughts replacing a single language reasoning step. Following Deng et al. (2024), we also reset the optimizer state when training stages switch. We insert <bot> and <eot> tokens (which are not counted towards $c$ ) to encapsulate the continuous thoughts.  \n  \nDuring the training process, we optimize the normal negative log-likelihood loss, but mask the loss on questions and latent thoughts. It is important to note that the objective does not encourage the continuous thought to compress the removed language thought, but rather to facilitate the prediction of future reasoning. Therefore, it\u2019s possible for the LLM to learn more efective representations of reasoning steps compared to human language.  \nTraining Details. Our proposed continuous thoughts are fully diferentiable and allow for back-propagation. We perform $n+1$ forward passes when $n$ latent thoughts are scheduled in the current training stage, computing a new latent thought with each pass and fnally conducting an additional forward pass to obtain a loss on the remaining text sequence. While we can save any repetitive computing by using a KV cache, the sequential nature of the multiple forward passes poses challenges for parallelism. Further optimizing the training efciency of Coconut remains an important direction for future research.  \nInference Process. The inference process for Coconut is analogous to standard language model decoding, except that in latent mode, we directly feed the last hidden state as the next input embedding. A challenge lies in determining when to switch between latent and language modes. As we focus on the problem-solving setting, we insert a <bot> token immediately following the question tokens. For <eot>, we consider two potential strategies: a) train a binary classifer on latent thoughts to enable the model to autonomously decide when to terminate the latent reasoning, or b) always pad the latent thoughts to a constant length. We found that both approaches work comparably well. Therefore, we use the second option in our experiment for simplicity, unless specifed otherwise.",
        "images": [
            {
                "type": "image",
                "img_path": "images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg",
                "img_caption": [
                    "Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. "
                ],
                "img_footnote": [],
                "page_idx": 1,
                "id": "Figure 1",
                "related_ids": [],
                "title": "Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT).",
                "description": "Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space. \n",
                "org_md_ref": "![](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg 'new')",
                "mod_md_ref": "![Figure 1 A comparison of Chain of Continuous Thought (Coconut) with Chain-of-Thought (CoT). In CoT, the model generates the reasoning process as a word token sequence (e.g., $[x_{i},x_{i+1},...,x_{i+j}]$ in the fgure). Coconut regards the last hidden state as a representation of the reasoning state (termed \u201ccontinuous thought\u201d), and directly uses it as the next input embedding. This allows the LLM to reason in an unrestricted latent space instead of a language space.](images/3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg 'Figure 1:')  "
            },
            {
                "type": "image",
                "img_path": "images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg",
                "img_caption": [
                    "Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts. "
                ],
                "img_footnote": [],
                "page_idx": 3,
                "id": "Figure 2",
                "related_ids": [],
                "title": "Figure 2 Training procedure of Chain of Continuous Thought (Coconut).",
                "description": "Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts. \n",
                "org_md_ref": "![](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg \"Yes!\")",
                "mod_md_ref": "![Figure 2 Training procedure of Chain of Continuous Thought (Coconut). Given training data with language reasoning steps, at each training stage we integrate $c$ additional continuous thoughts ( $c=1$ in this example), and remove one language reasoning step. The cross-entropy loss is then used on the remaining tokens after continuous thoughts.](images/e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg 'Figure 2: Yes!')  "
            }
        ],
        "tables": [],
        "references": [
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "externalIds": {
                    "ArXiv": "2405.14838",
                    "DBLP": "journals/corr/abs-2405-14838",
                    "DOI": "10.48550/arXiv.2405.14838",
                    "CorpusId": 269982648
                },
                "corpusId": 269982648,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2",
                "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                "abstract": "When leveraging language models for reasoning tasks, generating explicit chain-of-thought (CoT) steps often proves essential for achieving high accuracy in final outputs. In this paper, we investigate if models can be taught to internalize these CoT steps. To this end, we propose a simple yet effective method for internalizing CoT steps: starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model. This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance. Our approach enables a GPT-2 Small model to solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication. Furthermore, our method proves effective on larger language models, such as Mistral 7B, achieving over 50% accuracy on GSM8K without producing any intermediate steps.",
                "venue": "arXiv.org",
                "year": 2024,
                "referenceCount": 16,
                "citationCount": 24,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2024-05-23",
                "journal": {
                    "volume": "abs/2405.14838",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2024FromEC,\n author = {Yuntian Deng and Yejin Choi and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step},\n volume = {abs/2405.14838},\n year = {2024}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2285939518",
                        "name": "Yejin Choi"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            }
        ]
    },
    {
        "level": 1,
        "num": 5,
        "title": "Experiments",
        "text": "We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth. The number of newly generated tokens per question is also analyzed, as a measure of reasoning efciency. We report the clock-time comparison in Appendix B.  \n## Reasoning Tasks  \nMath Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade school-level math problems. Compared to the other datasets in our experiments, the problems are more diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng et al. (2023).  \nLogical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and He, 2022) questions, with fctional concept names. For each problem, a tree-structured ontology is randomly generated and described in natural language as a set of known conditions. The model is asked to judge whether a given statement is correct based on these conditions. This serves as a simplifed simulation of more advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).  \nWe found that the generation process of ProntoQA could be more challenging, especially since the size of distracting branches in the ontology is always small, reducing the need for complex planning. To fx that, we apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions. The resulting dataset requires the model to perform substantial planning and searching over the graph to fnd the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering). A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.  \n## Experimental Setup  \nWe use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is set to $1\\times10^{-4}$ while the efective batch size is 128. Following Deng et al. (2024), we also reset the optimizer when the training stages switch.  \nMath Reasoning. By default, we use 2 latent thoughts (i.e., $c=2$ ) for each reasoning step. We analyze the correlation between performance and $c$ in Section 4.4. The model goes through 3 stages besides the initial stage. Then, we have an additional stage, where we still use $3\\times c$ continuous thoughts as in the penultimate stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in each remaining stage.  \nLogical Reasoning. We use one continuous thought for every reasoning step (i.e., $c=1$ ). The model goes through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is 6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the last stage. We train the model for 5 epochs per stage.  \nFor all datasets, after the standard schedule, the model stays in the fnal training stage, until the 50th epoch. We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the number of continuous thoughts to be consistent with their fnal training stage. We use greedy decoding for all experiments.  \n## Baselines and Variants of Coconut  \nWe consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model with supervised fnetuning, and during inference, the model generates a reasoning chain before outputting an answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain. (3) $i C o T$ (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully designed schedule that \u201cinternalizes\u201d CoT. As the training goes on, tokens at the beginning of the reasoning chain are gradually removed until only the answer remains. During inference, the model directly predicts the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer, without a reasoning chain. However, diferent from No-CoT, special <pause> tokens are inserted between the question and answer, which are believed to provide the model with additional computational capacity to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous thoughts in Coconut.  \n<html><body><table><caption>Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. \u2217The result is from Deng et al. (2024). \n</caption><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 \u00b10.2</td><td>25.0</td><td>98.8 \u00b10.8</td><td>92.5</td><td>77.5 \u00b11.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 \u00b10.5</td><td>2.2</td><td>93.8 \u00b10.7</td><td>3.0</td><td>76.7 \u00b11.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 \u00b10.3</td><td>3.0</td><td>98.2 \u00b10.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 \u00b11.8</td><td>2.2</td><td>77.7: \u00b121.0</td><td>3.0</td><td>75.9 \u00b10.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 \u00b11.5</td><td>8.2</td><td>99.8 \u00b10.2</td><td>9.0</td><td>97.0 \u00b10.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 \u00b10.8</td><td>8.2</td><td>52.4 \u00b10.4</td><td>9.0</td><td>76.1 \u00b10.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 \u00b10.5</td><td>2.3</td><td>99.9 \u00b10.1</td><td>3.0</td><td>95.5 \u00b11.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 \u00b10.7</td><td>2.2</td><td>100.0 \u00b10.1</td><td>3.0</td><td>96.6 \u00b10.8</td><td>8.2</td></tr></table></body></html>  \nWe also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training, we directly use the data from the last stage which only includes questions and answers to train Coconut. The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage training which removes language reasoning steps gradually, but don\u2019t use any continuous latent thoughts. While this is similar to $i C o\\,T$ in the high-level idea, the exact training schedule is set to be consistent with Coconut, instead of $i C o T$ . This ensures a more strict comparison. (3) Pause as thought: We use special <pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as Coconut.  \n## Results and Discussion  \nWe show the overall results on all datasets in Table 1. Continuous thoughts efectively enhance LLM reasoning, as shown by the consistent improvement over $n o{-}C o T$ . It even shows better performance than $\\mathit{C o T}$ on ProntoQA and ProsQA. We describe several key conclusions from the experiments as follows.  \n\u201cChaining\u201d continuous thoughts enhances reasoning. In conventional CoT, the output token serves as the next input, which proves to increase the efective depth of LLMs and enhance their expressiveness (Feng et al., 2023). We explore whether latent space reasoning retains this property, as it would suggest that this method could scale to solve increasingly complex problems by chaining multiple latent thoughts.  \nIn our experiments with GSM8k, we found that Coconut outperformed other architectures trained with similar strategies, particularly surpassing the latest baseline, $i C o T$ (Deng et al., 2024). The performance is signifcantly better than Coconut (pause as thought) which also enables more computation in the LLMs. While Pfau et al. (2024) empirically shows that fller tokens, such as the special <pause> tokens, can beneft highly parallelizable problems, our results show that Coconut architecture is more efective for general problems, e.g., math word problems, where a reasoning step often heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter $c$ , which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As we increased $c$ from 0 to $^{1}$ to 2, the model\u2019s performance steadily improved.2 These results suggest that a chaining efect similar to CoT can be observed in the latent space.  \n![Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts.](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg 'Figure 3:')  \nIn two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and the $i C o T$ baseline also achieve impressive accuracy. This indicates that the model\u2019s computational capacity may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering task, likely involves more complex contextual understanding and modeling, placing higher demands on computational capability.  \nLatent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires the model to \u201clook ahead\u201d and evaluate the appropriateness of each step. Among our datasets, GSM8k and ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and limited branching. In contrast, ProsQA\u2019s randomly generated DAG structure signifcantly challenges the model\u2019s planning capabilities. As shown in Table 1, $C o T$ does not ofer notable improvement over No-CoT. However, Coconut, its variants, and $i C o T$ substantially enhance reasoning on ProsQA, indicating that latent space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of this process is provided in Section 5.  \nThe LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most efective continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut $w/o$ curriculum). However, from the experimental results, we found the models trained this way do not perform any better than no-CoT.  \nWith the multi-stage curriculum which decomposes the training into easier objectives, Coconut is able to achieve top performance across various tasks. The multi-stage training also integrates well with pause tokens (Coconut- pause as thought). Despite using the same architecture and similar multi-stage training objectives, we observed a small gap between the performance of $i C o T$ and Coconut (w/o thoughts). The fner-grained removal schedule (token by token) and a few other tricks in $i C o T$ may ease the training process. We leave combining $i C o T$ and Coconut as future work. While the multi-stage training used for Coconut has proven efective, further research is defnitely needed to develop better and more general strategies for learning reasoning in latent space, especially without the supervision from language reasoning chains.  \n![Figure 4 A case study where we decode the continuous thought into language tokens.](images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg 'Figure 4:')  \nContinuous thoughts are efficient representations of reasoning. Though the continuous thoughts are not intended to be decoded to language tokens, we can  \nstill use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of a math word problem solved by Coconut $\\mathit{\\Pi}_{c}=1\\;\\;$ ). The frst continuous thought can be decoded into tokens like \u201c $\\mathrm{\\Omega}180^{\\circ}$ , \u201c $180^{\\circ}$ (with a space), and \u201c9\u201d. Note that, the reasoning trace for this problem should be $3\\times3\\times60=9\\times60=540$ , or $3\\times3\\times60=3\\times180=540$ . The interpretations of the frst thought happen to be the frst intermediate variables in the calculation. Moreover, it encodes a distribution of diferent traces into the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for planning-intense reasoning tasks.  \n![Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg 'Figure 5:')",
        "lines": [
            {
                "id": 36,
                "line": "We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth. The number of newly generated tokens per question is also analyzed, as a measure of reasoning efciency. We report the clock-time comparison in Appendix B.  "
            },
            {
                "id": 37,
                "line": "## Reasoning Tasks  "
            },
            {
                "id": 38,
                "line": "Math Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade school-level math problems. Compared to the other datasets in our experiments, the problems are more diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng et al. (2023).  "
            },
            {
                "id": 39,
                "line": "Logical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and He, 2022) questions, with fctional concept names. For each problem, a tree-structured ontology is randomly generated and described in natural language as a set of known conditions. The model is asked to judge whether a given statement is correct based on these conditions. This serves as a simplifed simulation of more advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).  "
            },
            {
                "id": 40,
                "line": "We found that the generation process of ProntoQA could be more challenging, especially since the size of distracting branches in the ontology is always small, reducing the need for complex planning. To fx that, we apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions. The resulting dataset requires the model to perform substantial planning and searching over the graph to fnd the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering). A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.  "
            },
            {
                "id": 41,
                "line": "## Experimental Setup  "
            },
            {
                "id": 42,
                "line": "We use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is set to $1\\times10^{-4}$ while the efective batch size is 128. Following Deng et al. (2024), we also reset the optimizer when the training stages switch.  "
            },
            {
                "id": 43,
                "line": "Math Reasoning. By default, we use 2 latent thoughts (i.e., $c=2$ ) for each reasoning step. We analyze the correlation between performance and $c$ in Section 4.4. The model goes through 3 stages besides the initial stage. Then, we have an additional stage, where we still use $3\\times c$ continuous thoughts as in the penultimate stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in each remaining stage.  "
            },
            {
                "id": 44,
                "line": "Logical Reasoning. We use one continuous thought for every reasoning step (i.e., $c=1$ ). The model goes through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is 6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the last stage. We train the model for 5 epochs per stage.  "
            },
            {
                "id": 45,
                "line": "For all datasets, after the standard schedule, the model stays in the fnal training stage, until the 50th epoch. We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the number of continuous thoughts to be consistent with their fnal training stage. We use greedy decoding for all experiments.  "
            },
            {
                "id": 46,
                "line": "## Baselines and Variants of Coconut  "
            },
            {
                "id": 47,
                "line": "We consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model with supervised fnetuning, and during inference, the model generates a reasoning chain before outputting an answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain. (3) $i C o T$ (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully designed schedule that \u201cinternalizes\u201d CoT. As the training goes on, tokens at the beginning of the reasoning chain are gradually removed until only the answer remains. During inference, the model directly predicts the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer, without a reasoning chain. However, diferent from No-CoT, special <pause> tokens are inserted between the question and answer, which are believed to provide the model with additional computational capacity to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous thoughts in Coconut.  "
            },
            {
                "id": 49,
                "line": "<html><body><table><caption>Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. \u2217The result is from Deng et al. (2024). "
            },
            {
                "id": 50,
                "line": "</caption><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 \u00b10.2</td><td>25.0</td><td>98.8 \u00b10.8</td><td>92.5</td><td>77.5 \u00b11.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 \u00b10.5</td><td>2.2</td><td>93.8 \u00b10.7</td><td>3.0</td><td>76.7 \u00b11.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 \u00b10.3</td><td>3.0</td><td>98.2 \u00b10.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 \u00b11.8</td><td>2.2</td><td>77.7: \u00b121.0</td><td>3.0</td><td>75.9 \u00b10.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 \u00b11.5</td><td>8.2</td><td>99.8 \u00b10.2</td><td>9.0</td><td>97.0 \u00b10.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 \u00b10.8</td><td>8.2</td><td>52.4 \u00b10.4</td><td>9.0</td><td>76.1 \u00b10.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 \u00b10.5</td><td>2.3</td><td>99.9 \u00b10.1</td><td>3.0</td><td>95.5 \u00b11.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 \u00b10.7</td><td>2.2</td><td>100.0 \u00b10.1</td><td>3.0</td><td>96.6 \u00b10.8</td><td>8.2</td></tr></table></body></html>  "
            },
            {
                "id": 51,
                "line": "We also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training, we directly use the data from the last stage which only includes questions and answers to train Coconut. The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage training which removes language reasoning steps gradually, but don\u2019t use any continuous latent thoughts. While this is similar to $i C o\\,T$ in the high-level idea, the exact training schedule is set to be consistent with Coconut, instead of $i C o T$ . This ensures a more strict comparison. (3) Pause as thought: We use special <pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as Coconut.  "
            },
            {
                "id": 52,
                "line": "## Results and Discussion  "
            },
            {
                "id": 53,
                "line": "We show the overall results on all datasets in Table 1. Continuous thoughts efectively enhance LLM reasoning, as shown by the consistent improvement over $n o{-}C o T$ . It even shows better performance than $\\mathit{C o T}$ on ProntoQA and ProsQA. We describe several key conclusions from the experiments as follows.  "
            },
            {
                "id": 54,
                "line": "\u201cChaining\u201d continuous thoughts enhances reasoning. In conventional CoT, the output token serves as the next input, which proves to increase the efective depth of LLMs and enhance their expressiveness (Feng et al., 2023). We explore whether latent space reasoning retains this property, as it would suggest that this method could scale to solve increasingly complex problems by chaining multiple latent thoughts.  "
            },
            {
                "id": 55,
                "line": "In our experiments with GSM8k, we found that Coconut outperformed other architectures trained with similar strategies, particularly surpassing the latest baseline, $i C o T$ (Deng et al., 2024). The performance is signifcantly better than Coconut (pause as thought) which also enables more computation in the LLMs. While Pfau et al. (2024) empirically shows that fller tokens, such as the special <pause> tokens, can beneft highly parallelizable problems, our results show that Coconut architecture is more efective for general problems, e.g., math word problems, where a reasoning step often heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter $c$ , which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As we increased $c$ from 0 to $^{1}$ to 2, the model\u2019s performance steadily improved.2 These results suggest that a chaining efect similar to CoT can be observed in the latent space.  "
            },
            {
                "id": 56,
                "line": "![Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts.](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg 'Figure 3:')  "
            },
            {
                "id": 58,
                "line": "In two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and the $i C o T$ baseline also achieve impressive accuracy. This indicates that the model\u2019s computational capacity may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering task, likely involves more complex contextual understanding and modeling, placing higher demands on computational capability.  "
            },
            {
                "id": 59,
                "line": "Latent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires the model to \u201clook ahead\u201d and evaluate the appropriateness of each step. Among our datasets, GSM8k and ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and limited branching. In contrast, ProsQA\u2019s randomly generated DAG structure signifcantly challenges the model\u2019s planning capabilities. As shown in Table 1, $C o T$ does not ofer notable improvement over No-CoT. However, Coconut, its variants, and $i C o T$ substantially enhance reasoning on ProsQA, indicating that latent space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of this process is provided in Section 5.  "
            },
            {
                "id": 60,
                "line": "The LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most efective continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut $w/o$ curriculum). However, from the experimental results, we found the models trained this way do not perform any better than no-CoT.  "
            },
            {
                "id": 61,
                "line": "With the multi-stage curriculum which decomposes the training into easier objectives, Coconut is able to achieve top performance across various tasks. The multi-stage training also integrates well with pause tokens (Coconut- pause as thought). Despite using the same architecture and similar multi-stage training objectives, we observed a small gap between the performance of $i C o T$ and Coconut (w/o thoughts). The fner-grained removal schedule (token by token) and a few other tricks in $i C o T$ may ease the training process. We leave combining $i C o T$ and Coconut as future work. While the multi-stage training used for Coconut has proven efective, further research is defnitely needed to develop better and more general strategies for learning reasoning in latent space, especially without the supervision from language reasoning chains.  "
            },
            {
                "id": 62,
                "line": "![Figure 4 A case study where we decode the continuous thought into language tokens.](images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg 'Figure 4:')  "
            },
            {
                "id": 64,
                "line": "Continuous thoughts are efficient representations of reasoning. Though the continuous thoughts are not intended to be decoded to language tokens, we can  "
            },
            {
                "id": 65,
                "line": "still use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of a math word problem solved by Coconut $\\mathit{\\Pi}_{c}=1\\;\\;$ ). The frst continuous thought can be decoded into tokens like \u201c $\\mathrm{\\Omega}180^{\\circ}$ , \u201c $180^{\\circ}$ (with a space), and \u201c9\u201d. Note that, the reasoning trace for this problem should be $3\\times3\\times60=9\\times60=540$ , or $3\\times3\\times60=3\\times180=540$ . The interpretations of the frst thought happen to be the frst intermediate variables in the calculation. Moreover, it encodes a distribution of diferent traces into the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for planning-intense reasoning tasks.  "
            },
            {
                "id": 66,
                "line": "![Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg 'Figure 5:')  "
            }
        ],
        "refined_text": "We validate the feasibility of LLM reasoning in a continuous latent space through experiments on three datasets. We mainly evaluate the accuracy by comparing the model-generated answers with the ground truth. The number of newly generated tokens per question is also analyzed, as a measure of reasoning efciency. We report the clock-time comparison in Appendix B.  \n## Reasoning Tasks  \nMath Reasoning. We use GSM8k (Cobbe et al., 2021) as the dataset for math reasoning. It consists of grade school-level math problems. Compared to the other datasets in our experiments, the problems are more diverse and open-domain, closely resembling real-world use cases. Through this task, we explore the potential of latent reasoning in practical applications. To train the model, we use a synthetic dataset generated by Deng et al. (2023).  \nLogical Reasoning. Logical reasoning involves the proper application of known conditions to prove or disprove a conclusion using logical rules. This requires the model to choose from multiple possible reasoning paths, where the correct decision often relies on exploration and planning ahead. We use 5-hop ProntoQA (Saparov and He, 2022) questions, with fctional concept names. For each problem, a tree-structured ontology is randomly generated and described in natural language as a set of known conditions. The model is asked to judge whether a given statement is correct based on these conditions. This serves as a simplifed simulation of more advanced reasoning tasks, such as automated theorem proving (Chen et al., 2023; DeepMind, 2024).  \nWe found that the generation process of ProntoQA could be more challenging, especially since the size of distracting branches in the ontology is always small, reducing the need for complex planning. To fx that, we apply a new dataset construction pipeline using randomly generated DAGs to structure the known conditions. The resulting dataset requires the model to perform substantial planning and searching over the graph to fnd the correct reasoning chain. We refer to this new dataset as ProsQA (Proof with Search Question-Answering). A visualized example is shown in Figure 6. More details of datasets can be found in Appendix A.  \n  \n## Experimental Setup  \nWe use a pre-trained GPT-2 (Radford et al., 2019) as the base model for all experiments. The learning rate is set to $1\\times10^{-4}$ while the efective batch size is 128. Following Deng et al. (2024), we also reset the optimizer when the training stages switch.  \nMath Reasoning. By default, we use 2 latent thoughts (i.e., $c=2$ ) for each reasoning step. We analyze the correlation between performance and $c$ in Section 4.4. The model goes through 3 stages besides the initial stage. Then, we have an additional stage, where we still use $3\\times c$ continuous thoughts as in the penultimate stage, but remove all the remaining language reasoning chain. This handles the long-tail distribution of reasoning chains longer than 3 steps. We train the model for 6 epochs in the initial stage, and 3 epochs in each remaining stage.  \nLogical Reasoning. We use one continuous thought for every reasoning step (i.e., $c=1$ ). The model goes through 6 training stages in addition to the initial stage, because the maximum number of reasoning steps is 6 in these two datasets. The model then fully reasons with continuous thoughts to solve the problems in the last stage. We train the model for 5 epochs per stage.  \nFor all datasets, after the standard schedule, the model stays in the fnal training stage, until the 50th epoch. We select the checkpoint based on the accuracy on the validation set. For inference, we manually set the number of continuous thoughts to be consistent with their fnal training stage. We use greedy decoding for all experiments.  \n## Baselines and Variants of Coconut  \nWe consider the following baselines: (1) CoT : We use the complete reasoning chains to train the language model with supervised fnetuning, and during inference, the model generates a reasoning chain before outputting an answer. (2) No-CoT : The LLM is trained to directly generate the answer without using a reasoning chain. (3) $i C o T$ (Deng et al., 2024): The model is trained with language reasoning chains and follows a carefully designed schedule that \u201cinternalizes\u201d CoT. As the training goes on, tokens at the beginning of the reasoning chain are gradually removed until only the answer remains. During inference, the model directly predicts the answer. (4) Pause token (Goyal et al., 2023): The model is trained using only the question and answer, without a reasoning chain. However, diferent from No-CoT, special <pause> tokens are inserted between the question and answer, which are believed to provide the model with additional computational capacity to derive the answer. For a fair comparison, the number of <pause> tokens is set the same as continuous thoughts in Coconut.  \n<html><body><table><caption>Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. \u2217The result is from Deng et al. (2024). \n</caption><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 \u00b10.2</td><td>25.0</td><td>98.8 \u00b10.8</td><td>92.5</td><td>77.5 \u00b11.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 \u00b10.5</td><td>2.2</td><td>93.8 \u00b10.7</td><td>3.0</td><td>76.7 \u00b11.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 \u00b10.3</td><td>3.0</td><td>98.2 \u00b10.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 \u00b11.8</td><td>2.2</td><td>77.7: \u00b121.0</td><td>3.0</td><td>75.9 \u00b10.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 \u00b11.5</td><td>8.2</td><td>99.8 \u00b10.2</td><td>9.0</td><td>97.0 \u00b10.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 \u00b10.8</td><td>8.2</td><td>52.4 \u00b10.4</td><td>9.0</td><td>76.1 \u00b10.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 \u00b10.5</td><td>2.3</td><td>99.9 \u00b10.1</td><td>3.0</td><td>95.5 \u00b11.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 \u00b10.7</td><td>2.2</td><td>100.0 \u00b10.1</td><td>3.0</td><td>96.6 \u00b10.8</td><td>8.2</td></tr></table></body></html>  \nWe also evaluate some variants of our method: (1) w/o curriculum: Instead of the multi-stage training, we directly use the data from the last stage which only includes questions and answers to train Coconut. The model uses continuous thoughts to solve the whole problem. (2) w/o thought: We keep the multi-stage training which removes language reasoning steps gradually, but don\u2019t use any continuous latent thoughts. While this is similar to $i C o\\,T$ in the high-level idea, the exact training schedule is set to be consistent with Coconut, instead of $i C o T$ . This ensures a more strict comparison. (3) Pause as thought: We use special <pause> tokens to replace the continuous thoughts, and apply the same multi-stage training curriculum as Coconut.  \n## Results and Discussion  \nWe show the overall results on all datasets in Table 1. Continuous thoughts efectively enhance LLM reasoning, as shown by the consistent improvement over $n o{-}C o T$ . It even shows better performance than $\\mathit{C o T}$ on ProntoQA and ProsQA. We describe several key conclusions from the experiments as follows.  \n\u201cChaining\u201d continuous thoughts enhances reasoning. In conventional CoT, the output token serves as the next input, which proves to increase the efective depth of LLMs and enhance their expressiveness (Feng et al., 2023). We explore whether latent space reasoning retains this property, as it would suggest that this method could scale to solve increasingly complex problems by chaining multiple latent thoughts.  \nIn our experiments with GSM8k, we found that Coconut outperformed other architectures trained with similar strategies, particularly surpassing the latest baseline, $i C o T$ (Deng et al., 2024). The performance is signifcantly better than Coconut (pause as thought) which also enables more computation in the LLMs. While Pfau et al. (2024) empirically shows that fller tokens, such as the special <pause> tokens, can beneft highly parallelizable problems, our results show that Coconut architecture is more efective for general problems, e.g., math word problems, where a reasoning step often heavily depends on previous steps. Additionally, we experimented with adjusting the hyperparameter $c$ , which controls the number of latent thoughts corresponding to one language reasoning step (Figure 3). As we increased $c$ from 0 to $^{1}$ to 2, the model\u2019s performance steadily improved.2 These results suggest that a chaining efect similar to CoT can be observed in the latent space.  \n    \nIn two other synthetic tasks, we found that the variants of Coconut (w/o thoughts or pause as thought), and the $i C o T$ baseline also achieve impressive accuracy. This indicates that the model\u2019s computational capacity may not be the bottleneck in these tasks. In contrast, GSM8k, being an open-domain question-answering task, likely involves more complex contextual understanding and modeling, placing higher demands on computational capability.  \nLatent reasoning outperforms language reasoning in planning-intensive tasks. Complex reasoning often requires the model to \u201clook ahead\u201d and evaluate the appropriateness of each step. Among our datasets, GSM8k and ProntoQA are relatively straightforward for next-step prediction, due to intuitive problem structures and limited branching. In contrast, ProsQA\u2019s randomly generated DAG structure signifcantly challenges the model\u2019s planning capabilities. As shown in Table 1, $C o T$ does not ofer notable improvement over No-CoT. However, Coconut, its variants, and $i C o T$ substantially enhance reasoning on ProsQA, indicating that latent space reasoning provides a clear advantage in tasks demanding extensive planning. An in-depth analysis of this process is provided in Section 5.  \nThe LLM still needs guidance to learn latent reasoning. In the ideal case, the model should learn the most efective continuous thoughts automatically through gradient descent on questions and answers (i.e., Coconut $w/o$ curriculum). However, from the experimental results, we found the models trained this way do not perform any better than no-CoT.  \nWith the multi-stage curriculum which decomposes the training into easier objectives, Coconut is able to achieve top performance across various tasks. The multi-stage training also integrates well with pause tokens (Coconut- pause as thought). Despite using the same architecture and similar multi-stage training objectives, we observed a small gap between the performance of $i C o T$ and Coconut (w/o thoughts). The fner-grained removal schedule (token by token) and a few other tricks in $i C o T$ may ease the training process. We leave combining $i C o T$ and Coconut as future work. While the multi-stage training used for Coconut has proven efective, further research is defnitely needed to develop better and more general strategies for learning reasoning in latent space, especially without the supervision from language reasoning chains.  \n    \nContinuous thoughts are efficient representations of reasoning. Though the continuous thoughts are not intended to be decoded to language tokens, we can  \nstill use it as an intuitive interpretation of the continuous thought. We show a case study in Figure 4 of a math word problem solved by Coconut $\\mathit{\\Pi}_{c}=1\\;\\;$ ). The frst continuous thought can be decoded into tokens like \u201c $\\mathrm{\\Omega}180^{\\circ}$ , \u201c $180^{\\circ}$ (with a space), and \u201c9\u201d. Note that, the reasoning trace for this problem should be $3\\times3\\times60=9\\times60=540$ , or $3\\times3\\times60=3\\times180=540$ . The interpretations of the frst thought happen to be the frst intermediate variables in the calculation. Moreover, it encodes a distribution of diferent traces into the continuous thoughts. As shown in Section 5.3, this feature enables a more advanced reasoning pattern for planning-intense reasoning tasks.  \n  \n  ",
        "images": [
            {
                "type": "image",
                "img_path": "images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg",
                "img_caption": [
                    "Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly. "
                ],
                "img_footnote": [],
                "page_idx": 8,
                "id": "Figure 6",
                "related_ids": [],
                "title": "Figure 6 A case study of ProsQA.",
                "description": "Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly. \n",
                "org_md_ref": "![](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg)",
                "mod_md_ref": "![Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly.](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg 'Figure 6:')  "
            },
            {
                "type": "image",
                "img_path": "images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg",
                "img_caption": [
                    "Figure 4 A case study where we decode the continuous thought into language tokens. "
                ],
                "img_footnote": [],
                "page_idx": 6,
                "id": "Figure 4",
                "related_ids": [],
                "title": "Figure 4 A case study where we decode the continuous thought into language tokens.",
                "description": "Figure 4 A case study where we decode the continuous thought into language tokens. \n",
                "org_md_ref": "![](  images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg  )",
                "mod_md_ref": "![Figure 4 A case study where we decode the continuous thought into language tokens.](images/1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg 'Figure 4:')  "
            }
        ],
        "tables": [],
        "references": [
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "4411e6b32865933cab87696c2738cb7a204e4240",
                "externalIds": {
                    "ArXiv": "2311.01460",
                    "DBLP": "journals/corr/abs-2311-01460",
                    "DOI": "10.48550/arXiv.2311.01460",
                    "CorpusId": 264935229
                },
                "corpusId": 264935229,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4411e6b32865933cab87696c2738cb7a204e4240",
                "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
                "abstract": "To augment language models with the ability to reason, researchers usually prompt or finetune them to produce chain of thought reasoning steps before producing the final answer. However, although people use natural language to reason effectively, it may be that LMs could reason more effectively with some intermediate computation that is not in natural language. In this work, we explore an alternative reasoning approach: instead of explicitly producing the chain of thought reasoning steps, we use the language model's internal hidden states to perform implicit reasoning. The implicit reasoning steps are distilled from a teacher model trained on explicit chain-of-thought reasoning, and instead of doing reasoning\"horizontally\"by producing intermediate words one-by-one, we distill it such that the reasoning happens\"vertically\"among the hidden states in different layers. We conduct experiments on a multi-digit multiplication task and a grade school math problem dataset and find that this approach enables solving tasks previously not solvable without explicit chain-of-thought, at a speed comparable to no chain-of-thought.",
                "venue": "arXiv.org",
                "year": 2023,
                "referenceCount": 47,
                "citationCount": 33,
                "influentialCitationCount": 3,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Education",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-11-02",
                "journal": {
                    "volume": "abs/2311.01460",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Deng2023ImplicitCO,\n author = {Yuntian Deng and Kiran Prasad and Roland Fernandez and P. Smolensky and Vishrav Chaudhary and Stuart Shieber},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Implicit Chain of Thought Reasoning via Knowledge Distillation},\n volume = {abs/2311.01460},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2268298457",
                        "name": "Yuntian Deng"
                    },
                    {
                        "authorId": "2264135318",
                        "name": "Kiran Prasad"
                    },
                    {
                        "authorId": "2097712360",
                        "name": "Roland Fernandez"
                    },
                    {
                        "authorId": "1748557",
                        "name": "P. Smolensky"
                    },
                    {
                        "authorId": "113810201",
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "authorId": "2264965387",
                        "name": "Stuart Shieber"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
                "externalIds": {
                    "MAG": "2955855238",
                    "CorpusId": 160025533
                },
                "corpusId": 160025533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
                "title": "Language Models are Unsupervised Multitask Learners",
                "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                "venue": "",
                "year": 2019,
                "referenceCount": 75,
                "citationCount": 20315,
                "influentialCitationCount": 3229,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Linguistics",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": null,
                "publicationDate": null,
                "journal": {
                    "volume": "",
                    "name": ""
                },
                "citationStyles": {
                    "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and R. Child and D. Luan and Dario Amodei and I. Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "38909097",
                        "name": "Alec Radford"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "48422824",
                        "name": "R. Child"
                    },
                    {
                        "authorId": "150970919",
                        "name": "D. Luan"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "1701686",
                        "name": "I. Sutskever"
                    }
                ]
            },
            {
                "paperId": "abe90a291e7cf567ce5c9012a692beeae153068d",
                "externalIds": {
                    "ArXiv": "2310.02226",
                    "DBLP": "conf/iclr/GoyalJRMKN24",
                    "DOI": "10.48550/arXiv.2310.02226",
                    "CorpusId": 263608983
                },
                "corpusId": 263608983,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/abe90a291e7cf567ce5c9012a692beeae153068d",
                "title": "Think before you speak: Training Language Models With Pause Tokens",
                "abstract": "Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.",
                "venue": "International Conference on Learning Representations",
                "year": 2023,
                "referenceCount": 67,
                "citationCount": 59,
                "influentialCitationCount": 7,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "https://arxiv.org/pdf/2310.02226",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2023-10-03",
                "journal": {
                    "volume": "abs/2310.02226",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Goyal2023ThinkBY,\n author = {Sachin Goyal and Ziwei Ji and A. Rawat and A. Menon and Sanjiv Kumar and Vaishnavh Nagarajan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Think before you speak: Training Language Models With Pause Tokens},\n volume = {abs/2310.02226},\n year = {2023}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2253470854",
                        "name": "Sachin Goyal"
                    },
                    {
                        "authorId": "2253445320",
                        "name": "Ziwei Ji"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2844480",
                        "name": "A. Menon"
                    },
                    {
                        "authorId": "2254172047",
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "authorId": "34602162",
                        "name": "Vaishnavh Nagarajan"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            },
            {
                "paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01240",
                    "ArXiv": "2210.01240",
                    "DOI": "10.48550/arXiv.2210.01240",
                    "CorpusId": 252693237
                },
                "corpusId": 252693237,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
                "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
                "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
                "venue": "International Conference on Learning Representations",
                "year": 2022,
                "referenceCount": 36,
                "citationCount": 215,
                "influentialCitationCount": 33,
                "isOpenAccess": true,
                "openAccessPdf": {
                    "url": "http://arxiv.org/pdf/2210.01240",
                    "status": null
                },
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2022-10-03",
                "journal": {
                    "volume": "abs/2210.01240",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Saparov2022LanguageMA,\n author = {Abulhair Saparov and He He},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},\n volume = {abs/2210.01240},\n year = {2022}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "2407368",
                        "name": "Abulhair Saparov"
                    },
                    {
                        "authorId": "144533687",
                        "name": "He He"
                    }
                ]
            },
            {
                "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "externalIds": {
                    "ArXiv": "2110.14168",
                    "DBLP": "journals/corr/abs-2110-14168",
                    "CorpusId": 239998651
                },
                "corpusId": 239998651,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6045d2ccc9c09ca1671348de86d07da6bc28eea",
                "title": "Training Verifiers to Solve Math Word Problems",
                "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
                "venue": "arXiv.org",
                "year": 2021,
                "referenceCount": 31,
                "citationCount": 2849,
                "influentialCitationCount": 730,
                "isOpenAccess": false,
                "openAccessPdf": null,
                "fieldsOfStudy": [
                    "Computer Science"
                ],
                "s2FieldsOfStudy": [
                    {
                        "category": "Computer Science",
                        "source": "external"
                    },
                    {
                        "category": "Mathematics",
                        "source": "s2-fos-model"
                    },
                    {
                        "category": "Computer Science",
                        "source": "s2-fos-model"
                    }
                ],
                "publicationTypes": [
                    "JournalArticle"
                ],
                "publicationDate": "2021-10-27",
                "journal": {
                    "volume": "abs/2110.14168",
                    "name": "ArXiv"
                },
                "citationStyles": {
                    "bibtex": "@Article{Cobbe2021TrainingVT,\n author = {K. Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Training Verifiers to Solve Math Word Problems},\n volume = {abs/2110.14168},\n year = {2021}\n}\n"
                },
                "authors": [
                    {
                        "authorId": "6062736",
                        "name": "K. Cobbe"
                    },
                    {
                        "authorId": "13622184",
                        "name": "Vineet Kosaraju"
                    },
                    {
                        "authorId": "2400764",
                        "name": "Mohammad Bavarian"
                    },
                    {
                        "authorId": "2108828435",
                        "name": "Mark Chen"
                    },
                    {
                        "authorId": "35450887",
                        "name": "Heewoo Jun"
                    },
                    {
                        "authorId": "40527594",
                        "name": "Lukasz Kaiser"
                    },
                    {
                        "authorId": "3407285",
                        "name": "Matthias Plappert"
                    },
                    {
                        "authorId": "2065005836",
                        "name": "Jerry Tworek"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "7406311",
                        "name": "Reiichiro Nakano"
                    },
                    {
                        "authorId": "144239765",
                        "name": "Christopher Hesse"
                    },
                    {
                        "authorId": "47971768",
                        "name": "John Schulman"
                    }
                ]
            }
        ]
    },
    {
        "level": 1,
        "num": 6,
        "title": "Understanding the Latent Reasoning in Coconut",
        "text": "In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By leveraging its ability to switch between language and latent space reasoning, we are able to control the model to interpolate between fully latent reasoning and fully language reasoning and test their performance (Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3). Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).  \n## Experimental Setup  \nMethods. The design of Coconut allows us to control the number of latent thoughts by manually setting the position of the $<\\tt e o t>$ token during inference. When we enforce Coconut to use $k$ continuous thoughts, the model is expected to output the remaining reasoning chain in language, starting from the $k+1$ step. In our experiments, we test variants of Coconut on ProsQA with $k\\in\\{0,1,2,3,4,5,6\\}$ . Note that all these variants only difer in inference time while sharing the same model weights. Besides, we report the performance of $\\mathit{C o T}$ and no-CoT as references.  \nTo address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum by always mixing data from other stages with a certain probability $p=0.3$ ). This updated training curriculum yields similar performance and enables efective control over the switch between latent and language reasoning.  \nMetrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the fnal answer, regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4). To enable fne-grained analysis, we defne another metric on the reasoning process. Assuming we have a complete language reasoning chain which specifes a path in the graph, we can classify it into (1) Correct Path: The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one being asked. These four categories naturally apply to the output from Coconut ( $k=0$ ) and $C o T$ , which generate the full path. For Coconut with $k>0$ that outputs only partial paths in language (with the initial steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete it. Also, we defne Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes the path, it\u2019s classifed as hallucination. In no-CoT and Coconut with larger $k$ , the model may only output the fnal answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six categories cover all cases without overlap.  \n![Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly.](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg 'Figure 6:')  \n![Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., \u201clempus\u201d in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg 'Figure 7:')  \n## Interpolating between Latent and Language Reasoning  \nFigure 5 shows a comparative analysis of diferent reasoning methods on ProsQA. As more reasoning is done with continuous thoughts (increasing $k$ ), both fnal answer accuracy (Figure 5, left) and the rate of correct reasoning processes (\u201cCorrect Label\u201d and \u201cCorrect Path\u201d in Figure 5, right) improve. Additionally, the rate of \u201cHallucination\u201d and \u201cWrong Target\u201d decrease, which typically occur when the model makes a wrong move earlier. This also indicates the better planning ability when more reasoning happens in the latent space.  \nA case study is shown in Figure 6, where $\\mathit{C o T}$ hallucinates an nonexistent edge, Coconut ( $k=1$ ) leads to a wrong target, but Coconut ( $k=2$ ) successfully solves the problem. In this example, the model cannot accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in Section 5.3.  \nThe comparison between $\\mathit{C o T}$ and Coconut ( $k=0$ ) reveals another interesting observation: even when  \n![Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model\u2019s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model\u2019s transition toward more focused exploration in later stages.](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg 'Figure 8:')  \nCoconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than $\\mathit{C o T}$ . The generated reasoning paths are also more accurate with less hallucination. From this, we can infer that the training method of mixing diferent stages improves the model\u2019s ability to plan ahead. The training objective of $\\mathit{C o T}$ always concentrates on the generation of the immediate next step, making the model \u201cshortsighted\u201d. In later stages of Coconut training, the frst few steps are hidden, allowing the model to focus more on future steps. This is related to the fndings of Gloeckle et al. (2024), where they propose multi-token prediction as a new pretraining objective to improve the LLM\u2019s ability to plan ahead.  \n## Interpreting the Latent Search Tree  \nGiven the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning can be interpreted as a search tree, rather than merely a reasoning \u201cchain\u201d. Taking the case of Figure 6 as a concrete example, the frst step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus, grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the frontier nodes will be the grandchildren of Alex (Figure 7, right).  \nUnlike a standard breadth-frst search (BFS), which explores all frontier nodes uniformly, the model demonstrates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model\u2019s preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to switch back to language space after a single latent thought ( $k=1$ ), it predicts the next step in a structured format, such as \u201cevery [Concept A] is a [Concept B].\u201d By examining the probability distribution over potential fllers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left). Similarly, when $k=2$ , the prediction probabilities for all frontier nodes\u2014the grandchildren of Alex \u2014are obtained (Figure 7, right).  \nThe probability distribution can be viewed as the model\u2019s implicit value function, estimating each node\u2019s potential to reach the target. As shown in the fgure, \u201clempus\u201d, \u201czhorpus\u201d, \u201cgrimpus\u201d, and \u201csterpus\u201d have a value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the frst continuous thought, the model has mostly ruled out \u201csterpus\u201d as an option but remains uncertain about the correct choice among the other three. In the second thought, however, the model has mostly ruled out other options but focused on \u201crorpus\u201d.  \nFigure 8 presents an analysis of the parallelism in the model\u2019s latent reasoning across the frst and second thoughts. For the frst thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps between the three lines indicate that the model maintains signifcant diversity in its reasoning paths at this stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel) show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most promising paths.  \n## Why is a Latent Space Better for Planning?  \nIn this section, we explore why latent reasoning is advantageous for planning, drawing on the search tree perspective and the value function defned earlier. Referring to our illustrative example, a key distinction between \u201csterpus\u201d and the other three options lies in the structure of the search tree: \u201csterpus\u201d is a leaf node (Figure 6). This makes it immediately identifable as an incorrect choice, as it cannot lead to the target node \u201cbompus\u201d. In contrast, the other nodes have more descendants to explore, making their evaluation more challenging.  \nTo quantify a node\u2019s exploratory potential, we measure its height in the tree, defned as the shortest distance to any leaf node. Based on this notion, we hypothesize that nodes with lower heights are easier to evaluate accurately, as their exploratory potential is limited. Consistent with this hypothesis, in our example, the model exhibits greater uncertainty between \u201cgrimpus\u201d and \u201clempus\u201d, both of which have a height of 2\u2014higher than the other candidates.  \nTo test this hypothesis more rigorously, we analyze the correlation between the model\u2019s prediction probabilities and node heights during the frst and second latent reasoning steps across the test set. Figure 9 reveals a clear pattern: the model successfully assigns lower values to incorrect nodes and higher values to correct nodes when their heights are low. However, as node heights increase, this distinction becomes less pronounced, indicating greater difculty in accurate evaluation.  \n![Figure 9 The correlation between prediction probability of concepts and their heights.](images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg 'Figure 9:')  \nIn conclusion, these fndings highlight the benefts of leveraging latent space for planning. By delaying defnite decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search tree\u2019s terminal states, making it easier to distinguish correct nodes from incorrect ones.",
        "lines": [
            {
                "id": 69,
                "line": "In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By leveraging its ability to switch between language and latent space reasoning, we are able to control the model to interpolate between fully latent reasoning and fully language reasoning and test their performance (Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3). Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).  "
            },
            {
                "id": 70,
                "line": "## Experimental Setup  "
            },
            {
                "id": 71,
                "line": "Methods. The design of Coconut allows us to control the number of latent thoughts by manually setting the position of the $<\\tt e o t>$ token during inference. When we enforce Coconut to use $k$ continuous thoughts, the model is expected to output the remaining reasoning chain in language, starting from the $k+1$ step. In our experiments, we test variants of Coconut on ProsQA with $k\\in\\{0,1,2,3,4,5,6\\}$ . Note that all these variants only difer in inference time while sharing the same model weights. Besides, we report the performance of $\\mathit{C o T}$ and no-CoT as references.  "
            },
            {
                "id": 72,
                "line": "To address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum by always mixing data from other stages with a certain probability $p=0.3$ ). This updated training curriculum yields similar performance and enables efective control over the switch between latent and language reasoning.  "
            },
            {
                "id": 73,
                "line": "Metrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the fnal answer, regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4). To enable fne-grained analysis, we defne another metric on the reasoning process. Assuming we have a complete language reasoning chain which specifes a path in the graph, we can classify it into (1) Correct Path: The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one being asked. These four categories naturally apply to the output from Coconut ( $k=0$ ) and $C o T$ , which generate the full path. For Coconut with $k>0$ that outputs only partial paths in language (with the initial steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete it. Also, we defne Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes the path, it\u2019s classifed as hallucination. In no-CoT and Coconut with larger $k$ , the model may only output the fnal answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six categories cover all cases without overlap.  "
            },
            {
                "id": 74,
                "line": "![Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly.](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg 'Figure 6:')  "
            },
            {
                "id": 76,
                "line": "![Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., \u201clempus\u201d in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg 'Figure 7:')  "
            },
            {
                "id": 78,
                "line": "## Interpolating between Latent and Language Reasoning  "
            },
            {
                "id": 79,
                "line": "Figure 5 shows a comparative analysis of diferent reasoning methods on ProsQA. As more reasoning is done with continuous thoughts (increasing $k$ ), both fnal answer accuracy (Figure 5, left) and the rate of correct reasoning processes (\u201cCorrect Label\u201d and \u201cCorrect Path\u201d in Figure 5, right) improve. Additionally, the rate of \u201cHallucination\u201d and \u201cWrong Target\u201d decrease, which typically occur when the model makes a wrong move earlier. This also indicates the better planning ability when more reasoning happens in the latent space.  "
            },
            {
                "id": 80,
                "line": "A case study is shown in Figure 6, where $\\mathit{C o T}$ hallucinates an nonexistent edge, Coconut ( $k=1$ ) leads to a wrong target, but Coconut ( $k=2$ ) successfully solves the problem. In this example, the model cannot accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in Section 5.3.  "
            },
            {
                "id": 81,
                "line": "The comparison between $\\mathit{C o T}$ and Coconut ( $k=0$ ) reveals another interesting observation: even when  "
            },
            {
                "id": 82,
                "line": "![Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model\u2019s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model\u2019s transition toward more focused exploration in later stages.](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg 'Figure 8:')  "
            },
            {
                "id": 84,
                "line": "Coconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than $\\mathit{C o T}$ . The generated reasoning paths are also more accurate with less hallucination. From this, we can infer that the training method of mixing diferent stages improves the model\u2019s ability to plan ahead. The training objective of $\\mathit{C o T}$ always concentrates on the generation of the immediate next step, making the model \u201cshortsighted\u201d. In later stages of Coconut training, the frst few steps are hidden, allowing the model to focus more on future steps. This is related to the fndings of Gloeckle et al. (2024), where they propose multi-token prediction as a new pretraining objective to improve the LLM\u2019s ability to plan ahead.  "
            },
            {
                "id": 85,
                "line": "## Interpreting the Latent Search Tree  "
            },
            {
                "id": 86,
                "line": "Given the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning can be interpreted as a search tree, rather than merely a reasoning \u201cchain\u201d. Taking the case of Figure 6 as a concrete example, the frst step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus, grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the frontier nodes will be the grandchildren of Alex (Figure 7, right).  "
            },
            {
                "id": 87,
                "line": "Unlike a standard breadth-frst search (BFS), which explores all frontier nodes uniformly, the model demonstrates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model\u2019s preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to switch back to language space after a single latent thought ( $k=1$ ), it predicts the next step in a structured format, such as \u201cevery [Concept A] is a [Concept B].\u201d By examining the probability distribution over potential fllers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left). Similarly, when $k=2$ , the prediction probabilities for all frontier nodes\u2014the grandchildren of Alex \u2014are obtained (Figure 7, right).  "
            },
            {
                "id": 88,
                "line": "The probability distribution can be viewed as the model\u2019s implicit value function, estimating each node\u2019s potential to reach the target. As shown in the fgure, \u201clempus\u201d, \u201czhorpus\u201d, \u201cgrimpus\u201d, and \u201csterpus\u201d have a value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the frst continuous thought, the model has mostly ruled out \u201csterpus\u201d as an option but remains uncertain about the correct choice among the other three. In the second thought, however, the model has mostly ruled out other options but focused on \u201crorpus\u201d.  "
            },
            {
                "id": 89,
                "line": "Figure 8 presents an analysis of the parallelism in the model\u2019s latent reasoning across the frst and second thoughts. For the frst thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps between the three lines indicate that the model maintains signifcant diversity in its reasoning paths at this stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel) show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most promising paths.  "
            },
            {
                "id": 90,
                "line": "## Why is a Latent Space Better for Planning?  "
            },
            {
                "id": 91,
                "line": "In this section, we explore why latent reasoning is advantageous for planning, drawing on the search tree perspective and the value function defned earlier. Referring to our illustrative example, a key distinction between \u201csterpus\u201d and the other three options lies in the structure of the search tree: \u201csterpus\u201d is a leaf node (Figure 6). This makes it immediately identifable as an incorrect choice, as it cannot lead to the target node \u201cbompus\u201d. In contrast, the other nodes have more descendants to explore, making their evaluation more challenging.  "
            },
            {
                "id": 92,
                "line": "To quantify a node\u2019s exploratory potential, we measure its height in the tree, defned as the shortest distance to any leaf node. Based on this notion, we hypothesize that nodes with lower heights are easier to evaluate accurately, as their exploratory potential is limited. Consistent with this hypothesis, in our example, the model exhibits greater uncertainty between \u201cgrimpus\u201d and \u201clempus\u201d, both of which have a height of 2\u2014higher than the other candidates.  "
            },
            {
                "id": 93,
                "line": "To test this hypothesis more rigorously, we analyze the correlation between the model\u2019s prediction probabilities and node heights during the frst and second latent reasoning steps across the test set. Figure 9 reveals a clear pattern: the model successfully assigns lower values to incorrect nodes and higher values to correct nodes when their heights are low. However, as node heights increase, this distinction becomes less pronounced, indicating greater difculty in accurate evaluation.  "
            },
            {
                "id": 94,
                "line": "![Figure 9 The correlation between prediction probability of concepts and their heights.](images/f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg 'Figure 9:')  "
            },
            {
                "id": 96,
                "line": "In conclusion, these fndings highlight the benefts of leveraging latent space for planning. By delaying defnite decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search tree\u2019s terminal states, making it easier to distinguish correct nodes from incorrect ones.  "
            }
        ],
        "refined_text": "In this section, we present an analysis of the latent reasoning process with a variant of Coconut. By leveraging its ability to switch between language and latent space reasoning, we are able to control the model to interpolate between fully latent reasoning and fully language reasoning and test their performance (Section 5.2). This also enables us to interpret the the latent reasoning process as tree search (Section 5.3). Based on this perspective, we explain why latent reasoning can make the decision easier for LLMs (Section 5.4).  \n## Experimental Setup  \nMethods. The design of Coconut allows us to control the number of latent thoughts by manually setting the position of the $<\\tt e o t>$ token during inference. When we enforce Coconut to use $k$ continuous thoughts, the model is expected to output the remaining reasoning chain in language, starting from the $k+1$ step. In our experiments, we test variants of Coconut on ProsQA with $k\\in\\{0,1,2,3,4,5,6\\}$ . Note that all these variants only difer in inference time while sharing the same model weights. Besides, we report the performance of $\\mathit{C o T}$ and no-CoT as references.  \nTo address the issue of forgetting earlier training stages, we modify the original multi-stage training curriculum by always mixing data from other stages with a certain probability $p=0.3$ ). This updated training curriculum yields similar performance and enables efective control over the switch between latent and language reasoning.  \nMetrics. We apply two sets of evaluation metrics. One of them is based on the correctness of the fnal answer, regardless of the reasoning process. It is the metric used in the main experimental results above (Section 4.4). To enable fne-grained analysis, we defne another metric on the reasoning process. Assuming we have a complete language reasoning chain which specifes a path in the graph, we can classify it into (1) Correct Path: The output is one of the shortest paths to the correct answer. (2) Longer Path: A valid path that correctly answers the question but is longer than the shortest path. (3) Hallucination: The path includes nonexistent edges or is disconnected. (4) Wrong Target: A valid path in the graph, but the destination node is not the one being asked. These four categories naturally apply to the output from Coconut ( $k=0$ ) and $C o T$ , which generate the full path. For Coconut with $k>0$ that outputs only partial paths in language (with the initial steps in continuous reasoning), we classify the reasoning as a Correct Path if a valid explanation can complete it. Also, we defne Longer Path and Wrong Target for partial paths similarly. If no valid explanation completes the path, it\u2019s classifed as hallucination. In no-CoT and Coconut with larger $k$ , the model may only output the fnal answer without any partial path, and it falls into (5) Correct Label or (6) Incorrect Label. These six categories cover all cases without overlap.  \n    \n    \n  \n## Interpolating between Latent and Language Reasoning  \nFigure 5 shows a comparative analysis of diferent reasoning methods on ProsQA. As more reasoning is done with continuous thoughts (increasing $k$ ), both fnal answer accuracy (Figure 5, left) and the rate of correct reasoning processes (\u201cCorrect Label\u201d and \u201cCorrect Path\u201d in Figure 5, right) improve. Additionally, the rate of \u201cHallucination\u201d and \u201cWrong Target\u201d decrease, which typically occur when the model makes a wrong move earlier. This also indicates the better planning ability when more reasoning happens in the latent space.  \n  \nA case study is shown in Figure 6, where $\\mathit{C o T}$ hallucinates an nonexistent edge, Coconut ( $k=1$ ) leads to a wrong target, but Coconut ( $k=2$ ) successfully solves the problem. In this example, the model cannot accurately determine which edge to choose at the earlier step. However, as latent reasoning can avoid making a hard choice upfront, the model can progressively eliminate incorrect options in subsequent steps and achieves higher accuracy at the end of reasoning. We show more evidence and details of this reasoning process in Section 5.3.  \n  \nThe comparison between $\\mathit{C o T}$ and Coconut ( $k=0$ ) reveals another interesting observation: even when  \n    \nCoconut is forced to generate a complete reasoning chain, the accuracy of the answers is still higher than $\\mathit{C o T}$ . The generated reasoning paths are also more accurate with less hallucination. From this, we can infer that the training method of mixing diferent stages improves the model\u2019s ability to plan ahead. The training objective of $\\mathit{C o T}$ always concentrates on the generation of the immediate next step, making the model \u201cshortsighted\u201d. In later stages of Coconut training, the frst few steps are hidden, allowing the model to focus more on future steps. This is related to the fndings of Gloeckle et al. (2024), where they propose multi-token prediction as a new pretraining objective to improve the LLM\u2019s ability to plan ahead.  \n## Interpreting the Latent Search Tree  \nGiven the intuition that continuous thoughts can encode multiple potential next steps, the latent reasoning can be interpreted as a search tree, rather than merely a reasoning \u201cchain\u201d. Taking the case of Figure 6 as a concrete example, the frst step could be selecting one of the children of Alex, i.e., {lempus, sterpus, zhorpus, grimpus}. We depict all possible branches in the left part of Figure 7. Similarly, in the second step, the frontier nodes will be the grandchildren of Alex (Figure 7, right).  \n  \n  \nUnlike a standard breadth-frst search (BFS), which explores all frontier nodes uniformly, the model demonstrates the ability to prioritize promising nodes while pruning less relevant ones. To uncover the model\u2019s preferences, we analyze its subsequent outputs in language space. For instance, if the model is forced to switch back to language space after a single latent thought ( $k=1$ ), it predicts the next step in a structured format, such as \u201cevery [Concept A] is a [Concept B].\u201d By examining the probability distribution over potential fllers for [Concept A], we can derive numeric values for the children of the root node Alex (Figure 7, left). Similarly, when $k=2$ , the prediction probabilities for all frontier nodes\u2014the grandchildren of Alex \u2014are obtained (Figure 7, right).  \n  \n  \nThe probability distribution can be viewed as the model\u2019s implicit value function, estimating each node\u2019s potential to reach the target. As shown in the fgure, \u201clempus\u201d, \u201czhorpus\u201d, \u201cgrimpus\u201d, and \u201csterpus\u201d have a value of 0.33, 0.16, 0.32, and 0.01, respectively. This indicates that in the frst continuous thought, the model has mostly ruled out \u201csterpus\u201d as an option but remains uncertain about the correct choice among the other three. In the second thought, however, the model has mostly ruled out other options but focused on \u201crorpus\u201d.  \nFigure 8 presents an analysis of the parallelism in the model\u2019s latent reasoning across the frst and second thoughts. For the frst thoughts (left panel), the cumulative values of the top-1, top-2, and top-3 candidate nodes are computed and plotted against their respective percentiles across the test set. The noticeable gaps between the three lines indicate that the model maintains signifcant diversity in its reasoning paths at this stage, suggesting a broad exploration of alternative possibilities. In contrast, the second thoughts (right panel) show a narrowing of these gaps. This trend suggests that the model transitions from parallel exploration to more focused reasoning in the second latent reasoning step, likely as it gains more certainty about the most promising paths.  \n  \n## Why is a Latent Space Better for Planning?  \nIn this section, we explore why latent reasoning is advantageous for planning, drawing on the search tree perspective and the value function defned earlier. Referring to our illustrative example, a key distinction between \u201csterpus\u201d and the other three options lies in the structure of the search tree: \u201csterpus\u201d is a leaf node (Figure 6). This makes it immediately identifable as an incorrect choice, as it cannot lead to the target node \u201cbompus\u201d. In contrast, the other nodes have more descendants to explore, making their evaluation more challenging.  \n  \nTo quantify a node\u2019s exploratory potential, we measure its height in the tree, defned as the shortest distance to any leaf node. Based on this notion, we hypothesize that nodes with lower heights are easier to evaluate accurately, as their exploratory potential is limited. Consistent with this hypothesis, in our example, the model exhibits greater uncertainty between \u201cgrimpus\u201d and \u201clempus\u201d, both of which have a height of 2\u2014higher than the other candidates.  \nTo test this hypothesis more rigorously, we analyze the correlation between the model\u2019s prediction probabilities and node heights during the frst and second latent reasoning steps across the test set. Figure 9 reveals a clear pattern: the model successfully assigns lower values to incorrect nodes and higher values to correct nodes when their heights are low. However, as node heights increase, this distinction becomes less pronounced, indicating greater difculty in accurate evaluation.  \n    \nIn conclusion, these fndings highlight the benefts of leveraging latent space for planning. By delaying defnite decisions and expanding the latent reasoning process, the model pushes its exploration closer to the search tree\u2019s terminal states, making it easier to distinguish correct nodes from incorrect ones.",
        "images": [
            {
                "type": "image",
                "img_path": "images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg",
                "img_caption": [
                    "Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly. "
                ],
                "img_footnote": [],
                "page_idx": 8,
                "id": "Figure 6",
                "related_ids": [],
                "title": "Figure 6 A case study of ProsQA.",
                "description": "Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly. \n",
                "org_md_ref": "![](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg)",
                "mod_md_ref": "![Figure 6 A case study of ProsQA. The model trained with $C o T$ hallucinates an edge (Every yumpus is a rempus) after getting stuck in a dead end. Coconut $\\mathrm{k=1}$ ) outputs a path that ends with an irrelevant node. Coconut $\\scriptstyle\\mathrm{k=2}$ ) solves the problem correctly.](images/a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg 'Figure 6:')  "
            },
            {
                "type": "image",
                "img_path": "images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg",
                "img_caption": [
                    "Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA. "
                ],
                "img_footnote": [],
                "page_idx": 7,
                "id": "Figure 5",
                "related_ids": [],
                "title": "Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.",
                "description": "Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA. \n",
                "org_md_ref": "![](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg)",
                "mod_md_ref": "![Figure 5 The accuracy of fnal answer (left) and reasoning process (right) of multiple variants of Coconut and baselines on ProsQA.](images/cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg 'Figure 5:')  "
            },
            {
                "type": "image",
                "img_path": "images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg",
                "img_caption": [
                    "Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., \u201clempus\u201d in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer. "
                ],
                "img_footnote": [],
                "page_idx": 8,
                "id": "Figure 7",
                "related_ids": [
                    "Figure 6"
                ],
                "title": "Figure 7 An illustration of the latent search trees.",
                "description": "Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., \u201clempus\u201d in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer. \n",
                "org_md_ref": "![](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg)",
                "mod_md_ref": "![Figure 7 An illustration of the latent search trees. The example is the same test case as in Figure 6. The height of a node (denoted as $h$ in the fgure) is defned as the longest distance to any leaf nodes in the graph. We show the probability of the frst concept predicted by the model following latent thoughts (e.g., \u201clempus\u201d in the left fgure). It is calculated as the multiplication of the probability of all tokens within the concept conditioned on previous context (omitted in the fgure for brevity). This metric can be interpreted as an implicit value function estimated by the model, assessing the potential of each node leading to the correct answer.](images/b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg 'Figure 7:')  "
            },
            {
                "type": "image",
                "img_path": "images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg",
                "img_caption": [
                    "Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model\u2019s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model\u2019s transition toward more focused exploration in later stages. "
                ],
                "img_footnote": [],
                "page_idx": 9,
                "id": "Figure 8",
                "related_ids": [
                    "picts"
                ],
                "title": "Figure 8 Analysis of parallelism in latent tree search.",
                "description": "Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model\u2019s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model\u2019s transition toward more focused exploration in later stages. \n",
                "org_md_ref": "![](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg)",
                "mod_md_ref": "![Figure 8 Analysis of parallelism in latent tree search. The left plot depicts the cumulative value of the top-1, top-2, and top-3 candidate nodes for the frst thoughts, calculated across test cases and ranked by percentile. The signifcant gaps between the lines refect the model\u2019s ability to explore alternative latent thoughts in parallel. The right plot shows the corresponding analysis for the second thoughts, where the gaps between lines are narrower, indicating reduced parallelism and increased certainty in reasoning as the search tree develops. This shift highlights the model\u2019s transition toward more focused exploration in later stages.](images/ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg 'Figure 8:')  "
            }
        ],
        "tables": [],
        "references": []
    },
    {
        "level": 1,
        "num": 7,
        "title": "Conclusion",
        "text": "In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through extensive experiments, we demonstrated that Coconut signifcantly enhances LLM reasoning capabilities. Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an efective reasoning pattern similar to BFS. Future work is needed to further refne and scale latent reasoning methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models to generalize more efectively across a wider range of reasoning scenarios. We anticipate that our fndings will inspire further research into latent reasoning methods, ultimately contributing to the development of more advanced machine reasoning systems.",
        "lines": [
            {
                "id": 98,
                "line": "In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through extensive experiments, we demonstrated that Coconut signifcantly enhances LLM reasoning capabilities. Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an efective reasoning pattern similar to BFS. Future work is needed to further refne and scale latent reasoning methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models to generalize more efectively across a wider range of reasoning scenarios. We anticipate that our fndings will inspire further research into latent reasoning methods, ultimately contributing to the development of more advanced machine reasoning systems.  "
            }
        ],
        "refined_text": "In this paper, we presented Coconut, a novel paradigm for reasoning in continuous latent space. Through extensive experiments, we demonstrated that Coconut signifcantly enhances LLM reasoning capabilities. Notably, our detailed analysis highlighted how an unconstrained latent space allows the model to develop an efective reasoning pattern similar to BFS. Future work is needed to further refne and scale latent reasoning methods. One promising direction is pretraining LLMs with continuous thoughts, which may enable models to generalize more efectively across a wider range of reasoning scenarios. We anticipate that our fndings will inspire further research into latent reasoning methods, ultimately contributing to the development of more advanced machine reasoning systems.",
        "images": [],
        "tables": [],
        "references": []
    },
    {
        "level": 1,
        "num": 8,
        "title": "Acknowledgement",
        "text": "The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course of this work.",
        "lines": [
            {
                "id": 100,
                "line": "The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course of this work.  "
            }
        ],
        "refined_text": "The authors express their sincere gratitude to Jihoon Tack for his valuable discussions throughout the course of this work.",
        "images": [],
        "tables": [],
        "references": []
    },
    {
        "level": 1,
        "num": 9,
        "title": "References",
        "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nMarie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 189:19\u201331, 2019.   \nEden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.   \nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889\u20137901, 2023.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \nGoogle DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.   \nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.   \nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.   \nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \nYing Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024.   \nEvelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specifcity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428\u201316433, 2011.   \nEvelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575\u2013586, 2024.   \nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.   \nKanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   \nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398\u201311442. PMLR, 2023.   \nFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi\u00e8re, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   \nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.   \nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \nShibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.   \nAlex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.   \nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   \nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1\u201362, 2022.   \nLucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   \nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.   \nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For efective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.   \nWilliam Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   \nMartin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005\u20131016, 2007.   \nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive inference. Proceedings of the National Academy of Sciences, 106(30):12554\u201312559, 2009.   \nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of algebra and natural language. Psychological science, 23(8):914\u2013922, 2012.   \nJacob Pfau, William Merrill, and Samuel R Bowman. Let\u2019s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   \nAlec Radford, Jefrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   \nYuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.   \nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \nDiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.   \nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   \nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   \nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426\u20139439, 2024.   \nXinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.   \nSohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   \nShunyu Yao, Dian Yu, Jefrey Zhao, Izhak Shafran, Tom Grifths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023.   \nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efcient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.   \nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \nPing Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024b.   \nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   \nEric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.   \nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.",
        "lines": [
            {
                "id": 102,
                "line": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   "
            },
            {
                "id": 103,
                "line": "Marie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 189:19\u201331, 2019.   "
            },
            {
                "id": 104,
                "line": "Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.   "
            },
            {
                "id": 105,
                "line": "Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889\u20137901, 2023.   "
            },
            {
                "id": 106,
                "line": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   "
            },
            {
                "id": 107,
                "line": "Google DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.   "
            },
            {
                "id": 108,
                "line": "Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.   "
            },
            {
                "id": 109,
                "line": "Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.   "
            },
            {
                "id": 110,
                "line": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   "
            },
            {
                "id": 111,
                "line": "Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024.   "
            },
            {
                "id": 112,
                "line": "Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specifcity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428\u201316433, 2011.   "
            },
            {
                "id": 113,
                "line": "Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575\u2013586, 2024.   "
            },
            {
                "id": 114,
                "line": "Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.   "
            },
            {
                "id": 115,
                "line": "Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   "
            },
            {
                "id": 116,
                "line": "Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398\u201311442. PMLR, 2023.   "
            },
            {
                "id": 117,
                "line": "Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi\u00e8re, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   "
            },
            {
                "id": 118,
                "line": "Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.   "
            },
            {
                "id": 119,
                "line": "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   "
            },
            {
                "id": 120,
                "line": "Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.   "
            },
            {
                "id": 121,
                "line": "Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.   "
            },
            {
                "id": 122,
                "line": "Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   "
            },
            {
                "id": 123,
                "line": "Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1\u201362, 2022.   "
            },
            {
                "id": 124,
                "line": "Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   "
            },
            {
                "id": 125,
                "line": "Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.   "
            },
            {
                "id": 126,
                "line": "Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For efective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.   "
            },
            {
                "id": 127,
                "line": "William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   "
            },
            {
                "id": 128,
                "line": "Martin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005\u20131016, 2007.   "
            },
            {
                "id": 129,
                "line": "Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive inference. Proceedings of the National Academy of Sciences, 106(30):12554\u201312559, 2009.   "
            },
            {
                "id": 130,
                "line": "Martin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of algebra and natural language. Psychological science, 23(8):914\u2013922, 2012.   "
            },
            {
                "id": 131,
                "line": "Jacob Pfau, William Merrill, and Samuel R Bowman. Let\u2019s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   "
            },
            {
                "id": 132,
                "line": "Alec Radford, Jefrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   "
            },
            {
                "id": 133,
                "line": "Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   "
            },
            {
                "id": 134,
                "line": "Yuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.   "
            },
            {
                "id": 135,
                "line": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   "
            },
            {
                "id": 136,
                "line": "DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.   "
            },
            {
                "id": 137,
                "line": "Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   "
            },
            {
                "id": 138,
                "line": "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   "
            },
            {
                "id": 139,
                "line": "Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426\u20139439, 2024.   "
            },
            {
                "id": 140,
                "line": "Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.   "
            },
            {
                "id": 141,
                "line": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   "
            },
            {
                "id": 142,
                "line": "Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.   "
            },
            {
                "id": 143,
                "line": "Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   "
            },
            {
                "id": 144,
                "line": "Shunyu Yao, Dian Yu, Jefrey Zhao, Izhak Shafran, Tom Grifths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023.   "
            },
            {
                "id": 145,
                "line": "Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efcient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.   "
            },
            {
                "id": 146,
                "line": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   "
            },
            {
                "id": 147,
                "line": "Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024b.   "
            },
            {
                "id": 148,
                "line": "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   "
            },
            {
                "id": 149,
                "line": "Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.   "
            },
            {
                "id": 150,
                "line": "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.  "
            }
        ],
        "refined_text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \nMarie Amalric and Stanislas Dehaene. A distinct cortical network for mathematical knowledge in the human brain. NeuroImage, 189:19\u201331, 2019.   \nEden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and Amir Globerson. Hopping too late: Exploring the limitations of large language models on multi-hop queries. arXiv preprint arXiv:2406.12775, 2024.   \nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7889\u20137901, 2023.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \nGoogle DeepMind. Ai achieves silver-medal standard solving international mathematical olympiad problems, 2024. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.   \nYuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023.   \nYuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024.   \nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \nYing Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024.   \nEvelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specifcity for high-level linguistic processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428\u201316433, 2011.   \nEvelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575\u2013586, 2024.   \nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. Advances in Neural Information Processing Systems, 36, 2023.   \nKanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024.   \nAngeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398\u201311442. PMLR, 2023.   \nFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi\u00e8re, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   \nSachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023.   \nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \nShibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221, 2024.   \nAlex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024.   \nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.   \nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):1\u201362, 2022.   \nLucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\\*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024.   \nZhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. arXiv preprint arXiv:2402.12875, 2024.   \nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For efective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.   \nWilliam Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. arXiv preprint arXiv:2310.07923, 2023.   \nMartin M Monti, Daniel N Osherson, Michael J Martinez, and Lawrence M Parsons. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage, 37(3):1005\u20131016, 2007.   \nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. The boundaries of language and thought in deductive inference. Proceedings of the National Academy of Sciences, 106(30):12554\u201312559, 2009.   \nMartin M Monti, Lawrence M Parsons, and Daniel N Osherson. Thought beyond language: neural dissociation of algebra and natural language. Psychological science, 23(8):914\u2013922, 2012.   \nJacob Pfau, William Merrill, and Samuel R Bowman. Let\u2019s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024.   \nAlec Radford, Jefrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.   \nYuval Shalev, Amir Feder, and Ariel Goldstein. Distributional reasoning in llms: Parallel reasoning processes in multi-hop reasoning. arXiv preprint arXiv:2406.13858, 2024.   \nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \nDiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024.   \nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36, 2024.   \nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022.   \nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9426\u20139439, 2024.   \nXinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2023.   \nSohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, and Sebastian Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024.   \nShunyu Yao, Dian Yu, Jefrey Zhao, Izhak Shafran, Tom Grifths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023.   \nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Efcient training of llm policy with divergent thinking. arXiv preprint arXiv:2406.05673, 2024a.   \nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \nPing Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024b.   \nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.   \nEric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024.   \nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.",
        "images": [],
        "tables": [],
        "references": []
    },
    {
        "level": 1,
        "num": 10,
        "title": "Datasets",
        "text": "## Examples  \nexamples of the questions and CoT solutions for the datasets used  \n### GSM8k  \nQuestion $=$ \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $\\Phi100$ to get his grass cut. How much does he pay per year?\"  \n$\\mathrm{Steps}=[\"*4\u20132{=}2*\"$ , $\"\\!\\ll\\!2/.5\\!\\!=\\!\\!4\\!\\gg\\!\"$ , $\"\\ll12/4{=}3*\"$ \", $\"\\!\\ll\\!100\\!*\\!3\\!=\\!300\\!*\\!\"]$ Answer $=~\"300\"$ \"  \n#### ProntoQA  \nQuestion $=$ \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus. Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are not foral. Lempuses are cold. Brimpuses are impuses. Every lorpus is foral. Every rompus is transparent. Grimpuses are mufed. Rompuses are yumpuses. Rompuses are wumpuses. Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus. Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses. Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not foral.\"  \nSteps $=$ [\"Stella is a zumpus. Zumpuses are gorpuses.\", \"Stella is a gorpus. Gorpuses are rompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus is a lorpus.\", \"Stella is a lorpus. Every lorpus is foral.\", \"Stella is foral.\"] An $\\mathsf{\\Pi}_{\\mathrm{Wer}}=\\mathsf{\\Pi}^{\\mathsf{W}}\\mathbb{F}\\mathsf{a l s}.$ e\"  \n##### ProsQA  \nQuestion $=$ \"Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus. Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus. Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus. Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?\"   \nSteps $=$ [\"Tom is a terpus.\", \"Every terpus is a brimpus.\", \"Every brimpus is a lempus.\"] An $\\mathtt{s w e r=\"10m}$ is a lempus.\"  \n## Construction of ProsQA  \nTo construct the dataset, we frst compile a set of typical entity names, such as \u201cAlex\u201d and \u201cJack,\u201d along with fctional concept names like \u201clorpus\u201d and \u201crorpus,\u201d following the setting of ProntoQA (Saparov and He, 2022). Each problem is structured as a binary question: \u201cIs [Entity] a [Concept A] or [Concept B]?\u201d Assuming [Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to [Concept B].  \nAlgorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation maintains distinct families of nodes and balances their sizes to prevent model shortcuts.  \nTable 2 Statistics of the graph structure in ProsQA.   \n<html><body><table><caption>Table 2 Statistics of the graph structure in ProsQA. \n</caption><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>  \nAfter the graph is constructed, nodes without parents are assigned entity names, while other nodes receive concept names. To formulate a question of the form \u201cIs [Entity] a [Concept A] or [Concept B]?\u201d, we designate node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B]. This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are randomly permuted in each question.  \n### Algorithm 1 Graph Construction for ProsQA  \n$e d g e s\\gets\\{\\}$   \n$n o d e s\\gets\\{0,1\\}$   \n$l a b e l s\\gets\\{0:1,1:2\\}$ $\\triangleright$ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).   \n$g r o u p s\\gets\\{0:\\{\\},1:\\{0\\},2:\\{1\\},3:\\{\\}\\}$   \n$i d x\\gets2$   \nwhile $i d x<N$ do \u25b7 For each new node, randomly add edges from existing nodes $n\\_i n\\_n o d e s\\gets\\mathrm{poisson}($ (1.5) $r a n d\\gets\\mathrm{random}()$ if $r a n d\\leq0.35$ then candidat $z\\leftarrow g r o u p s[0]\\cup g r o u p s[1]$ \u25b7 Cannot be a descendant of node 1. else if $r a n d\\leq0.7$ then candidates $\\leftarrow$ groups[0] \u222a groups[2] \u25b7 Cannot be a descendant of node 0. else candidates \u2190 nodes end if $n_{-}i n_{-}n o d e s\\gets\\operatorname*{min}(\\log(c a n d i d a t e s),n_{-}i n_{-}n o d e s)$ weights $\\leftarrow$ [depth_to_root(c) \u00b7 1.5 + 1 \u2200c \u2208 candidates] \u25b7 Defne sampling weights to prioritize deeper nodes. $\\triangleright$ This way, the solution reasoning chain is expected to be longer. in_nodes $\\leftarrow$ random_choice(candidates, n_in_nodes, prob = weights/sum(weights)) $c u r\\_l a b e l\\gets0$ for $i n\\_i d x\\in i n_{.}$ _nodes do cur_label \u2190 cur_label | labels[in_idx] \u25b7 Update label using bitwise OR. edges.append((in_idx, idx)) end for groups[cur_label].append(idx) labels[idx] \u2190 cur_label $n o d e s\\overleftarrow{}\\leftarrow{\\mathit{n o d e s}}\\cup\\{i d x\\}$ idx idx + 1   \nend while  \n## Statistics  \nWe show the size of all datasets in Table 3.  \nTable 3 Statistics of the datasets.   \n<html><body><table><caption>Table 3 Statistics of the datasets. \n</caption><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>",
        "lines": [
            {
                "id": 153,
                "line": "## Examples  "
            },
            {
                "id": 154,
                "line": "examples of the questions and CoT solutions for the datasets used  "
            },
            {
                "id": 155,
                "line": "### GSM8k  "
            },
            {
                "id": 156,
                "line": "Question $=$ \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $\\Phi100$ to get his grass cut. How much does he pay per year?\"  "
            },
            {
                "id": 157,
                "line": "$\\mathrm{Steps}=[\"*4\u20132{=}2*\"$ , $\"\\!\\ll\\!2/.5\\!\\!=\\!\\!4\\!\\gg\\!\"$ , $\"\\ll12/4{=}3*\"$ \", $\"\\!\\ll\\!100\\!*\\!3\\!=\\!300\\!*\\!\"]$ Answer $=~\"300\"$ \"  "
            },
            {
                "id": 158,
                "line": "#### ProntoQA  "
            },
            {
                "id": 159,
                "line": "Question $=$ \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus. Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are not foral. Lempuses are cold. Brimpuses are impuses. Every lorpus is foral. Every rompus is transparent. Grimpuses are mufed. Rompuses are yumpuses. Rompuses are wumpuses. Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus. Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses. Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not foral.\"  "
            },
            {
                "id": 160,
                "line": "Steps $=$ [\"Stella is a zumpus. Zumpuses are gorpuses.\", \"Stella is a gorpus. Gorpuses are rompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus is a lorpus.\", \"Stella is a lorpus. Every lorpus is foral.\", \"Stella is foral.\"] An $\\mathsf{\\Pi}_{\\mathrm{Wer}}=\\mathsf{\\Pi}^{\\mathsf{W}}\\mathbb{F}\\mathsf{a l s}.$ e\"  "
            },
            {
                "id": 161,
                "line": "##### ProsQA  "
            },
            {
                "id": 162,
                "line": "Question $=$ \"Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus. Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus. Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus. Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?\"   "
            },
            {
                "id": 163,
                "line": "Steps $=$ [\"Tom is a terpus.\", \"Every terpus is a brimpus.\", \"Every brimpus is a lempus.\"] An $\\mathtt{s w e r=\"10m}$ is a lempus.\"  "
            },
            {
                "id": 164,
                "line": "## Construction of ProsQA  "
            },
            {
                "id": 165,
                "line": "To construct the dataset, we frst compile a set of typical entity names, such as \u201cAlex\u201d and \u201cJack,\u201d along with fctional concept names like \u201clorpus\u201d and \u201crorpus,\u201d following the setting of ProntoQA (Saparov and He, 2022). Each problem is structured as a binary question: \u201cIs [Entity] a [Concept A] or [Concept B]?\u201d Assuming [Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to [Concept B].  "
            },
            {
                "id": 166,
                "line": "Algorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation maintains distinct families of nodes and balances their sizes to prevent model shortcuts.  "
            },
            {
                "id": 167,
                "line": "Table 2 Statistics of the graph structure in ProsQA.   "
            },
            {
                "id": 168,
                "line": "<html><body><table><caption>Table 2 Statistics of the graph structure in ProsQA. "
            },
            {
                "id": 169,
                "line": "</caption><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>  "
            },
            {
                "id": 170,
                "line": "After the graph is constructed, nodes without parents are assigned entity names, while other nodes receive concept names. To formulate a question of the form \u201cIs [Entity] a [Concept A] or [Concept B]?\u201d, we designate node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B]. This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are randomly permuted in each question.  "
            },
            {
                "id": 171,
                "line": "### Algorithm 1 Graph Construction for ProsQA  "
            },
            {
                "id": 172,
                "line": "$e d g e s\\gets\\{\\}$   "
            },
            {
                "id": 173,
                "line": "$n o d e s\\gets\\{0,1\\}$   "
            },
            {
                "id": 174,
                "line": "$l a b e l s\\gets\\{0:1,1:2\\}$ $\\triangleright$ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).   "
            },
            {
                "id": 175,
                "line": "$g r o u p s\\gets\\{0:\\{\\},1:\\{0\\},2:\\{1\\},3:\\{\\}\\}$   "
            },
            {
                "id": 176,
                "line": "$i d x\\gets2$   "
            },
            {
                "id": 177,
                "line": "while $i d x<N$ do \u25b7 For each new node, randomly add edges from existing nodes $n\\_i n\\_n o d e s\\gets\\mathrm{poisson}($ (1.5) $r a n d\\gets\\mathrm{random}()$ if $r a n d\\leq0.35$ then candidat $z\\leftarrow g r o u p s[0]\\cup g r o u p s[1]$ \u25b7 Cannot be a descendant of node 1. else if $r a n d\\leq0.7$ then candidates $\\leftarrow$ groups[0] \u222a groups[2] \u25b7 Cannot be a descendant of node 0. else candidates \u2190 nodes end if $n_{-}i n_{-}n o d e s\\gets\\operatorname*{min}(\\log(c a n d i d a t e s),n_{-}i n_{-}n o d e s)$ weights $\\leftarrow$ [depth_to_root(c) \u00b7 1.5 + 1 \u2200c \u2208 candidates] \u25b7 Defne sampling weights to prioritize deeper nodes. $\\triangleright$ This way, the solution reasoning chain is expected to be longer. in_nodes $\\leftarrow$ random_choice(candidates, n_in_nodes, prob = weights/sum(weights)) $c u r\\_l a b e l\\gets0$ for $i n\\_i d x\\in i n_{.}$ _nodes do cur_label \u2190 cur_label | labels[in_idx] \u25b7 Update label using bitwise OR. edges.append((in_idx, idx)) end for groups[cur_label].append(idx) labels[idx] \u2190 cur_label $n o d e s\\overleftarrow{}\\leftarrow{\\mathit{n o d e s}}\\cup\\{i d x\\}$ idx idx + 1   "
            },
            {
                "id": 178,
                "line": "end while  "
            },
            {
                "id": 179,
                "line": "## Statistics  "
            },
            {
                "id": 180,
                "line": "We show the size of all datasets in Table 3.  "
            },
            {
                "id": 181,
                "line": "Table 3 Statistics of the datasets.   "
            },
            {
                "id": 182,
                "line": "<html><body><table><caption>Table 3 Statistics of the datasets. "
            },
            {
                "id": 183,
                "line": "</caption><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>  "
            }
        ],
        "refined_text": "## Examples  \nexamples of the questions and CoT solutions for the datasets used  \n### GSM8k  \nQuestion $=$ \"John cuts his grass to 2 inches. It grows .5 inches per month. When it gets to 4 inches he cuts it back down to 2 inches. It cost $\\Phi100$ to get his grass cut. How much does he pay per year?\"  \n$\\mathrm{Steps}=[\"*4\u20132{=}2*\"$ , $\"\\!\\ll\\!2/.5\\!\\!=\\!\\!4\\!\\gg\\!\"$ , $\"\\ll12/4{=}3*\"$ \", $\"\\!\\ll\\!100\\!*\\!3\\!=\\!300\\!*\\!\"]$ Answer $=~\"300\"$ \"  \n#### ProntoQA  \nQuestion $=$ \"Brimpuses are not luminous. Shumpuses are amenable. Each yumpus is a lorpus. Gorpuses are shumpuses. Each zumpus is a grimpus. Gorpuses are rompuses. Dumpuses are not foral. Lempuses are cold. Brimpuses are impuses. Every lorpus is foral. Every rompus is transparent. Grimpuses are mufed. Rompuses are yumpuses. Rompuses are wumpuses. Zumpuses are fast. Wumpuses are bitter. Every sterpus is orange. Each lorpus is a vumpus. Yumpuses are feisty. Each yumpus is a lempus. Gorpuses are snowy. Zumpuses are gorpuses. Every lorpus is a sterpus. Stella is a brimpus. Stella is a zumpus. True or false: Stella is not foral.\"  \nSteps $=$ [\"Stella is a zumpus. Zumpuses are gorpuses.\", \"Stella is a gorpus. Gorpuses are rompuses.\", \"Stella is a rompus. Rompuses are yumpuses.\", \"Stella is a yumpus. Each yumpus is a lorpus.\", \"Stella is a lorpus. Every lorpus is foral.\", \"Stella is foral.\"] An $\\mathsf{\\Pi}_{\\mathrm{Wer}}=\\mathsf{\\Pi}^{\\mathsf{W}}\\mathbb{F}\\mathsf{a l s}.$ e\"  \n##### ProsQA  \nQuestion $=$ \"Every shumpus is a rempus. Every shumpus is a yimpus. Every terpus is a fompus. Every terpus is a gerpus. Every gerpus is a brimpus. Alex is a rempus. Every rorpus is a scrompus. Every rorpus is a yimpus. Every terpus is a brimpus. Every brimpus is a lempus. Tom is a terpus. Every shumpus is a timpus. Every yimpus is a boompus. Davis is a shumpus. Every gerpus is a lorpus. Davis is a fompus. Every shumpus is a boompus. Every shumpus is a rorpus. Every terpus is a lorpus. Every boompus is a timpus. Every fompus is a yerpus. Tom is a dumpus. Every rempus is a rorpus. Is Tom a lempus or scrompus?\"   \nSteps $=$ [\"Tom is a terpus.\", \"Every terpus is a brimpus.\", \"Every brimpus is a lempus.\"] An $\\mathtt{s w e r=\"10m}$ is a lempus.\"  \n## Construction of ProsQA  \nTo construct the dataset, we frst compile a set of typical entity names, such as \u201cAlex\u201d and \u201cJack,\u201d along with fctional concept names like \u201clorpus\u201d and \u201crorpus,\u201d following the setting of ProntoQA (Saparov and He, 2022). Each problem is structured as a binary question: \u201cIs [Entity] a [Concept A] or [Concept B]?\u201d Assuming [Concept A] is the correct answer, we build a directed acyclic graph (DAG) where each node represents an entity or a concept. The graph is constructed such that a path exists from [Entity] to [Concept A] but not to [Concept B].  \nAlgorithm 1 describes the graph construction process. The DAG is incrementally built by adding nodes and randomly connecting them with edges. To preserve the validity of the binary choice, with some probability, we enforce that the new node cannot simultaneously serve as a descendant to both node 0 and 1. This separation maintains distinct families of nodes and balances their sizes to prevent model shortcuts.  \nTable 2 Statistics of the graph structure in ProsQA.   \n<html><body><table><caption>Table 2 Statistics of the graph structure in ProsQA. \n</caption><tr><td>Nodes</td><td># Edges</td><td>Len. of Shortest Path</td><td># Shortest Paths</td></tr><tr><td>23.0</td><td>36.0</td><td>3.8</td><td>1.6</td></tr></table></body></html>  \nAfter the graph is constructed, nodes without parents are assigned entity names, while other nodes receive concept names. To formulate a question of the form \u201cIs [Entity] a [Concept A] or [Concept B]?\u201d, we designate node 0 in the graph as [Entity], a leaf node labeled 1 as [Concept A], and a leaf node labeled 2 as [Concept B]. This setup ensures a path from [Entity] to [Concept A] without any connection to [Concept B], introducing a moderately complex reasoning path. Finally, to avoid positional biases, [Concept A] and [Concept B] are randomly permuted in each question.  \n### Algorithm 1 Graph Construction for ProsQA  \n$e d g e s\\gets\\{\\}$   \n$n o d e s\\gets\\{0,1\\}$   \n$l a b e l s\\gets\\{0:1,1:2\\}$ $\\triangleright$ Labels: 1 (descendant of node 0), 2 (descendant of node 1), 3 (both), 0 (neither).   \n$g r o u p s\\gets\\{0:\\{\\},1:\\{0\\},2:\\{1\\},3:\\{\\}\\}$   \n$i d x\\gets2$   \nwhile $i d x<N$ do \u25b7 For each new node, randomly add edges from existing nodes $n\\_i n\\_n o d e s\\gets\\mathrm{poisson}($ (1.5) $r a n d\\gets\\mathrm{random}()$ if $r a n d\\leq0.35$ then candidat $z\\leftarrow g r o u p s[0]\\cup g r o u p s[1]$ \u25b7 Cannot be a descendant of node 1. else if $r a n d\\leq0.7$ then candidates $\\leftarrow$ groups[0] \u222a groups[2] \u25b7 Cannot be a descendant of node 0. else candidates \u2190 nodes end if $n_{-}i n_{-}n o d e s\\gets\\operatorname*{min}(\\log(c a n d i d a t e s),n_{-}i n_{-}n o d e s)$ weights $\\leftarrow$ [depth_to_root(c) \u00b7 1.5 + 1 \u2200c \u2208 candidates] \u25b7 Defne sampling weights to prioritize deeper nodes. $\\triangleright$ This way, the solution reasoning chain is expected to be longer. in_nodes $\\leftarrow$ random_choice(candidates, n_in_nodes, prob = weights/sum(weights)) $c u r\\_l a b e l\\gets0$ for $i n\\_i d x\\in i n_{.}$ _nodes do cur_label \u2190 cur_label | labels[in_idx] \u25b7 Update label using bitwise OR. edges.append((in_idx, idx)) end for groups[cur_label].append(idx) labels[idx] \u2190 cur_label $n o d e s\\overleftarrow{}\\leftarrow{\\mathit{n o d e s}}\\cup\\{i d x\\}$ idx idx + 1   \nend while  \n## Statistics  \nWe show the size of all datasets in Table 3.  \nTable 3 Statistics of the datasets.   \n<html><body><table><caption>Table 3 Statistics of the datasets. \n</caption><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Test</td></tr><tr><td>GSM8k</td><td>385,620</td><td>500</td><td>1319</td></tr><tr><td>ProntoQA</td><td>9,000</td><td>200</td><td>800</td></tr><tr><td>ProsQA</td><td>17,886</td><td>300</td><td>500</td></tr></table></body></html>",
        "images": [],
        "tables": [],
        "references": []
    },
    {
        "level": 1,
        "num": 11,
        "title": "Clock-Time Reasoning Efficiency Metric",
        "text": "We present a clock-time comparison to evaluate reasoning efciency. The reported values represent the average inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our results show that clock time is generally proportional to the number of newly generated tokens, as detailed in Table 1.  \nTable 4 Inference time (in seconds) comparison across tasks and methods.   \n<html><body><table><caption>Table 4 Inference time (in seconds) comparison across tasks and methods. \n</caption><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>",
        "lines": [
            {
                "id": 185,
                "line": "We present a clock-time comparison to evaluate reasoning efciency. The reported values represent the average inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our results show that clock time is generally proportional to the number of newly generated tokens, as detailed in Table 1.  "
            },
            {
                "id": 186,
                "line": "Table 4 Inference time (in seconds) comparison across tasks and methods.   "
            },
            {
                "id": 187,
                "line": "<html><body><table><caption>Table 4 Inference time (in seconds) comparison across tasks and methods. "
            },
            {
                "id": 188,
                "line": "</caption><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>  "
            }
        ],
        "refined_text": "We present a clock-time comparison to evaluate reasoning efciency. The reported values represent the average inference time per test case (in seconds), with a batch size of 1, measured on an Nvidia A100 GPU. For the no-CoT and CoT baselines, we employ the standard generate method from the transformers3 library. Our results show that clock time is generally proportional to the number of newly generated tokens, as detailed in Table 1.  \n  \nTable 4 Inference time (in seconds) comparison across tasks and methods.   \n<html><body><table><caption>Table 4 Inference time (in seconds) comparison across tasks and methods. \n</caption><tr><td>Method</td><td>GSM8k</td><td>ProntoQA</td><td>ProsQA</td></tr><tr><td>No-CoT</td><td>0.03</td><td>0.03</td><td>0.08</td></tr><tr><td>CoT</td><td>0.26</td><td>0.85</td><td>0.47</td></tr><tr><td>COCONUT</td><td>0.09</td><td>0.11</td><td>0.15</td></tr></table></body></html>",
        "images": [],
        "tables": [
            {
                "type": "table",
                "img_path": "images/c81e5cdf7aa362b6915254737e207d052d121cdf144405aa782f3632384b8feb.jpg",
                "table_caption": [
                    "Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. \u2217The result is from Deng et al. (2024). "
                ],
                "table_footnote": [],
                "table_body": "\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 \u00b10.2</td><td>25.0</td><td>98.8 \u00b10.8</td><td>92.5</td><td>77.5 \u00b11.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 \u00b10.5</td><td>2.2</td><td>93.8 \u00b10.7</td><td>3.0</td><td>76.7 \u00b11.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 \u00b10.3</td><td>3.0</td><td>98.2 \u00b10.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 \u00b11.8</td><td>2.2</td><td>77.7: \u00b121.0</td><td>3.0</td><td>75.9 \u00b10.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 \u00b11.5</td><td>8.2</td><td>99.8 \u00b10.2</td><td>9.0</td><td>97.0 \u00b10.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 \u00b10.8</td><td>8.2</td><td>52.4 \u00b10.4</td><td>9.0</td><td>76.1 \u00b10.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 \u00b10.5</td><td>2.3</td><td>99.9 \u00b10.1</td><td>3.0</td><td>95.5 \u00b11.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 \u00b10.7</td><td>2.2</td><td>100.0 \u00b10.1</td><td>3.0</td><td>96.6 \u00b10.8</td><td>8.2</td></tr></table></body></html>\n\n",
                "page_idx": 5,
                "id": "Table 1",
                "related_ids": [],
                "title": "Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA.",
                "description": "Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. \u2217The result is from Deng et al. (2024). \n",
                "org_md_ref": "<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 \u00b10.2</td><td>25.0</td><td>98.8 \u00b10.8</td><td>92.5</td><td>77.5 \u00b11.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 \u00b10.5</td><td>2.2</td><td>93.8 \u00b10.7</td><td>3.0</td><td>76.7 \u00b11.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 \u00b10.3</td><td>3.0</td><td>98.2 \u00b10.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 \u00b11.8</td><td>2.2</td><td>77.7: \u00b121.0</td><td>3.0</td><td>75.9 \u00b10.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 \u00b11.5</td><td>8.2</td><td>99.8 \u00b10.2</td><td>9.0</td><td>97.0 \u00b10.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 \u00b10.8</td><td>8.2</td><td>52.4 \u00b10.4</td><td>9.0</td><td>76.1 \u00b10.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 \u00b10.5</td><td>2.3</td><td>99.9 \u00b10.1</td><td>3.0</td><td>95.5 \u00b11.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 \u00b10.7</td><td>2.2</td><td>100.0 \u00b10.1</td><td>3.0</td><td>96.6 \u00b10.8</td><td>8.2</td></tr></table></body></html>  ",
                "mod_md_ref": "<html><body><table><caption>Table 1 Results on three datasets: GSM8l, ProntoQA and ProsQA. Higher accuracy indicates stronger reasoning ability, while generating fewer tokens indicates better efciency. \u2217The result is from Deng et al. (2024). \n</caption><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">GSM8k</td><td colspan=\"2\">ProntoQA</td><td colspan=\"2\">ProsQA</td></tr><tr><td>Acc. . (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td><td>Acc. (%)</td><td># Tokens</td></tr><tr><td>CoT</td><td>42.9 \u00b10.2</td><td>25.0</td><td>98.8 \u00b10.8</td><td>92.5</td><td>77.5 \u00b11.9</td><td>49.4</td></tr><tr><td>No-CoT</td><td>16.5 \u00b10.5</td><td>2.2</td><td>93.8 \u00b10.7</td><td>3.0</td><td>76.7 \u00b11.0</td><td>8.2</td></tr><tr><td>iCoT</td><td>30.0*</td><td>2.2</td><td>99.8 \u00b10.3</td><td>3.0</td><td>98.2 \u00b10.3</td><td>8.2</td></tr><tr><td>Pause Token</td><td>16.4 \u00b11.8</td><td>2.2</td><td>77.7: \u00b121.0</td><td>3.0</td><td>75.9 \u00b10.7</td><td>8.2</td></tr><tr><td>COCONUT (Ours)</td><td>34.1 \u00b11.5</td><td>8.2</td><td>99.8 \u00b10.2</td><td>9.0</td><td>97.0 \u00b10.3</td><td>14.2</td></tr><tr><td>- w/o curriculum</td><td>14.4 \u00b10.8</td><td>8.2</td><td>52.4 \u00b10.4</td><td>9.0</td><td>76.1 \u00b10.2</td><td>14.2</td></tr><tr><td>- w/o thought</td><td>21.6 \u00b10.5</td><td>2.3</td><td>99.9 \u00b10.1</td><td>3.0</td><td>95.5 \u00b11.1</td><td>8.2</td></tr><tr><td>- pause as thought</td><td>24.1 \u00b10.7</td><td>2.2</td><td>100.0 \u00b10.1</td><td>3.0</td><td>96.6 \u00b10.8</td><td>8.2</td></tr></table></body></html>  "
            }
        ],
        "references": []
    },
    {
        "level": 1,
        "num": 12,
        "title": "Using More Continuous Thoughts",
        "text": "In Figure 3, we present the performance of Coconut on GSM8k using $c\\in\\{0,1,2\\}$ . When experimenting with $c=3$ , we observe a slight performance drop accompanied by increased variance. Analysis of the training logs indicates that adding three continuous thoughts at once \u2013 particularly during the fnal stage transition \u2013 leads to a sharp spike in training loss, causing instability. Future work will explore fner-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning\u2014e.g., generating the reasoning skeleton in language and completing the reasoning process in latent space\u2014could provide a promising direction for improving performance and stability.",
        "lines": [
            {
                "id": 190,
                "line": "In Figure 3, we present the performance of Coconut on GSM8k using $c\\in\\{0,1,2\\}$ . When experimenting with $c=3$ , we observe a slight performance drop accompanied by increased variance. Analysis of the training logs indicates that adding three continuous thoughts at once \u2013 particularly during the fnal stage transition \u2013 leads to a sharp spike in training loss, causing instability. Future work will explore fner-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning\u2014e.g., generating the reasoning skeleton in language and completing the reasoning process in latent space\u2014could provide a promising direction for improving performance and stability.  "
            }
        ],
        "refined_text": "In Figure 3, we present the performance of Coconut on GSM8k using $c\\in\\{0,1,2\\}$ . When experimenting with $c=3$ , we observe a slight performance drop accompanied by increased variance. Analysis of the training logs indicates that adding three continuous thoughts at once \u2013 particularly during the fnal stage transition \u2013 leads to a sharp spike in training loss, causing instability. Future work will explore fner-grained schedules, such as incrementally adding continuous thoughts one at a time while removing fewer language tokens, as in iCoT (Deng et al., 2024). Additionally, combining language and latent reasoning\u2014e.g., generating the reasoning skeleton in language and completing the reasoning process in latent space\u2014could provide a promising direction for improving performance and stability.\n  ",
        "images": [
            {
                "type": "image",
                "img_path": "images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg",
                "img_caption": [
                    "Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts. "
                ],
                "img_footnote": [],
                "page_idx": 5,
                "id": "Figure 3",
                "related_ids": [],
                "title": "Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts.",
                "description": "Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts. \n",
                "org_md_ref": "![  ](  images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg   'test')",
                "mod_md_ref": "![Figure 3 Accuracy on GSM8k with diferent number of continuous thoughts.](images/4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg 'Figure 3:')  "
            }
        ],
        "tables": [],
        "references": []
    }
]