{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = [\"Training Large Language Models to Reason in a Continuous Latent Space\", \n",
    "#           \"Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought\",\n",
    "#           \"s1: Simple test-time scaling\",\n",
    "#           \"From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation\",\n",
    "#           \"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search\"]\n",
    "title = \"Training Large Language Models to Reason in a Continuous Latent Space\"\n",
    "\n",
    "# pdf_pathes = [\"./data/2412.06769v2.pdf\",\n",
    "#               \"./data/2501.04682v1.pdf\",\n",
    "#               \"./data/2501.19393v2.pdf\",\n",
    "#               \"./data/2502.00330v1.pdf\",\n",
    "#               \"./data/2502.02508v1.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前脚本所在目录的父目录 (即 my_project)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# 将父目录添加到 sys.path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Downloads a file from the given URL and saves it as filename.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"Successfully downloaded: {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(original_zip_file, destination_folder):\n",
    "    assert os.path.splitext(original_zip_file)[-1] == '.zip'\n",
    "    with zipfile.ZipFile(original_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Metadata Extractijon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.arxiv_tool import ArxivKit\n",
    "from apis.semanticscholar_tool import SemanticScholarKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:08:31,568 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Training+Large+Language+Models+to+Reason+in+a+Continuous+Latent+Space&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "2025-02-13 09:08:35,822 - INFO - Got first page: 100 of 2656582 total results\n"
     ]
    }
   ],
   "source": [
    "arxiv = ArxivKit()\n",
    "\n",
    "# arxiv_metadata = []\n",
    "# for title in titles:\n",
    "#     candit_arxiv_metadata = arxiv.retrieve_metadata_by_paper(query_term=title, max_cnt=3)\n",
    "#     arxiv_metadata.append(candit_arxiv_metadata)\n",
    "#     time.sleep(5)\n",
    "\n",
    "arxiv_metadata = arxiv.retrieve_metadata_by_paper(query_term=title, max_cnt=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:08:37,611 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Training%20Large%20Language%20Models%20to%20Reason%20in%20a%20Continuous%20Latent%20Space&fields=abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "ss = SemanticScholarKit()\n",
    "\n",
    "# ss_metadata = []\n",
    "# for title in titles:\n",
    "#     candit_ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)\n",
    "#     ss_metadata.append(candit_ss_metadata)\n",
    "#     time.sleep(5)\n",
    "ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference and Citedby Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673fbdd957cada770d10dffca5e45b53da43a3c6\n"
     ]
    }
   ],
   "source": [
    "# paper_ss_id = ss_metadata[0][0].get('paperId')\n",
    "paper_ss_id = ss_metadata[0].get('paperId')\n",
    "print(paper_ss_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:08:46,239 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/references?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reference_metadata = ss.get_semanticscholar_references(paper_id=paper_ss_id, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 09:08:50,728 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/673fbdd957cada770d10dffca5e45b53da43a3c6/citations?fields=contexts,intents,contextsWithIntent,isInfluential,abstract,authors,citationCount,citationStyles,corpusId,externalIds,fieldsOfStudy,influentialCitationCount,isOpenAccess,journal,openAccessPdf,paperId,publicationDate,publicationTypes,publicationVenue,referenceCount,s2FieldsOfStudy,title,url,venue,year&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "citedby_metadata = ss.get_semanticscholar_citedby(paper_id=paper_ss_id, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(citedby_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper PDF Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def pdf_outline_detection(pdf_path, excpert_len:Optional[int]=300):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    toc_infos = doc.get_toc(simple=False) or []\n",
    "\n",
    "    pdf_toc = []\n",
    "    for item in toc_infos:\n",
    "        lvl = item[0] if len(item) > 0 else None\n",
    "        title = item[1] if len(item) > 1 else None\n",
    "        start_page = item[2] if len(item) > 2 else None\n",
    "        end_pos = item[3].get('to') if len(item) > 3 and item[3] else None\n",
    "        nameddest = item[3].get('nameddest') if len(item) > 3 and item[3] else None\n",
    "        if_collapse = item[3].get('collapse', False) if len(item) > 3 and item[3] else None\n",
    "\n",
    "        if start_page is not None:\n",
    "            page = doc[start_page-1]\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "\n",
    "            lines = \"\"\n",
    "            for block in blocks:\n",
    "                x0, y0, x1, y1, text, _, _ = block\n",
    "                if len(lines) < excpert_len:\n",
    "                    if end_pos and x0 >= end_pos[0]:\n",
    "                        lines += text\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            pdf_toc.append({\n",
    "                \"level\": lvl,\n",
    "                \"title\": title,\n",
    "                \"page\": start_page,\n",
    "                \"position\": end_pos,\n",
    "                \"nameddest\": nameddest,\n",
    "                'if_collapse': if_collapse,\n",
    "                \"excerpt\": lines + \"...\"\n",
    "            })\n",
    "    return pdf_toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/home/jiezi/Code/Temp/data/2412.06769v2.pdf\"\n",
    "pdf_toc = pdf_outline_detection(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miner U PDF Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.mineru_tool import MinerUKit\n",
    "\n",
    "mineru_api_key = os.getenv('MINERU_API_KEY_1')\n",
    "mineru = MinerUKit(api_key=mineru_api_key)\n",
    "upload_res = mineru.batch_process_files(pdf_files=pdf_pathes, if_ocr=False, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = upload_res.json().get('data', {}).get('batch_id')\n",
    "running_res = mineru.batch_status_check(batch_id=batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/jiezi/Code/Temp/tmp\"\n",
    "\n",
    "if running_res.json().get('msg') == 'ok':\n",
    "    results = running_res.json().get('data', {}).get('extract_result', []) \n",
    "    for item in results:\n",
    "        if item.get('state') == 'done':\n",
    "            file_name_nosuffix = item.get('file_name').rsplit('.', 1)[0] \n",
    "            zip_url = item.get('full_zip_url')\n",
    "            download_file_name = os.path.join(temp_path, file_name_nosuffix+\".zip\") \n",
    "            unzip_folder_name = os.path.join(temp_path, file_name_nosuffix) \n",
    "            download_file(zip_url, download_file_name)\n",
    "            unzip_file(download_file_name, unzip_folder_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Syntax Coversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "temp_path = \"/home/jiezi/Code/Temp/tmp\"\n",
    "file_name_nosuffix = \"2412.06769v2\"\n",
    "file_path = os.path.join(temp_path, file_name_nosuffix)\n",
    "\n",
    "from pathlib import Path  \n",
    " \n",
    "for file in Path(file_path).glob('*'): \n",
    "    file_nm = os.path.basename(file)\n",
    "    if \"_origin.pdf\" in file_nm:\n",
    "        os.remove(file) \n",
    "    elif \"_content_list.json\" in file_nm:\n",
    "        os.rename(file, os.path.join(file_path, \"content_list.json\"))\n",
    "\n",
    "md_file = os.path.join(file_path, \"full.md\")\n",
    "content_json_file = os.path.join(file_path, \"content_list.json\")\n",
    "layout_json_file = os.path.join(file_path, \"layout.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/jiezi/Code/Temp/tmp/2412.06769v2/content_list.json\") as json_data:\n",
    "    content_json = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = \"/home/jiezi/Code/Temp/tmp/2412.06769v2/full.md\"\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown Syntax Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert picture from html syntax to markdown syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert table from Markdown syntax to html form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def markdown_table_to_html(markdown_text):\n",
    "    \"\"\"\n",
    "    将 Markdown 文本中的 Markdown 表格转换为 HTML 表格。\n",
    "\n",
    "    Args:\n",
    "        markdown_text: 包含 Markdown 表格的 Markdown 文本。\n",
    "\n",
    "    Returns:\n",
    "        转换后的 Markdown 文本，表格部分已转换为 HTML 表格。\n",
    "    \"\"\"\n",
    "\n",
    "    lines = markdown_text.splitlines()\n",
    "    output_lines = []\n",
    "    in_table = False\n",
    "    table_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip().startswith('|'):\n",
    "            in_table = True\n",
    "            table_lines.append(line)\n",
    "        else:\n",
    "            if in_table:\n",
    "                # 表格结束，处理之前收集的表格行\n",
    "                html_table = _convert_table_lines_to_html(table_lines)\n",
    "                output_lines.append(html_table)\n",
    "                in_table = False\n",
    "                table_lines = []\n",
    "            output_lines.append(line)\n",
    "\n",
    "    # 处理文本末尾可能存在的表格\n",
    "    if in_table:\n",
    "        html_table = _convert_table_lines_to_html(table_lines)\n",
    "        output_lines.append(html_table)\n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "def _convert_table_lines_to_html(table_lines):\n",
    "    \"\"\"\n",
    "    将 Markdown 表格行转换为 HTML 表格。\n",
    "\n",
    "    Args:\n",
    "        table_lines: Markdown 表格行的列表。\n",
    "\n",
    "    Returns:\n",
    "        HTML 表格字符串。\n",
    "    \"\"\"\n",
    "    html_lines = [\"<table>\", \"  <thead>\", \"    <tr>\"]\n",
    "    header_cells = [cell.strip() for cell in table_lines[0].strip('|').split('|')]\n",
    "    for header in header_cells:\n",
    "        html_lines.append(f\"      <th>{header}</th>\")\n",
    "    html_lines.append(\"    </tr>\")\n",
    "    html_lines.append(\"  </thead>\")\n",
    "    html_lines.append(\"  <tbody>\")\n",
    "\n",
    "    if len(table_lines) > 1 and re.match(r'^\\|[-:| ]+\\|[-:| ]*$', table_lines[1].strip()):\n",
    "        # 存在分隔行，跳过分隔行，从第三行开始是数据行\n",
    "        data_start_index = 2\n",
    "    else:\n",
    "        data_start_index = 1 # 没有分隔行，从第二行开始是数据行\n",
    "\n",
    "    for i in range(data_start_index, len(table_lines)):\n",
    "        html_lines.append(\"    <tr>\")\n",
    "        data_cells = [cell.strip() for cell in table_lines[i].strip('|').split('|')]\n",
    "        for cell in data_cells:\n",
    "            html_lines.append(f\"      <td>{cell}</td>\")\n",
    "        html_lines.append(\"    </tr>\")\n",
    "\n",
    "    html_lines.append(\"  </tbody>\")\n",
    "    html_lines.append(\"</table>\")\n",
    "    return \"\\n\".join(html_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = markdown_table_to_html(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Markdown Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align markdown title with pdf ToC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def restore_md_toc(md_content, pdf_toc):\n",
    "    \"\"\"\n",
    "    Align markdown title with pdf table of content (generated from fitz)\n",
    "\n",
    "    Args:\n",
    "        md_file: Path to the markdown file.\n",
    "        pdf_toc: pdf toc from pdf_outline_detection function\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a section\n",
    "        with 'level', 'section_num', 'title', and 'text' keys.\n",
    "        Returns an empty list if the file doesn't exist.\n",
    "        Returns None if an error occurs.\n",
    "    \"\"\"\n",
    "    if pdf_toc:\n",
    "        modified_lines = []  # 用于存储修改后的行的列表\n",
    "\n",
    "        title_pattern = r\"^#{1,}\\s*.*$\"  # patttern of markdown title\n",
    "        md_titles = []\n",
    "\n",
    "        for idx, line in enumerate(md_content.splitlines()):  # iterate markdown lines\n",
    "            if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"]:\n",
    "                match = re.search(title_pattern, line)\n",
    "                if match:  # find markdown title\n",
    "                    sec_title = line\n",
    "                    flag = 0\n",
    "\n",
    "                    for x in pdf_toc:  # iterate pdf toc, refine markdown title based on toc title\n",
    "                        toc_title = x['title'] \n",
    "                        toc_level = int(x['level'])  \n",
    "                        if toc_title in line:  \n",
    "                            sec_title = \"#\"*toc_level + \" \" + toc_title + \"  \"\n",
    "                            flag = 1\n",
    "                            break\n",
    "                    \n",
    "                    if flag == 0:  # markdown title not exit in toc\n",
    "                        for item in ['Acknowledgement', 'Reference', 'Appendix']:\n",
    "                            if item in line:\n",
    "                                sec_title = line\n",
    "                                flag = 1\n",
    "                    \n",
    "                    if flag == 0:\n",
    "                        if len(md_titles) > 0:\n",
    "                            if re.match('^#{1,}', md_titles[-1]):\n",
    "                                pre_level = re.match('^#{1,}', md_titles[-1]).group(0) + \"#\"\n",
    "                                sec_title = re.sub('^#{1,}', pre_level, line)\n",
    "                            else:\n",
    "                                sec_title = \"#\" + line\n",
    "\n",
    "                    modified_lines.append(sec_title)\n",
    "                    md_titles.append(sec_title)  # get markdown title\n",
    "\n",
    "                else:\n",
    "                    modified_lines.append(line)\n",
    "    return \"\\n\".join(modified_lines), md_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_content_rvsd, md_titles = restore_md_toc(markdown_content, pdf_toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Json Content (Charts, Tables, and Equations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get id, title, desc, etc. for charts, tables, and equations in content json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_lines(text, sentence_length):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # 使用正则表达式分割句子\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|;|!)\\s', text) # 更精确的断句正则\n",
    "\n",
    "    result = \"\"\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.strip()\n",
    "\n",
    "        if cleaned_sentence:\n",
    "            result += cleaned_sentence + \" \"\n",
    "            current_length = len(result.strip())\n",
    "\n",
    "            if current_length >= sentence_length:\n",
    "                return result.strip()\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def process_content_json(content_json):\n",
    "    \"\"\"assign title and ids to images/ charts, tables, and equations\n",
    "    \"\"\"\n",
    "    img_lst, tbl_lst, formula_lst = [], [], []\n",
    "    i, j, k = 1, 1, 1\n",
    "    for x in content_json:\n",
    "        if x['type'] in ['image']:\n",
    "            desc = \"\\n\".join(x.get('img_caption', [])) + \"\\n\" + \"\\n\".join(x.get('img_footnote', []))\n",
    "            ptrn = r\"(pic|picture|img|image|chart|figure|fig|table|tbl)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "            mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "            img_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                img_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "            if len(img_ids) == 0:\n",
    "                img_ids = [f\"Image_Number_{i}\"]\n",
    "                i += 1\n",
    "            x['id'] = img_ids[0]\n",
    "            x['related_ids'] = img_ids[1:]\n",
    "            x['title'] = get_first_lines(desc, 10)\n",
    "            x['description'] = desc\n",
    "            img_lst.append(x)\n",
    "\n",
    "        elif x['type'] == 'table':\n",
    "            desc = \"\\n\".join(x.get('table_caption', [])) + \"\\n\" + \"\\n\".join(x.get('table_footnote', []))\n",
    "            ptrn = r\"(tbl|table|chart|figure|fig)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "            mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "            tbl_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                tbl_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "            if len(tbl_ids) == 0:\n",
    "                tbl_ids = [f\"Table_Number_{j}\"]\n",
    "                j += 1\n",
    "            x['id'] = tbl_ids[0]\n",
    "            x['related_ids'] = tbl_ids[1:]\n",
    "            x['title'] = get_first_lines(desc, 10)\n",
    "            x['description'] = desc\n",
    "            tbl_lst.append(x)\n",
    "\n",
    "            # for table with image\n",
    "            if x.get('img_path') is not None:\n",
    "                item = {'type':'image', 'img_path': x.get('img_path'), 'img_caption': x.get('table_caption'), 'table_footnote': x.get('table_footnote'), 'page_idx': x.get('page_idx')}\n",
    "                desc = \"\\n\".join(item.get('img_caption', [])) + \"\\n\" + \"\\n\".join(item.get('img_footnote', []))\n",
    "                ptrn = r\"(table|tbl|pic|picture|img|image|chart|figure|fig)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "                mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "                img_ids = []\n",
    "                for match in mtch_rslts:\n",
    "                    img_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "                if len(img_ids) == 0:\n",
    "                    img_ids = [f\"Table_Image_Number_{i}\"]\n",
    "                    i += 1\n",
    "                item['id'] = img_ids[0]\n",
    "                item['related_ids'] = img_ids[1:]\n",
    "                item['title'] = get_first_lines(desc, 10)\n",
    "                item['description'] = desc\n",
    "                img_lst.append(item)\n",
    "\n",
    "        elif x['type'] == 'equation':\n",
    "\n",
    "            desc = x.get('text')\n",
    "            ptrn = r\"(formula|equation|notation|syntax)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "            mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "            equation_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                equation_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "            if len(equation_ids) == 0:\n",
    "                equation_ids = [f\"Equation_Number_{k}\"]\n",
    "                k += 1\n",
    "            x['id'] = equation_ids[0]\n",
    "            x['related_ids'] = equation_ids[1:]\n",
    "            x['title'] = equation_ids[0]\n",
    "            x['description'] = equation_ids[0]\n",
    "            formula_lst.append(x)\n",
    "\n",
    "            # for table with image\n",
    "            if x.get('img_path') is not None:\n",
    "                item = {'type':'image', 'img_path': x.get('img_path'), 'img_caption': x.get('img_caption'), 'img_caption': x.get('img_caption'), 'page_idx': x.get('page_idx')}\n",
    "                desc = item.get('text')\n",
    "                ptrn = r\"(formula|equation|notation|syntax)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "                mtch_rslts = re.finditer(ptrn, desc, re.IGNORECASE)\n",
    "\n",
    "                img_ids = []\n",
    "                for match in mtch_rslts:\n",
    "                    img_ids.append(match.group(0))  # 直接获取整个匹配的字符串\n",
    "\n",
    "                if len(img_ids) == 0:\n",
    "                    img_ids = [f\"Equation_Image_Number_{i}\"]\n",
    "                    i += 1\n",
    "                x['id'] = img_ids[0]\n",
    "                x['related_ids'] = img_ids[1:]\n",
    "                x['title'] = equation_ids[0]\n",
    "                x['description'] = equation_ids[0]\n",
    "                img_lst.append(x)\n",
    "    return img_lst, tbl_lst, formula_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst, tbl_lst, formula_lst = process_content_json(content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def llm_gen_w_images(api_key, model_name, qa_prompt, pil_images, sys_prompt=None, temperature=0.3):\n",
    "    \"\"\"q&a with images\n",
    "    Args:\n",
    "        pil_images:\n",
    "            import PIL.Image\n",
    "            image = PIL.Image.open('/path/to/image.png')\n",
    "    \"\"\"\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=sys_prompt,\n",
    "        temperature=temperature)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,  #　\"gemini-2.0-flash-exp\",\n",
    "        contents=[qa_prompt]+pil_images,\n",
    "        config=config)\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "example = {\"img_id\":\"[original imgage id provided]\", \"img_nm\":\"[original image name provided as attached]\", \"img_title\": \"[suggested image title]\", \"img_desc\":\"[a detailed description of the image]\"}\n",
    "img_analysis_prompt = \"\"\"You are provided with multiple images. Please analyze the images, try to extract their image title and give a desctiption of the images.\n",
    "Output in json format like:\n",
    "```json\n",
    "{example_json}\n",
    "```\n",
    "\n",
    "## INPUTf\n",
    "Here are images ids and names. \n",
    "{img_info}\n",
    "\n",
    "## OUTPUT\n",
    "Now get started!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "tmp_path = \"/home/jiezi/Code/Temp/tmp/2412.06769v2\"\n",
    "\n",
    "img_info = \"\"\n",
    "pil_images = []\n",
    "for img in img_lst:\n",
    "    img_url = os.path.join(tmp_path, img.get('img_path'))\n",
    "    pil_images.append(PIL.Image.open(img_url))\n",
    "    img_info += f\"img_id: {img.get('id')}   img_nm: {os.path.basename(img.get('img_path'))}\\n\"\n",
    "\n",
    "qa_prompt = img_analysis_prompt.format(example_json=json.dumps(example, ensure_ascii=False), img_info=img_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GEMINI_API_KEY_1')\n",
    "temperature = 0.7\n",
    "res = llm_gen_w_images(\n",
    "    api_key=api_key, model_name='gemini-2.0-flash-thinking-exp', \n",
    "    qa_prompt=qa_prompt, pil_images=pil_images, sys_prompt=None, temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with multiple images. Please analyze the images, try to extract their image title and give a desctiption of the images.\n",
      "Output in json format like:\n",
      "```json\n",
      "{\"img_id\": \"[original imgage id provided]\", \"img_nm\": \"[original image name provided as attached]\", \"img_title\": \"[suggested image title]\", \"img_desc\": \"[a detailed description of the image]\"}\n",
      "```\n",
      "\n",
      "## INPUTf\n",
      "Here are images ids and names. \n",
      "img_id: Figure 1   img_nm: 3b0f18697b44445c12ab2b41e0ff7a5fa498867fbcd33644600f98041b2a9f6a.jpg\n",
      "img_id: Figure 2   img_nm: e72083f8a262261062393a5c15691de83822ae7b995a8d74e27b22ad37bcb993.jpg\n",
      "img_id: Table 1   img_nm: c81e5cdf7aa362b6915254737e207d052d121cdf144405aa782f3632384b8feb.jpg\n",
      "img_id: Figure 3   img_nm: 4692454aa0746096ebd571ecdc3a93b136498d5be71a2a1a90c85d5df37c9864.jpg\n",
      "img_id: Figure 4   img_nm: 1dffc8860659d93110f6f8489bbebab79afecb8b091a9ccd9d75f3d097fa6ccd.jpg\n",
      "img_id: Figure 5   img_nm: cc15459750485d6d98eafccaf8e50ced906abc8a704d695310089ac62cca1f28.jpg\n",
      "img_id: Figure 6   img_nm: a0c2573a9332eaeb07eb01590cf6511fb96d22cbff67fa864582655891a99cd7.jpg\n",
      "img_id: Figure 7   img_nm: b18da923e56cfb7bcd2f9d9f22fbe92198ebe3923ede4f78d1d8516935e50af4.jpg\n",
      "img_id: Figure 8   img_nm: ca864f9295907420e789c958e97cbc104cc7583fe124badc3aa4a02406e19504.jpg\n",
      "img_id: Figure 9   img_nm: f07b61cbc539d327d8e037a04ff8fcae61d745c2cad9f031e1c0883c8546b38e.jpg\n",
      "img_id: Table 2   img_nm: 1b673db2e64bddfe21934298637d6cb120b86a6d220234001473a03b7899791a.jpg\n",
      "img_id: Table 3   img_nm: 5c0ee50b58fd2b8c96024326986c44075d7064d5bea7fb7ac9033bcd9e8657b9.jpg\n",
      "img_id: Table 4   img_nm: ebfe3c484ccf358de959ecc8d6ea60ad068ed53297db326bc4becbcfffc94a18.jpg\n",
      "\n",
      "\n",
      "## OUTPUT\n",
      "Now get started!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Context Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Image Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify markdown image text to better align with standard syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def modify_image_info(md_text, img_lst):\n",
    "    \"\"\"update image information with alternative text, image title, etc.\"\"\"\n",
    "    img_ptrn = re.compile(\n",
    "        r'!\\s*\\[\\s*(?P<alt>.*?)\\s*\\]'  # 匹配 ![alt] alt 部分\n",
    "        r'\\s*\\(\\s*(?P<link>.*?)\\s*'   # 匹配 (link) link 部分\n",
    "        r'(?:'                         # 非捕获组，处理可选的 title 部分\n",
    "        r'(?P<quotetitle>\"(?P<title_double_quote>.*?)\"|'  # 匹配 双引号 title， 命名组 quotetitle 和 title_double_quote\n",
    "        r\"'(?P<title_single_quote>.*?)')\"                 # 匹配 单引号 title， 命名组 title_single_quote\n",
    "        r')?'                          # title 部分可选\n",
    "        r'\\s*\\)'                      # 匹配 ) 括号结尾\n",
    "    )\n",
    "    lines = md_text.splitlines()\n",
    "    img_lst_rvsd = copy.deepcopy(img_lst)\n",
    "    \n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"]:\n",
    "\n",
    "            # image match logic\n",
    "            img_matches = list(re.finditer(img_ptrn, line))  # 使用 finditer 获取所有匹配项\n",
    "\n",
    "            if img_matches:\n",
    "                for match in reversed(img_matches):  # 逆序遍历匹配项，避免替换位置错乱\n",
    "                    alt_text = match.group(1).strip()\n",
    "                    image_url = match.group(2)\n",
    "                    title = match.group(4).strip() if match.group(4) else None\n",
    "\n",
    "                    for item in img_lst_rvsd:\n",
    "                        if item.get('img_path') == image_url:\n",
    "                            alt_text = item.get('description') if alt_text is None or alt_text == \"\" else alt_text\n",
    "                            title = item.get('title', \"\") if title is None or title == \"\" else title\n",
    "                            title = f\"{item.get('id')}: {title}\" if item.get('id').lower() not in title.lower() else title\n",
    "                            img_md = f\"![{alt_text.strip()}]({image_url.strip()} '{title.strip()}')\"\n",
    "\n",
    "                            # 计算替换的起始和结束位置\n",
    "                            start, end = match.span()\n",
    "                            if item.get('org_md_ref') is None:\n",
    "                                item['org_md_ref'] = line[start:end]  # 在image list中添加原始的markdown引用格式 \n",
    "\n",
    "                            lines[idx] = line[:start] + img_md + line[end:]  # 精确替换\n",
    "                            if item.get('mod_md_ref') is None:\n",
    "                                item['mod_md_ref'] = line[:start] + img_md + line[end:]  # 在image list中添加修订后的markdown引用格式 \n",
    "\n",
    "                            # 改进删除重复信息逻辑\n",
    "                            caption = \"\\n\".join(item.get('img_caption')).strip()\n",
    "                            footnote = \"\\n\".join(item.get('img_footnote')).strip()\n",
    "\n",
    "                            # 由于alt_text和title中已经包括了足够的信息，删除上下文中的重复信息\n",
    "                            if caption and len(caption) > 20 and caption != title:\n",
    "                                if idx > 0 and caption in lines[idx-1]:\n",
    "                                    lines[idx-1] = lines[idx-1].replace(caption, \"\")\n",
    "                                if idx < len(lines) - 1 and caption in lines[idx+1]:\n",
    "                                    lines[idx+1] = lines[idx+1].replace(caption, \"\")\n",
    "\n",
    "                            if footnote and len(footnote) > 20 and footnote != title:\n",
    "                                if idx > 0 and footnote in lines[idx-1]:\n",
    "                                    lines[idx-1] = lines[idx-1].replace(footnote, \"\")\n",
    "                                if idx < len(lines) - 1 and footnote in lines[idx+1]:\n",
    "                                    lines[idx+1] = lines[idx+1].replace(footnote, \"\")\n",
    "                            break  # 找到匹配的 item 后跳出循环\n",
    "    return \"\\n\".join(lines), img_lst_rvsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_content_rvsd, img_lst_rvsd = modify_image_info(md_content_rvsd, img_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Tables Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def modify_tables_info(md_text, tbl_lst):\n",
    "    \"\"\"update table information with alternative text, image title, etc.\"\"\"\n",
    "    \n",
    "    tbl_lst_rvsd = copy.deepcopy(tbl_lst)\n",
    "\n",
    "    lines = md_text.splitlines()\n",
    "\n",
    "    for idx, line in enumerate(lines):  # iterate lines\n",
    "        soup = BeautifulSoup(line, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "\n",
    "        if table:\n",
    "            for item in tbl_lst_rvsd:  # iterate over table list \n",
    "                tbl_desc = item.get('description')\n",
    "                tbl_caption = \"\\n\".join(item.get('table_caption', [])).strip()\n",
    "                tbl_footnote = \"\\n\".join(item.get('table_footnote', [])).strip()\n",
    "                tbl_body = BeautifulSoup(item.get('table_body') , 'html.parser').find('table')\n",
    "                tbl_title = item.get('title')\n",
    "\n",
    "                if table == tbl_body:\n",
    "                    md_caption = table.find('caption')\n",
    "                    if md_caption:\n",
    "                        md_caption.string = tbl_desc      # 将<caption>标签的文本内容替换为 tbl_desc\n",
    "                    else:\n",
    "                        # 如果没有<caption>标签，则创建一个新的<caption>标签并添加到table中\n",
    "                        new_caption_tag = soup.new_tag('caption')\n",
    "                        new_caption_tag.string = tbl_desc\n",
    "                        table.insert(0, new_caption_tag) # 将新的<caption>标签插入到table的开头 (作为第一个子元素)\n",
    "                        \n",
    "                    lines[idx] = f\"<html><body>{table}</body></html>  \"\n",
    "\n",
    "                    # 计算替换的起始和结束位置\n",
    "                    if item.get('org_md_ref') is None:\n",
    "                        item['org_md_ref'] = f\"<html><body>{tbl_body}</body></html>  \" # original table\n",
    "\n",
    "                    if item.get('mod_md_ref') is None:\n",
    "                        item['mod_md_ref'] = f\"<html><body>{table}</body></html>  \"  # table with caption\n",
    "\n",
    "\n",
    "                    # 由于alt_text和title中已经包括了足够的信息，删除上下文中的重复信息\n",
    "                    if tbl_caption and len(tbl_caption) > 20 and tbl_caption != tbl_title:\n",
    "                        if idx > 0 and tbl_caption in lines[idx-1]:\n",
    "                            lines[idx-1] = lines[idx-1].replace(tbl_caption, \"\")\n",
    "                        if idx < len(lines) - 1 and tbl_caption in lines[idx+1]:\n",
    "                            lines[idx+1] = lines[idx+1].replace(tbl_caption, \"\")\n",
    "\n",
    "                    if tbl_footnote and len(tbl_footnote) > 20 and tbl_footnote != tbl_title:\n",
    "                        if idx > 0 and tbl_footnote in lines[idx-1]:\n",
    "                            lines[idx-1] = lines[idx-1].replace(tbl_footnote, \"\")\n",
    "                        if idx < len(lines) - 1 and tbl_footnote in lines[idx+1]:\n",
    "                            lines[idx+1] = lines[idx+1].replace(tbl_footnote, \"\")\n",
    "\n",
    "\n",
    "                    break  # 找到匹配的 item 后跳出循环\n",
    "\n",
    "    return \"\\n\".join(lines), tbl_lst_rvsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7973/410547484.py:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(line, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "md_content_rvsd, tbl_lst_rvsd = modify_tables_info(md_content_rvsd, tbl_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Equations Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 本处非必要\n",
    "# def modify_formula_info(md_text, formula_lst):\n",
    "#     \"\"\"update table information with alternative text, image title, etc.\"\"\"\n",
    "#     md_content_rvsd = md_text\n",
    "#     for formula in formula_lst:\n",
    "#         text = formula.get('text', '')\n",
    "#         id = formula.get('id', '')\n",
    "#         format = formula.get('text_format', '')\n",
    "#         md_content_rvsd = md_content_rvsd.replace(text, f\"``'{format} {id}\\n{text}\\n```\")\n",
    "\n",
    "#         # 计算替换的起始和结束位置\n",
    "#         if item.get('org_md_ref') is None:\n",
    "#             item['org_md_ref'] = text # original formula\n",
    "\n",
    "#         if item.get('mod_md_ref') is None:\n",
    "#             item['mod_md_ref'] = f\"``'{format} {id}\\n{text}\\n```\"  # formula with format information\n",
    "#     return md_content_rvsd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Reference Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the author list in refernce might be to long\n",
    "Cut down and keep within top 5 authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add original text (citation) information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with no reference cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Segmentation Proceess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process markdown into segments:\n",
    "- cut markdown content into segments\n",
    "- restore images, tables, equations positions and informations in each segement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def md_seg_by_title(md_content, level):\n",
    "    title_pattern = re.compile(rf\"^#{{{level}}}\\s+(.+)$\", re.MULTILINE)\n",
    "\n",
    "    segments = []\n",
    "\n",
    "    lines = []\n",
    "    current_section = \"\"\n",
    "    current_title = \"\"\n",
    "\n",
    "    num = 1  # Initialize section number\n",
    "    para_id = 1  # initialize pragraph number\n",
    "\n",
    "    for idx, line in enumerate(md_content.splitlines()):\n",
    "        if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"]:\n",
    "            match = title_pattern.match(line)\n",
    "            if match:\n",
    "                if current_section:  # Save the previous section\n",
    "                    segments.append({\n",
    "                        'level': level,\n",
    "                        'num': num,\n",
    "                        'title': current_title,\n",
    "                        'text': current_section.strip(),  # Remove leading/trailing whitespace\n",
    "                        'lines': lines\n",
    "                    })\n",
    "                    num += 1  # Increment for the next section\n",
    "                \n",
    "                # ready for next section\n",
    "                current_title = match.group(1).strip()\n",
    "                current_section = \"\"  # Start a new section (no title line)\n",
    "                lines = []\n",
    "                para_id = 1\n",
    "            else:\n",
    "                current_section += line + \"\\n\"  # Add to the current section\n",
    "                lines.append({'id': idx, 'line': line})\n",
    "                para_id += 1\n",
    "\n",
    "    if current_section:  # Save the last section\n",
    "        segments.append({\n",
    "            'level': level,\n",
    "            'num': num,\n",
    "            'title': current_title,\n",
    "            'text': current_section.strip(),\n",
    "            'lines': lines\n",
    "        })\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl_1_segments = md_seg_by_title(md_content_rvsd, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# level 1 segment\n",
    "if 2 in set([x.get('level') for x in pdf_toc]): \n",
    "    for seg in lvl_1_segments:  # iterate each level 1 seg\n",
    "        seg_title = seg.get('title')\n",
    "        seg_md = seg.get('text')\n",
    "        for toc in pdf_toc:\n",
    "            if toc.get('title') in seg_title and toc.get('if_collapse') == True:\n",
    "                lvl_2_segments = md_seg_by_title(seg_md, 2)\n",
    "                seg['sub_segmentations'] = lvl_2_segments\n",
    "\n",
    "# level 2 segment\n",
    "if 3 in set([x.get('level') for x in pdf_toc]): \n",
    "    for sub_seg in lvl_2_segments:  # iterate each level 1 seg\n",
    "        sub_title = seg.get('title')\n",
    "        sub_md = seg.get('text')\n",
    "        for toc in pdf_toc:\n",
    "            if toc.get('title') in sub_title and toc.get('if_collapse') == True:\n",
    "                lvl_3_segments = md_seg_by_title(sub_md, 3)\n",
    "                sub_seg['sub_segmentations'] = lvl_3_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation to Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further break segementation to blocks for better emebding and for more comprehnsiable topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- restore postion of images, tables and refernces\n",
    "- specify external sources information\n",
    "  - external sources like image helps LLM for further analysis\n",
    "  - external sources like references help build knowledge graph\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_seg_image(md_text, img_lst, tbl_lst, ref_lst):\n",
    "    \"\"\"restore images within md_text\"\"\"\n",
    "    lines = md_text.splitlines()\n",
    "\n",
    "    seg_images, seg_tbls, seg_refs = [], [], []\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"]:\n",
    "            # resore images in segment\n",
    "            for img in img_lst:\n",
    "                md_ref = img.get('mod_md_ref', '').strip()\n",
    "                # image cited in line but not exist in section \n",
    "                if (md_ref not in \"\\n\".join(lines).strip()\n",
    "                    and (img.get('id') in line.strip() or img.get('title') in line.strip())):\n",
    "                    lines.insert(idx+1, md_ref)\n",
    "                    if img not in seg_images:\n",
    "                        seg_images.append(img)\n",
    "\n",
    "                # line contains image ref but not cited in section\n",
    "                if md_ref in line.strip():\n",
    "                    if img.get('id') not in \"\\n\".join(lines).strip() or img.get('title') in \"\\n\".join(lines).strip():\n",
    "                        lines[idx] = line.replace(md_ref, \"  \")\n",
    "                    elif img not in seg_images:\n",
    "                        seg_images.append(img)\n",
    "\n",
    "            # resore tables in segment\n",
    "            for tbl in tbl_lst:\n",
    "                md_ref = tbl.get('mod_md_ref').strip()\n",
    "\n",
    "                # image cited in line but not exist in section \n",
    "                if (md_ref not in \"\\n\".join(lines).strip()\n",
    "                    and (tbl.get('id') in line.strip() or tbl.get('title') in line.strip())):\n",
    "                    lines.insert(idx+1, md_ref)\n",
    "                    if tbl not in seg_tbls:\n",
    "                        seg_tbls.append(tbl)\n",
    "\n",
    "                # line contains image ref but not cited in section\n",
    "                if md_ref in line.strip():\n",
    "                    if (tbl.get('id') not in \"\\n\".join(lines).strip() or tbl.get('title') in \"\\n\".join(lines).strip()):\n",
    "                        lines[idx] = line.replace(md_ref, \"  \")\n",
    "                    elif tbl not in seg_tbls:\n",
    "                        seg_tbls.append(tbl)     \n",
    "\n",
    "            # resore refs in segment\n",
    "            for idx, line in enumerate(lines):\n",
    "                if line.strip() not in [\"\\n\", \"\\s\", \"\\r\", \"\"]:\n",
    "                    for ref in ref_lst:\n",
    "                        if ref not in seg_refs:\n",
    "                            contexts = ref.get('contexts')\n",
    "                            for x in contexts:\n",
    "                                if x.strip() in line:\n",
    "                                    seg_refs.append(ref.get('citedPaper', {}))  # get only ref paper information, neglect isInfluential, intent, etc.\n",
    "                                    break\n",
    "    \n",
    "    # to-do: append references here\n",
    "    # if len(seg_refs) > 0:\n",
    "    #     lines.extend()\n",
    "\n",
    "    return \"\\n\".join(lines), seg_images, seg_tbls, seg_refs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg in lvl_1_segments:\n",
    "    md_text = seg.get('text')\n",
    "    md_text_new, seg_images, seg_tbls, seg_refs = restore_seg_image(md_text, img_lst_rvsd, tbl_lst_rvsd, reference_metadata)\n",
    "    seg['refined_text'] = md_text_new\n",
    "    seg['images'] = seg_images\n",
    "    seg['tables'] = seg_tbls\n",
    "    seg['references'] = seg_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_wip_json_path = \"pdf_processed_wip_20250212.json\"\n",
    "\n",
    "with open(tmp_wip_json_path, \"w\") as file:\n",
    "    json.dump(lvl_1_segments, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalize process documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save final markdown and processed json list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
